{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529f0706-3417-4285-80a8-82ea8180b25c",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95207d-c5b5-4148-adae-8873e462df1d",
   "metadata": {},
   "source": [
    "### This notebook is inspired by https://github.com/yandexdataschool/nlp_course\n",
    "#### Let's build a GPT model using numpy only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2448f6d-8491-449e-b249-caf6445e8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union\n",
    "from IPython.display import HTML, clear_output\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b79186-9620-48ac-8882-63a7b48f9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS, EOS = ' ', '\\n'\n",
    "\n",
    "data = pd.read_json(\"./data/arxivData.json\")\n",
    "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:128], axis=1) \\\n",
    "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
    "            .tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc12d65-759f-40b6-99f7-c2f839a56c48",
   "metadata": {},
   "source": [
    "#### We're doing char-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f696ef-3707-4640-8afa-11fd8ed48960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens =  136\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(''.join(lines)))\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print('num_tokens = ', num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ef92b6-173c-46a1-b1de-7902bf7f4d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F': 0,\n",
       " 'Ł': 1,\n",
       " 'ő': 2,\n",
       " '6': 3,\n",
       " 'Q': 4,\n",
       " 'à': 5,\n",
       " 'μ': 6,\n",
       " 'æ': 7,\n",
       " 'b': 8,\n",
       " 'i': 9,\n",
       " ':': 10,\n",
       " 'p': 11,\n",
       " 'E': 12,\n",
       " '<': 13,\n",
       " 'G': 14,\n",
       " ')': 15,\n",
       " 'h': 16,\n",
       " 'K': 17,\n",
       " 'Σ': 18,\n",
       " 'f': 19,\n",
       " 'α': 20,\n",
       " 'Ö': 21,\n",
       " 'k': 22,\n",
       " 'I': 23,\n",
       " 'y': 24,\n",
       " 'σ': 25,\n",
       " 'V': 26,\n",
       " 'e': 27,\n",
       " 'q': 28,\n",
       " ']': 29,\n",
       " '\\\\': 30,\n",
       " '_': 31,\n",
       " '5': 32,\n",
       " 'Ü': 33,\n",
       " '=': 34,\n",
       " 'D': 35,\n",
       " 'ω': 36,\n",
       " '`': 37,\n",
       " 'ô': 38,\n",
       " '°': 39,\n",
       " 'N': 40,\n",
       " 'd': 41,\n",
       " '-': 42,\n",
       " 'J': 43,\n",
       " '%': 44,\n",
       " 'â': 45,\n",
       " '&': 46,\n",
       " 'γ': 47,\n",
       " ' ': 48,\n",
       " 'w': 49,\n",
       " '(': 50,\n",
       " 't': 51,\n",
       " 'ρ': 52,\n",
       " 'é': 53,\n",
       " ';': 54,\n",
       " 'U': 55,\n",
       " '0': 56,\n",
       " 'H': 57,\n",
       " '@': 58,\n",
       " 'A': 59,\n",
       " 'g': 60,\n",
       " 'ç': 61,\n",
       " '^': 62,\n",
       " 'n': 63,\n",
       " 'u': 64,\n",
       " '#': 65,\n",
       " 'ś': 66,\n",
       " '+': 67,\n",
       " 'j': 68,\n",
       " 'ε': 69,\n",
       " 'ï': 70,\n",
       " 'Ω': 71,\n",
       " 'ä': 72,\n",
       " 'O': 73,\n",
       " 'C': 74,\n",
       " '9': 75,\n",
       " '|': 76,\n",
       " 'c': 77,\n",
       " '{': 78,\n",
       " 'X': 79,\n",
       " 'v': 80,\n",
       " '8': 81,\n",
       " 'ã': 82,\n",
       " '.': 83,\n",
       " 'M': 84,\n",
       " 'm': 85,\n",
       " 'a': 86,\n",
       " '4': 87,\n",
       " '/': 88,\n",
       " 'o': 89,\n",
       " '\\x7f': 90,\n",
       " 'T': 91,\n",
       " '2': 92,\n",
       " 'λ': 93,\n",
       " 'Z': 94,\n",
       " 's': 95,\n",
       " 'τ': 96,\n",
       " 'ö': 97,\n",
       " 'ê': 98,\n",
       " 'É': 99,\n",
       " '!': 100,\n",
       " 'í': 101,\n",
       " 'õ': 102,\n",
       " 'r': 103,\n",
       " '\"': 104,\n",
       " 'L': 105,\n",
       " 'P': 106,\n",
       " 'z': 107,\n",
       " '3': 108,\n",
       " 'B': 109,\n",
       " 'S': 110,\n",
       " 'á': 111,\n",
       " 'R': 112,\n",
       " ',': 113,\n",
       " '$': 114,\n",
       " 'ν': 115,\n",
       " 'Y': 116,\n",
       " '\\n': 117,\n",
       " 'χ': 118,\n",
       " 'Π': 119,\n",
       " 'è': 120,\n",
       " '~': 121,\n",
       " 'ü': 122,\n",
       " 'x': 123,\n",
       " '?': 124,\n",
       " 'l': 125,\n",
       " '[': 126,\n",
       " '7': 127,\n",
       " 'W': 128,\n",
       " 'ó': 129,\n",
       " '}': 130,\n",
       " '>': 131,\n",
       " '*': 132,\n",
       " '1': 133,\n",
       " 'β': 134,\n",
       " \"'\": 135}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8018bc8a-f97d-4af1-80f6-d99fd07f30d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems alright!\n"
     ]
    }
   ],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d3a524-9c6f-4bab-ac01-7336a2536a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first=True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, data))\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[c] for c in data[i]]\n",
    "        data_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        data_ix = np.transpose(data_ix)\n",
    "\n",
    "    return data_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63c970c-e3dc-4132-9454-eb4cd9633ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5f8b41-baad-447b-9f5c-03438afba69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48  35  64  86 125  48 112  27  77  64 103 103  27  63  51  48  59  51\n",
      "   51  27  63  51   9  89  63  48  55  63   9  51  95  48  19  89 103  48\n",
      "   26   9  95  64  86 125  48   4  64  27  95  51   9  89  63  48  59  63\n",
      "   95  49  27 103   9  63  60  48  54  48 128  27  48  11 103  89  11  89\n",
      "   95  27  48  86  63  48  86 103  77  16   9  51  27  77  51  64 103  27\n",
      "   48  19  89 103  48  26   4  59  48  49  16   9  77  16  48  64  51   9\n",
      "  125   9 107  27  95  48 103  27  77  64 103 103  27  63  51  48 125  86\n",
      "   24  27 103 117]\n",
      " [ 48 109  86 125  86  63  77   9  63  60  48   8   9  22  27  48  95  16\n",
      "   86 103   9  63  60  48  95  24  95  51  27  85  95  48  50 109 109 110\n",
      "  110  15  10  48   9  63  95  51  86  63  77  27  48  60  27  63  27 103\n",
      "   86  51   9  89  63  48  19 103  89  85  48  51  16  27  48  48  48  74\n",
      "    9  51   9 109   9  22  27  48  40 116  74  48  41  86  51  86  48  54\n",
      "   48 109   9  22  27  48  95  16  86 103   9  63  60  48  95  24  95  51\n",
      "   27  85  95  48  86 103  27  48  86  48  80  27 103  24  48  11  89  11\n",
      "   64 125  86 117]\n",
      " [ 48  73   8  68  27  77  51  63  27  95  95  48 110  77  89 103   9  63\n",
      "   60  48  86  63  41  48  35  27  51  27  77  51   9  89  63  48 106 103\n",
      "   89  11  89  95  86 125  95  48   9  63  48   0  89 103  49  86 103  41\n",
      "   42 105  89  89  22   9  63  60  48 110  89  63  86 103  48  48  48  23\n",
      "   85  86  60  27  95  48  49   9  51  16  48  74  89  63  80  89 125  64\n",
      "   51   9  89  63  86 125  48  40  27  64 103  86 125  48  40  27  51  49\n",
      "   89 103  22  95  48  54  48   0  89 103  49  86 103  41  42 125  89  89\n",
      "   22   9  63 117]]\n"
     ]
    }
   ],
   "source": [
    "# print('\\n'.join(lines[::20000]))\n",
    "print(to_matrix(lines[::20000], token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddccb212-d7b4-4df3-a566-75feefacf9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, lines))\n",
    "sample = to_matrix(np.random.choice(lines, size=5), token_to_id, max_len=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8cfe4-d9cd-45b4-9a75-1587fcbf0173",
   "metadata": {},
   "source": [
    "## Let's describe all layers that we need\n",
    "https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#self_attention\n",
    "\n",
    "You can find proofs for all derivatives (using torch.autograd) [here](https://github.com/RomanSafronenkov/ds_stuff/blob/main/Useful%20Stuff/%D0%A7%D0%B5%D1%80%D0%BD%D0%BE%D0%B2%D0%B8%D0%BA%D0%B8/Layer%20derivatives.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c893a515-1542-475b-8da8-e0090ee22e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        return self.forward(x, grad)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear3d(BaseLayer):\n",
    "    \"\"\"\n",
    "    Linear class permorms ordinary FC layer in neural networks\n",
    "    Parameters:\n",
    "    n_input - size of input neurons\n",
    "    n_output - size of output neurons\n",
    "    Methods:\n",
    "    set_optimizer(optimizer) - is used for setting an optimizer for gradient descent\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input: int, n_output: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.w = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(n_input, n_output))\n",
    "        self.b = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(1, n_output))\n",
    "\n",
    "        self.w_optimizer = None\n",
    "        self.b_optimizer = None\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.w_optimizer = copy.copy(optimizer)\n",
    "        self.b_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.w_optimizer.set_weight(self.w)\n",
    "        self.b_optimizer.set_weight(self.b)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return np.matmul(x, self.w) + self.b  # the same as @\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        assert self.w_optimizer is not None and self.b_optimizer is not None, 'You should set an optimizer'\n",
    "        # перемножаем последние 2 измерения друг с другом с помощью matmul и суммируем\n",
    "        w_grad = np.sum(np.transpose(self.input, (0, 2, 1)) @ output_error, axis=0)\n",
    "        b_grad = np.sum(output_error, axis=(0, 1))\n",
    "        input_error = output_error @ self.w.T\n",
    "\n",
    "        self.w = self.w_optimizer.step(w_grad)\n",
    "        self.b = self.b_optimizer.step(b_grad)\n",
    "        return input_error\n",
    "\n",
    "\n",
    "class Activation(BaseLayer):\n",
    "    \"\"\"\n",
    "    Activation class is used for activation function of the FC layer\n",
    "    Params:\n",
    "    activation_function - activation function (e.g. sigmoid, RElU, tanh)\n",
    "    activation_derivative - derivative of the activation function\n",
    "    Methods:\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_function: callable, activation_derivative: callable) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.activation = activation_function\n",
    "        self.derivative = activation_derivative\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return self.activation(x)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return output_error * self.derivative(self.input)\n",
    "\n",
    "\n",
    "class Embedding(BaseLayer):\n",
    "    def __init__(self, n_input, emb_dim, pad_idx=None):\n",
    "        self.n_input = n_input\n",
    "        self.emb_dim = emb_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.weights = np.random.normal(scale=np.sqrt(2/(n_input+emb_dim)), size=(n_input, emb_dim))\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.weights_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.weights_optimizer.set_weight(self.weights)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        self.input = x\n",
    "        return self.weights[x]\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        weights_grad = np.zeros_like(self.weights)\n",
    "        input_shape_len = len(self.input.shape)\n",
    "\n",
    "        if input_shape_len == 2:\n",
    "            for batch_n, s in enumerate(self.input):\n",
    "                for i, emb_i in enumerate(s):\n",
    "                    weights_grad[emb_i] += output_error[batch_n][i]\n",
    "\n",
    "        elif input_shape_len == 1:\n",
    "            for i, emb_i in enumerate(self.input):\n",
    "                weights_grad[emb_i] += output_error[i]\n",
    "\n",
    "        if self.pad_idx is not None:\n",
    "            weights_grad[self.pad_idx] = 0\n",
    "\n",
    "        self.weights = self.weights_optimizer.step(weights_grad)\n",
    "\n",
    "\n",
    "class SoftMaxLayer3D(BaseLayer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.forward_result = None\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        self.input = x\n",
    "        exp = np.exp(x)\n",
    "        self.forward_result = exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "        return self.forward_result\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        \"https://binpord.github.io/2021/09/26/softmax_backprop.html\"\n",
    "        return (output_error - (output_error*self.forward_result).sum(axis=-1, keepdims=True)) * self.forward_result\n",
    "\n",
    "        \n",
    "class MultiHeadAttentionLayer(BaseLayer):\n",
    "    def __init__(self, hid_dim: int, n_heads: int) -> None:\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.input = None\n",
    "        self.attn_bias = None\n",
    "        self.attentions = None\n",
    "        self.q = None\n",
    "        self.k = None\n",
    "        self.v = None\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = hid_dim // n_heads\n",
    "        \n",
    "        self.c_attn = Linear3d(hid_dim, hid_dim * 3)\n",
    "        self.c_proj = Linear3d(hid_dim, hid_dim)\n",
    "        self.softmax = SoftMaxLayer3D()\n",
    "\n",
    "        self.scale = np.sqrt(self.head_size)\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.c_attn.set_optimizer(optimizer)\n",
    "        self.c_proj.set_optimizer(optimizer)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        \n",
    "        q_k_v = self.c_attn(x)\n",
    "        self.q = q_k_v[:, :, :self.hid_dim]\n",
    "        self.k = q_k_v[:, :, self.hid_dim:self.hid_dim*2]\n",
    "        self.v = q_k_v[:, :, self.hid_dim*2:self.hid_dim*3]\n",
    "        assert self.q.shape == self.k.shape == self.v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
    "\n",
    "        head_outputs = []\n",
    "        self.attentions = []\n",
    "        for head_index in range(self.n_heads):\n",
    "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
    "\n",
    "            head_queries = self.q[..., head_selector]\n",
    "            head_keys = self.k[..., head_selector]\n",
    "            head_values = self.v[..., head_selector]\n",
    "\n",
    "            single_head_output = self._attention_for_head(\n",
    "                head_queries, head_keys, head_values,\n",
    "                is_causal=True)\n",
    "            head_outputs.append(single_head_output)\n",
    "\n",
    "        combined_head_outputs = np.concatenate(head_outputs, axis=-1)\n",
    "        return self.c_proj(combined_head_outputs)\n",
    "\n",
    "    def _attention_for_head(self, query, key, value, is_causal=False):\n",
    "        L, S = query.shape[-2], key.shape[-2]\n",
    "        self.attn_bias = np.zeros((L, S), dtype=query.dtype)\n",
    "        if is_causal:\n",
    "            temp_mask = np.tril(np.ones((L, S)))\n",
    "            self.attn_bias = np.where(temp_mask==0, float('-inf'), 0)\n",
    "            self.attn_bias = self.attn_bias.astype(query.dtype)\n",
    "    \n",
    "        attn_weight = query @ key.transpose(0, 2, 1) / self.scale\n",
    "        attn_weight += self.attn_bias\n",
    "        \n",
    "        attn_weight = self.softmax(attn_weight)\n",
    "        self.attentions.append(attn_weight)\n",
    "        \n",
    "        result = attn_weight @ value\n",
    "\n",
    "        # result = self.softmax((query @ key.transpose(0, 2, 1) / self.scale) + self.attn_bias) @ value\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _attention_for_head_grad(self, query, key, value, output_error, head_n):\n",
    "        # looking at the result expression in the forward pass\n",
    "        cur_attention = self.attentions[head_n]\n",
    "        \n",
    "        v_grad = cur_attention.transpose((0, 2, 1)) @ output_error\n",
    "        input_error = output_error @ value.transpose((0, 2, 1))\n",
    "\n",
    "        # we use layer, which uses it's states to calculate grad, so we need to set state that was used during forward pass in current head\n",
    "        self.softmax.forward_result = cur_attention\n",
    "        softmax_grad = self.softmax.backward(input_error)\n",
    "\n",
    "        k_grad = softmax_grad.transpose(0, 2, 1) @ query / self.scale # we need to transpose output error due to the fact key was transposed in forward\n",
    "        q_grad = softmax_grad @ key / self.scale\n",
    "\n",
    "        return q_grad, k_grad, v_grad\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        projection_error = self.c_proj.backward(output_error)\n",
    "        \n",
    "        q_grads, k_grads, v_grads = [], [], []\n",
    "        for head_index in range(self.n_heads):\n",
    "            # for each head we choose it's error\n",
    "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
    "\n",
    "            attention_error = projection_error[..., head_selector]\n",
    "            head_queries = self.q[..., head_selector]\n",
    "            head_keys = self.k[..., head_selector]\n",
    "            head_values = self.v[..., head_selector]\n",
    "\n",
    "            q_grad, k_grad, v_grad = self._attention_for_head_grad(head_queries, head_keys, head_values, attention_error, head_index)\n",
    "            q_grads.append(q_grad)\n",
    "            k_grads.append(k_grad)\n",
    "            v_grads.append(v_grad)\n",
    "\n",
    "        q_grads = np.concatenate(q_grads, axis=-1)\n",
    "        k_grads = np.concatenate(k_grads, axis=-1)\n",
    "        v_grads = np.concatenate(v_grads, axis=-1)\n",
    "\n",
    "        q_k_v_output = np.concatenate([q_grads, k_grads, v_grads], axis=-1)\n",
    "        \n",
    "        input_error = self.c_attn.backward(q_k_v_output)\n",
    "        return input_error\n",
    "\n",
    "        \n",
    "class LayerNorm(BaseLayer):\n",
    "    def __init__(self, dim, eps=1e-5, elementwise_affine=True, bias=True):\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        self.use_bias = bias\n",
    "\n",
    "        self.input = None\n",
    "        self.normalized = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "\n",
    "        self.weight = np.ones(shape=dim)\n",
    "        self.bias = np.zeros(shape=dim)\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.weight_optimizer = copy.copy(optimizer)\n",
    "        self.bias_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.weight_optimizer.set_weight(self.weight)\n",
    "        self.bias_optimizer.set_weight(self.bias)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        \"\"\"Only for -1 axis\"\"\"\n",
    "        self.input = x\n",
    "\n",
    "        self.mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        self.var = np.var(x, axis=-1, keepdims=True, ddof=0)\n",
    "        self.x_centered = (x - self.mean)\n",
    "        self.std = np.sqrt(self.var+self.eps)\n",
    "\n",
    "        self.normalized = self.x_centered / self.std\n",
    "\n",
    "        result = self.normalized * self.weight + self.bias\n",
    "        return result\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        weight_grad = None\n",
    "        bias_grad = None\n",
    "        if self.elementwise_affine:\n",
    "            weight_grad = np.sum(output_error * self.normalized, axis=(0, 1))\n",
    "\n",
    "        if self.use_bias:\n",
    "            bias_grad = np.sum(output_error, axis=(0, 1))\n",
    "\n",
    "        # производная по normalized\n",
    "        dldx = output_error * self.weight\n",
    "        # производная по дисперсии в знаменателе\n",
    "        dldsigma2 = np.sum(dldx * self.x_centered * (-1/2) * (self.std ** -3), axis=-1, keepdims=True)\n",
    "        dldmu = np.sum((dldx * (-1/self.std)), axis=-1, keepdims=True) + dldsigma2\\\n",
    "        * np.sum(-2*self.x_centered, axis=-1, keepdims=True) / self.dim\n",
    "\n",
    "        input_error = dldx / self.std + dldsigma2 * 2 * self.x_centered / self.dim + dldmu / self.dim\n",
    "\n",
    "        self.weight = self.weight_optimizer.step(weight_grad)\n",
    "        self.bias = self.bias_optimizer.step(bias_grad)\n",
    "        return input_error\n",
    "\n",
    "\n",
    "class FullyConnected(BaseLayer):\n",
    "    def __init__(self, dim):\n",
    "        self.c_fc = Linear3d(dim, 4*dim)\n",
    "        self.relu = Activation(relu, relu_derivative)\n",
    "        self.c_proj = Linear3d(4*dim, dim)\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.c_fc.set_optimizer(optimizer)\n",
    "        self.c_proj.set_optimizer(optimizer)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        outputs = self.c_fc(x)\n",
    "\n",
    "        outputs = self.relu(outputs)\n",
    "\n",
    "        outputs = self.c_proj(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        c_proj_error = self.c_proj.backward(output_error)\n",
    "        relu_error = self.relu.backward(c_proj_error)\n",
    "        c_fc_error = self.c_fc.backward(relu_error)\n",
    "\n",
    "        return c_fc_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e5f3f-c1c8-480a-9b28-e8760db854cc",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/1sq2vHO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fedaa-e7c5-42bf-b5cd-fb4eb9cede63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(BaseLayer):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        self.ln_1 = LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttentionLayer(dim, num_heads)\n",
    "        self.ln_2 = LayerNorm(dim)\n",
    "        self.mlp = FullyConnected(dim)\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.ln_1.set_optimizer(optimizer)\n",
    "        self.attn.set_optimizer(optimizer)\n",
    "        self.ln_2.set_optimizer(optimizer)\n",
    "        self.mlp.set_optimizer(optimizer)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        output1 = self.ln_1(x)\n",
    "        output1 = self.attn(output1)\n",
    "        output1 = output1 + x\n",
    "\n",
    "        output2 = self.ln_2(output1)\n",
    "        output2 = self.mlp(output2)\n",
    "        output = output2 + output1\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # output = output1 + output2 + x\n",
    "        # doutput_doutput2 = 1 и надо это умножить на output_error\n",
    "        mlp_error = self.mlp.backward(output_error)\n",
    "        ln2_error = self.ln_2.backward(mlp_error)\n",
    "        # doutput_doutput1 = 1 и надо это умножить на output_error + ошибка, которая пришла выше\n",
    "        \n",
    "        attn_error = self.attn.backward(ln2_error+output_error) \n",
    "        ln1_error = self.ln_1.backward(attn_error)\n",
    "        # doutput_dx = 1 и надо это умножить на output_error + ошибка, которая пришла выше\n",
    "        \n",
    "        return ln1_error+ln2_error+output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "443034d3-7f90-468f-87b6-792626ce15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z: Union[np.array, float, int, list]) -> Union[np.array, float, int]:\n",
    "    \"\"\"\n",
    "    ReLU function\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def relu_derivative(z: Union[np.array, float, int, list]) -> np.array:\n",
    "    \"\"\"\n",
    "    ReLU function derivative\n",
    "    \"\"\"\n",
    "    return (z > 0).astype(int)\n",
    "\n",
    "def cross_entropy_loss(y_true: np.array, a_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    arg = -a_pred[lenght_y, y_true.ravel()]\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1)\n",
    "    loss = np.sum(arg + np.log(sum_exp))\n",
    "    return loss / len(y_true)\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true: np.array, a_pred: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss derivative for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: np.array with shape [n_samples, n_classes] with CrossEntropyLoss derivatives for each weight\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1).reshape(-1, 1)\n",
    "    loss = np.exp(a_pred.copy()) / sum_exp\n",
    "    loss[lenght_y, y_true.ravel()] -= 1\n",
    "\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb537e-e532-4403-a74b-49a66ee81db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOptimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ADAM(BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm.\n",
    "\n",
    "    learning_rate (float, optional) – learning rate (default: 1e-3)\n",
    "    beta1, beta2 (Tuple[float, float], optional) –\n",
    "    coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "    eps (float, optional) – term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8,\n",
    "                 learning_rate: float = 3e-4, weight_decay: float = 0) -> None:\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.EMA1 = None\n",
    "        self.EMA2 = None\n",
    "\n",
    "        self.weight = None\n",
    "\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        self.weight = weight.copy()\n",
    "        self.EMA1 = np.zeros(shape=self.weight.shape)\n",
    "        self.EMA2 = np.zeros(shape=self.weight.shape)\n",
    "\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        assert self.weight is not None, 'You should set the weight'\n",
    "        grad = grad.copy() + self.weight_decay * self.weight\n",
    "        self.EMA1 = (1 - self.beta1) * grad + self.beta1 * self.EMA1\n",
    "        self.EMA2 = (1 - self.beta2) * grad ** 2 + self.beta2 * self.EMA2\n",
    "        self.weight -= self.learning_rate * self.EMA1 / (np.sqrt(self.EMA2) + self.eps)\n",
    "\n",
    "        return self.weight.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a771ed47-3352-4420-906c-159b82bc4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT:\n",
    "    def __init__(self, vocab_size: int, dim: int, num_heads: int, num_layers: int, max_position_embeddings: int = 256):\n",
    "        self.wte = Embedding(vocab_size, dim)  # token embeddings\n",
    "        self.wpe = Embedding(max_position_embeddings, dim)  # position embeddings\n",
    "        self.ln_f = LayerNorm(dim)   # final layer norm - goes after all transformer layers, but before logits\n",
    "\n",
    "        self.h = [Transformer(dim, num_heads) for layer in range(num_layers)]\n",
    "        self.to_logits = Linear3d(dim, vocab_size)\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.wte.set_optimizer(optimizer)\n",
    "        self.wpe.set_optimizer(optimizer)\n",
    "        self.ln_f.set_optimizer(optimizer)\n",
    "        self.to_logits.set_optimizer(optimizer)\n",
    "\n",
    "        for layer in self.h:\n",
    "            layer.set_optimizer(optimizer)\n",
    "\n",
    "    def __call__(self, input_ids):\n",
    "        return self.forward(input_ids)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids.shape: [batch_size, sequence_length], int64 token ids\n",
    "        position_ids = np.arange(input_ids.shape[1])[None, ...]\n",
    "\n",
    "        token_embeddings = self.wte(input_ids)\n",
    "        position_embeddings = self.wpe(position_ids)\n",
    "        full_embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "        transformer_output = full_embeddings\n",
    "        for transformer_layer in self.h:\n",
    "            transformer_output = transformer_layer(transformer_output)\n",
    "        transformer_output_ln = self.ln_f(transformer_output)\n",
    "\n",
    "        # final layer: we predict logits by re-using token embeddings as linear weights\n",
    "        output_logits = self.to_logits(transformer_output_ln)\n",
    "        return output_logits\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        logits_error = self.to_logits.backward(output_error)\n",
    "\n",
    "        transformer_error = logits_error\n",
    "        for transformer_layer in reversed(self.h):\n",
    "            transformer_error = transformer_layer.backward(transformer_error)\n",
    "\n",
    "        self.wte.backward(transformer_error)\n",
    "        self.wpe.backward(transformer_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860325a4-2178-4fd7-8686-db6ffc38caa6",
   "metadata": {},
   "source": [
    "# Training transformer might be tricky\n",
    "![](./images/how_to_train_lm.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3a59c47-2dbe-49fa-8167-fd6fa8948d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "\n",
    "optimizer = ADAM(learning_rate=0.001)\n",
    "model = GPT(vocab_size=len(token_to_id), dim=dim, num_heads=num_heads, num_layers=num_layers)\n",
    "model.set_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fff351-a921-4290-9d4d-84b10149b0e7",
   "metadata": {},
   "source": [
    "![](./images/nan.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6a273b1-7fac-4685-888f-1e81222391bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_history = []\n",
    "for i in range(5000):\n",
    "    batch_ix = to_matrix(np.random.choice(lines, size=batch_size, replace=False), token_to_id, max_len=MAX_LENGTH)\n",
    "    \n",
    "    pred = model(batch_ix[:, :-1])\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(batch_ix.shape[1]-1):\n",
    "        loss += cross_entropy_loss(batch_ix[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "        \n",
    "    errors = np.zeros(shape=(batch_size, MAX_LENGTH-1, len(token_to_id)))\n",
    "    for t in range(errors.shape[1]-1):\n",
    "        errors[:, t, :] = cross_entropy_loss_derivative(batch_ix[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "    \n",
    "    err = model.backward(errors)\n",
    "    \n",
    "    # visualizing training process\n",
    "    assert not np.isnan(loss), 'loss is nan'\n",
    "    train_history.append((i, loss / batch_size))\n",
    "    if (i + 1) % 10 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
    "        plt.legend(); plt.grid(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "638829d2-64f5-431e-8102-692fdcff7f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 2.7049051480273785)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "477941c9-fe77-4c18-baca-32c7ac33cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = SoftMaxLayer3D()\n",
    "def generate_sample(model, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    phrase = copy.copy(seed_phrase)\n",
    "    \n",
    "    for t in range(len(seed_phrase)-1, max_length-len(seed_phrase)):\n",
    "        x_sequence = to_matrix([phrase], token_to_id, max_len=max_length)\n",
    "\n",
    "        pred = model(x_sequence)\n",
    "        probs = softmax(pred[:, t] / temperature).ravel()\n",
    "        next_ix = np.random.choice(len(tokens), p=probs)\n",
    "        phrase += tokens[next_ix]\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbb95473-d4d0-438f-9eaf-6579f90b0f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Sent Algorithm for Expert Semantics ; The problem of a sensore of the problem of unital probabilistic interencests of solve eva\n",
      " Contextual Reduction Semantics ; Deep Learning algorithm for image the sensors of the longuage specification of deep neural netwa\n",
      " A Conving of Distributed Sentence Memory ; The of a new with attention main maps of an application machine of a propagation betwe\n",
      " Computational Inference for Optimization of Words ; We sensitive the the study of probabilistic process with a semantic sensity s\n",
      " A Spatial Attentive Deep Learning for Sensor Spectral Terministic   Segmentation ; The predict of the is the imilar as a pose ari\n",
      " Proception of Modular Continuous of Subspectanal Scalares ; This paper presents the a serieval proposal with an algorithm for a a\n",
      " Automatic Discussive Avarial Networks ; We present a in the bailmited of the controllect unifiered for related of processing thea\n",
      " Learning an State-Sembetic Between Accuracy Model ; We present of the problem of setting an algorithm for the a classification or\n",
      " Adversarial Recurrent Compression Using a Optimal Restrietic Networks ; In this paper, we propose a new an between accurators ane\n",
      " Manifold Text Convolutional Networks ; We propose a population are framework for the problem of parants a representation of the s\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(model, seed_phrase=' ', temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51313657-4be6-49d0-b742-75f614e05f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_39",
   "language": "python",
   "name": "data_science_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
