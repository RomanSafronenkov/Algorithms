{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbda81a",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16265124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union\n",
    "from IPython.display import HTML, clear_output\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4455dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS, EOS = ' ', '\\n'\n",
    "\n",
    "data = pd.read_json(\"./data/arxivData.json\")\n",
    "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:128], axis=1) \\\n",
    "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
    "            .tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54e6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens =  136\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(''.join(lines)))\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print('num_tokens = ', num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b1e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 0,\n",
       " 'W': 1,\n",
       " 'Ł': 2,\n",
       " 'Z': 3,\n",
       " '=': 4,\n",
       " '*': 5,\n",
       " 'Σ': 6,\n",
       " 'L': 7,\n",
       " '&': 8,\n",
       " 'q': 9,\n",
       " 'K': 10,\n",
       " 'ó': 11,\n",
       " 'g': 12,\n",
       " 'ε': 13,\n",
       " '6': 14,\n",
       " ',': 15,\n",
       " '|': 16,\n",
       " 'ã': 17,\n",
       " '5': 18,\n",
       " 'ő': 19,\n",
       " 'Q': 20,\n",
       " '$': 21,\n",
       " 'J': 22,\n",
       " 'i': 23,\n",
       " 'σ': 24,\n",
       " 'í': 25,\n",
       " 'ρ': 26,\n",
       " 'l': 27,\n",
       " 'u': 28,\n",
       " '\\x7f': 29,\n",
       " 'P': 30,\n",
       " 'Ö': 31,\n",
       " 'm': 32,\n",
       " 'a': 33,\n",
       " '\\\\': 34,\n",
       " 'w': 35,\n",
       " 'I': 36,\n",
       " '?': 37,\n",
       " 'z': 38,\n",
       " '°': 39,\n",
       " '/': 40,\n",
       " '9': 41,\n",
       " 'c': 42,\n",
       " 'É': 43,\n",
       " '4': 44,\n",
       " 'M': 45,\n",
       " '!': 46,\n",
       " 'μ': 47,\n",
       " 'á': 48,\n",
       " 'à': 49,\n",
       " '{': 50,\n",
       " 'V': 51,\n",
       " 's': 52,\n",
       " ':': 53,\n",
       " 'R': 54,\n",
       " '-': 55,\n",
       " 'v': 56,\n",
       " 'O': 57,\n",
       " '^': 58,\n",
       " '>': 59,\n",
       " '<': 60,\n",
       " 'ô': 61,\n",
       " ';': 62,\n",
       " 'x': 63,\n",
       " 'ö': 64,\n",
       " 'ê': 65,\n",
       " '2': 66,\n",
       " '(': 67,\n",
       " 'n': 68,\n",
       " '+': 69,\n",
       " '`': 70,\n",
       " 'E': 71,\n",
       " 'y': 72,\n",
       " 'λ': 73,\n",
       " 'X': 74,\n",
       " 'β': 75,\n",
       " ']': 76,\n",
       " ' ': 77,\n",
       " 'Π': 78,\n",
       " 'ü': 79,\n",
       " '\\n': 80,\n",
       " '#': 81,\n",
       " 'U': 82,\n",
       " '~': 83,\n",
       " 'ç': 84,\n",
       " 'S': 85,\n",
       " 'ν': 86,\n",
       " 'γ': 87,\n",
       " 'F': 88,\n",
       " 'Y': 89,\n",
       " 'p': 90,\n",
       " 'b': 91,\n",
       " 'D': 92,\n",
       " 'h': 93,\n",
       " ')': 94,\n",
       " 'T': 95,\n",
       " '\"': 96,\n",
       " 'f': 97,\n",
       " 'N': 98,\n",
       " '_': 99,\n",
       " 'τ': 100,\n",
       " '0': 101,\n",
       " 'é': 102,\n",
       " 'C': 103,\n",
       " 'H': 104,\n",
       " \"'\": 105,\n",
       " 'd': 106,\n",
       " 'ä': 107,\n",
       " '@': 108,\n",
       " '[': 109,\n",
       " 'Ü': 110,\n",
       " '%': 111,\n",
       " 'ω': 112,\n",
       " 'o': 113,\n",
       " 'ï': 114,\n",
       " '7': 115,\n",
       " 'è': 116,\n",
       " '8': 117,\n",
       " 'õ': 118,\n",
       " 'â': 119,\n",
       " 'j': 120,\n",
       " 'A': 121,\n",
       " 'B': 122,\n",
       " 'χ': 123,\n",
       " '3': 124,\n",
       " '.': 125,\n",
       " 'æ': 126,\n",
       " '1': 127,\n",
       " 'ś': 128,\n",
       " '}': 129,\n",
       " 'r': 130,\n",
       " 'e': 131,\n",
       " 't': 132,\n",
       " 'α': 133,\n",
       " 'Ω': 134,\n",
       " 'G': 135}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185191cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems alright!\n"
     ]
    }
   ],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e75a93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first=True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, data))\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[c] for c in data[i]]\n",
    "        data_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        data_ix = np.transpose(data_ix)\n",
    "\n",
    "    return data_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537348a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dual Recurrent Attention Units for Visual Question Answering ; We propose an architecture for VQA which utilizes recurrent layer\n",
      "\n",
      " Exploring Deep and Recurrent Architectures for Optimal Control ; Sophisticated multilayer neural networks have achieved state of\n",
      "\n",
      " Space and camera path reconstruction for omni-directional vision ; In this paper, we address the inverse problem of reconstructi\n",
      "\n",
      " Provable Algorithms for Inference in Topic Models ; Recently, there has been considerable progress on designing algorithms with \n",
      "\n",
      " Fixing Weight Decay Regularization in Adam ; L$_2$ regularization and weight decay regularization are equivalent for standard st\n",
      "\n",
      " Query-Efficient Imitation Learning for End-to-End Autonomous Driving ; One way to approach end-to-end autonomous driving is to l\n",
      "\n",
      " Multiple Kernel Learning and Automatic Subspace Relevance Determination   for High-dimensional Neuroimaging Data ; Alzheimer's d\n",
      "\n",
      " A Search for Improved Performance in Regular Expressions ; The primary aim of automated performance improvement is to reduce the\n",
      "\n",
      " Solving General Arithmetic Word Problems ; This paper presents a novel approach to automatically solving arithmetic word problem\n",
      "\n",
      " BDD-based reasoning in the fluent calculus - first results ; The paper reports on first preliminary results and insights gained \n",
      "\n",
      " Balancing bike sharing systems (BBSS): instance generation from the   CitiBike NYC data ; Bike sharing systems are a very popula\n",
      "\n",
      " Variational Bayesian inference for linear and logistic regression ; The article describe the model, derivation, and implementati\n",
      "\n",
      " Fusion of Range and Thermal Images for Person Detection ; Detecting people in images is a challenging problem. Differences in po\n",
      "\n",
      " Spontaneous Facial Micro-Expression Recognition using Discriminative   Spatiotemporal Local Binary Pattern with an Improved Inte\n",
      "\n",
      " CAD Priors for Accurate and Flexible Instance Reconstruction ; We present an efficient and automatic approach for accurate recon\n",
      "\n",
      " An Automatic Diagnosis Method of Facial Acne Vulgaris Based on   Convolutional Neural Network ; In this paper, we present a new \n",
      "\n",
      " Bayesian Multitask Learning with Latent Hierarchies ; We learn multiple hypotheses for related tasks under a latent hierarchical\n",
      "\n",
      " PACE: Pattern Accurate Computationally Efficient Bootstrapping for   Timely Discovery of Cyber-Security Concepts ; Public disclo\n",
      "\n",
      " Quantum decision theory as quantum theory of measurement ; We present a general theory of quantum information processing devices\n",
      "\n",
      " Fast Robust Methods for Singular State-Space Models ; State-space models are used in a wide range of time series analysis formul\n",
      "\n",
      " Objectness Scoring and Detection Proposals in Forward-Looking Sonar   Images with Convolutional Neural Networks ; Forward-lookin\n",
      "\n",
      "[[ 77  92  28 ... 131 130  80]\n",
      " [ 77  71  63 ... 113  97  80]\n",
      " [ 77  85  90 ... 132  23  80]\n",
      " ...\n",
      " [ 77  20  28 ... 131  52  80]\n",
      " [ 77  88  33 ...  28  27  80]\n",
      " [ 77  57  91 ...  23  68  80]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(lines[::2000]))\n",
    "print(to_matrix(lines[::2000], token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfba1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, lines))\n",
    "sample = to_matrix(np.random.choice(lines, size=5), token_to_id, max_len=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c90cd4d-85b7-404b-8049-eee1fc3a20d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6fa5bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 130, 136)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy\n",
    "encoded_data = np.zeros(shape=(5, MAX_LENGTH, len(token_to_id)))\n",
    "\n",
    "for text_i, text in enumerate(encoded_data):\n",
    "    for letter_i, letter in enumerate(text):\n",
    "        encoded_data[text_i, letter_i, sample[text_i, letter_i]] = 1\n",
    "\n",
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d61fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        return self.forward(x, grad)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(BaseLayer):\n",
    "    \"\"\"\n",
    "    Linear class permorms ordinary FC layer in neural networks\n",
    "    Parameters:\n",
    "    n_input - size of input neurons\n",
    "    n_output - size of output neurons\n",
    "    Methods:\n",
    "    set_optimizer(optimizer) - is used for setting an optimizer for gradient descent\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input: int, n_output: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.w = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(n_input, n_output))\n",
    "        self.b = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(1, n_output))\n",
    "\n",
    "        self.w_optimizer = None\n",
    "        self.b_optimizer = None\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.w_optimizer = copy.copy(optimizer)\n",
    "        self.b_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.w_optimizer.set_weight(self.w)\n",
    "        self.b_optimizer.set_weight(self.b)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return x.dot(self.w) + self.b\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        assert self.w_optimizer is not None and self.b_optimizer is not None, 'You should set an optimizer'\n",
    "        w_grad = self.input.T.dot(output_error)\n",
    "        b_grad = np.ones((1, len(output_error))).dot(output_error)\n",
    "        input_error = output_error.dot(self.w.T)\n",
    "\n",
    "        self.w = self.w_optimizer.step(w_grad)\n",
    "        self.b = self.b_optimizer.step(b_grad)\n",
    "        return input_error\n",
    "\n",
    "\n",
    "class Activation(BaseLayer):\n",
    "    \"\"\"\n",
    "    Activation class is used for activation function of the FC layer\n",
    "    Params:\n",
    "    activation_function - activation function (e.g. sigmoid, RElU, tanh)\n",
    "    activation_derivative - derivative of the activation function\n",
    "    Methods:\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_function: callable, activation_derivative: callable) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.activation = activation_function\n",
    "        self.derivative = activation_derivative\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return self.activation(x)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return output_error * self.derivative(self.input)\n",
    "\n",
    "class BaseOptimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "class ADAM(BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm.\n",
    "\n",
    "    learning_rate (float, optional) – learning rate (default: 1e-3)\n",
    "    beta1, beta2 (Tuple[float, float], optional) –\n",
    "    coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "    eps (float, optional) – term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8,\n",
    "                 learning_rate: float = 3e-4, weight_decay: float = 0) -> None:\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.EMA1 = None\n",
    "        self.EMA2 = None\n",
    "\n",
    "        self.weight = None\n",
    "\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        self.weight = weight.copy()\n",
    "        self.EMA1 = np.zeros(shape=self.weight.shape)\n",
    "        self.EMA2 = np.zeros(shape=self.weight.shape)\n",
    "\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        assert self.weight is not None, 'You should set the weight'\n",
    "        grad = grad.copy() + self.weight_decay * self.weight\n",
    "        self.EMA1 = (1 - self.beta1) * grad + self.beta1 * self.EMA1\n",
    "        self.EMA2 = (1 - self.beta2) * grad ** 2 + self.beta2 * self.EMA2\n",
    "        self.weight -= self.learning_rate * self.EMA1 / (np.sqrt(self.EMA2) + self.eps)\n",
    "\n",
    "        return self.weight.copy()\n",
    "\n",
    "class Embedding(BaseLayer):\n",
    "    def __init__(self, n_input, emb_dim, pad_idx=None):\n",
    "        self.n_input = n_input\n",
    "        self.emb_dim = emb_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.weights = np.random.normal(scale=np.sqrt(2/(n_input+emb_dim)), size=(n_input, emb_dim))\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.weights_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.weights_optimizer.set_weight(self.weights)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        self.input = x\n",
    "        return self.weights[x]\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        weights_grad = np.zeros_like(self.weights)\n",
    "        input_shape_len = len(self.input.shape)\n",
    "\n",
    "        if input_shape_len == 2:\n",
    "            for batch_n, s in enumerate(self.input):\n",
    "                for i, emb_i in enumerate(s):\n",
    "                    weights_grad[emb_i] += output_error[batch_n][i]\n",
    "\n",
    "        elif input_shape_len == 1:\n",
    "            for i, emb_i in enumerate(self.input):\n",
    "                weights_grad[emb_i] += output_error[i]\n",
    "\n",
    "        if self.pad_idx is not None:\n",
    "            weights_grad[self.pad_idx] = 0\n",
    "\n",
    "        self.weights = self.weights_optimizer.step(weights_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6375dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Tanh function\n",
    "    \"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Tanh function derivative\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "def sigmoid(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Sigmoid function derivative\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(z: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \"\"\"\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1, 1)\n",
    "\n",
    "def cross_entropy_loss(y_true: np.array, a_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    arg = -a_pred[lenght_y, y_true.ravel()]\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1)\n",
    "    loss = np.sum(arg + np.log(sum_exp))\n",
    "    return loss / len(y_true)\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true: np.array, a_pred: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss derivative for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: np.array with shape [n_samples, n_classes] with CrossEntropyLoss derivatives for each weight\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1).reshape(-1, 1)\n",
    "    loss = np.exp(a_pred.copy()) / sum_exp\n",
    "    loss[lenght_y, y_true.ravel()] -= 1\n",
    "\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04065a",
   "metadata": {},
   "source": [
    "![](./images/lstm.png)\n",
    "![](./images/graph_meanings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627b9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input,\n",
    "        n_hidden,\n",
    "        n_output,\n",
    "        bptt_trunc=4\n",
    "    ):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.bptt_trunc = bptt_trunc\n",
    "        \n",
    "        self.forget_gate = Linear(n_input + n_hidden, n_hidden)\n",
    "        self.forget_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        self.input_layer_gate = Linear(n_input + n_hidden, n_hidden)\n",
    "        self.input_layer_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        self.candidate_gate = Linear(n_input + n_hidden, n_hidden)\n",
    "        self.candidate_gate_activation = Activation(tanh, tanh_derivative)\n",
    "        self.output_gate = Linear(n_input + n_hidden, n_hidden)\n",
    "        self.output_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        self.output_to_logits = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.x_and_h = None\n",
    "        self.hidden_states = None\n",
    "        self.cell_states = None\n",
    "        \n",
    "        self.forget_inputs = None\n",
    "        self.forget_states = None\n",
    "        \n",
    "        self.input_inputs = None\n",
    "        self.input_states = None\n",
    "        \n",
    "        self.candidate_inputs = None\n",
    "        self.candidate_states = None\n",
    "        \n",
    "        self.output_input = None\n",
    "        self.output_states = None\n",
    "        \n",
    "        self.outputs = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.forget_gate.set_optimizer(optimizer)\n",
    "        self.input_layer_gate.set_optimizer(optimizer)\n",
    "        self.candidate_gate.set_optimizer(optimizer)\n",
    "        self.output_gate.set_optimizer(optimizer)\n",
    "        self.output_to_logits.set_optimizer(optimizer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        batch_size, timesteps, input_dim = x.shape\n",
    "        \n",
    "        self.x_and_h = np.zeros((batch_size, timesteps, self.n_input+self.n_hidden))\n",
    "        self.hidden_states = np.zeros((batch_size, timesteps+1, self.n_hidden))\n",
    "        self.cell_states = np.zeros((batch_size, timesteps+1, self.n_hidden))\n",
    "        \n",
    "        self.forget_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.forget_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.input_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.input_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.candidate_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.candidate_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.output_input = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.output_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.outputs = np.zeros((batch_size, timesteps, self.n_output))\n",
    "\n",
    "        self.hidden_states[:, -1] = np.zeros((batch_size, self.n_hidden))\n",
    "        self.cell_states[:, -1] = np.zeros((batch_size, self.n_hidden))\n",
    "        for t in range(timesteps):\n",
    "            # соединяем вход и прошлый h\n",
    "            self.x_and_h[:, t] = np.concatenate((x[:, t], self.hidden_states[:, t-1]), axis=1)\n",
    "            \n",
    "            # forget gate проход\n",
    "            self.forget_inputs[:, t] = self.forget_gate(self.x_and_h[:, t])\n",
    "            self.forget_states[:, t] = self.forget_gate_activation(self.forget_inputs[:, t])\n",
    "            \n",
    "            # выбор кандидатов для C\n",
    "            self.input_inputs[:, t] = self.input_layer_gate(self.x_and_h[:, t])\n",
    "            self.input_states[:, t] = self.input_layer_gate_activation(self.input_inputs[:, t])\n",
    "            \n",
    "            self.candidate_inputs[:, t] = self.candidate_gate(self.x_and_h[:, t])\n",
    "            self.candidate_states[:, t] = self.candidate_gate_activation(self.candidate_inputs[:, t])\n",
    "            \n",
    "            self.cell_states[:, t] = self.forget_states[:, t] * self.cell_states[:, t-1]\\\n",
    "            + self.input_states[:, t] * self.candidate_states[:, t]\n",
    "            \n",
    "            self.output_input[:, t] = self.output_gate(self.x_and_h[:, t])\n",
    "            self.output_states[:, t] = self.output_gate_activation(self.output_input[:, t])\n",
    "            \n",
    "            self.hidden_states[:, t] = self.output_states[:, t] * tanh(self.cell_states[:, t])\n",
    "            \n",
    "            # дополнительный слой, не указан на рисунке, для перевода состояния в логиты выхода\n",
    "            self.outputs[:, t] = self.output_to_logits(self.hidden_states[:, t])\n",
    "\n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, output_error):        \n",
    "        _, timesteps, _ = output_error.shape\n",
    "\n",
    "        input_error = np.zeros_like(self.input)\n",
    "        for t in np.arange(timesteps)[::-1]:\n",
    "            # в разные моменты времени у слоев был разный вход, необходимо искусственно его поменять\n",
    "            self.forget_gate.input = self.x_and_h[:, t]\n",
    "            self.forget_gate_activation.input = self.forget_inputs[:, t]\n",
    "            self.input_layer_gate.input = self.x_and_h[:, t]\n",
    "            self.input_layer_gate_activation.input = self.input_inputs[:, t]\n",
    "            self.candidate_gate.input = self.x_and_h[:, t]\n",
    "            self.candidate_gate_activation.input = self.candidate_inputs[:, t]\n",
    "            self.output_gate.input = self.x_and_h[:, t]\n",
    "            self.output_gate_activation.input = self.output_input[:, t]\n",
    "            self.output_to_logits.input = self.hidden_states[:, t]\n",
    "            \n",
    "            # проход по нижнему уровню\n",
    "            hidden_error = self.output_to_logits.backward(output_error[:, t])\n",
    "            # та, что идет вверх\n",
    "            cell_error = tanh_derivative(self.cell_states[:, t]) * self.output_states[:, t] * hidden_error\n",
    "            # ошибка идет и вниз и вверх\n",
    "            hidden_error = self.output_gate_activation.backward(hidden_error) * tanh(self.cell_states[:, t])\n",
    "            # та, что идет вниз\n",
    "            hidden_error = self.output_gate.backward(hidden_error)\n",
    "\n",
    "            \n",
    "            # идем по верху\n",
    "            hidden_candidate_error = self.candidate_gate_activation.backward(cell_error) * self.input_states[:, t]\n",
    "            hidden_candidate_error = self.candidate_gate.backward(hidden_candidate_error)\n",
    "            \n",
    "            hidden_inputs_error = self.input_layer_gate_activation.backward(cell_error) * self.candidate_states[:, t]\n",
    "            hidden_inputs_error = self.input_layer_gate.backward(hidden_inputs_error)\n",
    "            \n",
    "            hidden_forget_error = self.forget_gate_activation.backward(cell_error) * self.cell_states[:, t-1]\n",
    "            hidden_forget_error = self.forget_gate.backward(hidden_forget_error)\n",
    "            \n",
    "            # добавляются ошибки с мест копии\n",
    "            hidden_error += hidden_candidate_error\n",
    "            hidden_error += hidden_inputs_error\n",
    "            hidden_error += hidden_forget_error\n",
    "\n",
    "            # ошибка входа\n",
    "            input_error[:, t] = hidden_error[:, :self.n_input]\n",
    "            # ошибка, которая по времени уходит по низу назад\n",
    "            hidden_error = hidden_error[:, self.n_input:]\n",
    "            # ошибка, которая по времени уходит по верху назад\n",
    "            cell_error = cell_error * self.forget_states[:, t]\n",
    "            \n",
    "            for t_ in np.arange(max(0, t - self.bptt_trunc), t)[::-1]:\n",
    "                # проход по времени\n",
    "                self.forget_gate.input = self.x_and_h[:, t_]\n",
    "                self.forget_gate_activation.input = self.forget_inputs[:, t_]\n",
    "                self.input_layer_gate.input = self.x_and_h[:, t_]\n",
    "                self.input_layer_gate_activation.input = self.input_inputs[:, t_]\n",
    "                self.candidate_gate.input = self.x_and_h[:, t_]\n",
    "                self.candidate_gate_activation.input = self.candidate_inputs[:, t_]\n",
    "                self.output_gate.input = self.x_and_h[:, t_]\n",
    "                self.output_gate_activation.input = self.output_input[:, t_]\n",
    "                \n",
    "                # та, что идет по верху\n",
    "                cell_error += tanh_derivative(self.cell_states[:, t_]) * self.output_states[:, t_]\\\n",
    "                * hidden_error\n",
    "                \n",
    "                hidden_error = self.output_gate_activation.backward(hidden_error) * tanh(self.cell_states[:, t_])\n",
    "                # та, что идет вниз\n",
    "                hidden_error = self.output_gate.backward(hidden_error)\n",
    "                \n",
    "                hidden_candidate_error = self.candidate_gate_activation.backward(cell_error) * self.input_states[:, t_]\n",
    "                hidden_candidate_error = self.candidate_gate.backward(hidden_candidate_error)\n",
    "\n",
    "                hidden_inputs_error = self.input_layer_gate_activation.backward(cell_error) * self.candidate_states[:, t_]\n",
    "                hidden_inputs_error = self.input_layer_gate.backward(hidden_inputs_error)\n",
    "\n",
    "                hidden_forget_error = self.forget_gate_activation.backward(cell_error) * self.cell_states[:, t_-1]\n",
    "                hidden_forget_error = self.forget_gate.backward(hidden_forget_error)\n",
    "                \n",
    "                # добавляются ошибки с мест копии\n",
    "                hidden_error += hidden_candidate_error\n",
    "                hidden_error += hidden_inputs_error\n",
    "                hidden_error += hidden_forget_error\n",
    "\n",
    "                # ошибка которая по времени уходит по низу назад\n",
    "                hidden_error = hidden_error[:, self.n_input:]\n",
    "                cell_error = cell_error * self.forget_states[:, t_]\n",
    "\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b6d16",
   "metadata": {},
   "source": [
    "![](./images/lstm_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e39fce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 130, 16)\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "emb = Embedding(len(token_to_id), emb_size)\n",
    "\n",
    "encoded_data = emb(sample)\n",
    "print(encoded_data.shape)\n",
    "\n",
    "lstm_unit = LSTM(n_input=emb_size,\n",
    "                 n_hidden=64,\n",
    "                 n_output=len(token_to_id),\n",
    "                 bptt_trunc=15)\n",
    "\n",
    "lstm_unit.set_optimizer(ADAM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d57e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 130, 136)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_unit.forward(encoded_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dafb612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.95990624e-02, -9.71520632e-03,  3.34700326e-03, ...,\n",
       "          1.30230343e-02, -4.77261659e-02, -1.53955901e-03],\n",
       "        [-1.03271889e-01, -4.96441055e-02,  6.23509224e-02, ...,\n",
       "          1.02424136e-01, -2.84650669e-01,  1.68562749e-02],\n",
       "        [-5.28707830e-03,  9.32539449e-04,  9.54039307e-03, ...,\n",
       "          3.12340121e-03, -1.78017845e-03,  4.16816201e-03],\n",
       "        ...,\n",
       "        [-2.81297753e-03, -2.11667539e-03, -1.76698838e-03, ...,\n",
       "         -3.55162981e-03,  2.54524700e-03, -5.26920082e-03],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-2.92847507e-02, -1.74590173e-02,  1.85815298e-02, ...,\n",
       "          1.77227756e-02, -6.94293749e-02, -7.26623039e-04],\n",
       "        [-1.03473049e-01, -5.61172971e-02,  5.58972278e-02, ...,\n",
       "          1.04822190e-01, -3.03586503e-01,  8.55597566e-04],\n",
       "        [-9.10766953e-02, -4.07785565e-02,  4.46251297e-02, ...,\n",
       "          8.10176259e-02, -2.48574885e-01,  1.81762665e-03],\n",
       "        ...,\n",
       "        [-2.89349354e-03,  2.12732730e-03,  9.21337065e-03, ...,\n",
       "         -3.26351605e-03,  6.10746491e-03,  2.20164770e-04],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-1.86043532e-02, -1.27233496e-02,  1.61270320e-02, ...,\n",
       "          1.94581443e-02, -8.04978815e-02, -2.60196963e-03],\n",
       "        [-8.15548016e-02, -3.19269412e-02,  3.64956610e-02, ...,\n",
       "          8.84081966e-02, -2.08327535e-01,  1.49518678e-02],\n",
       "        [-1.05347762e-01, -5.04732717e-02,  6.20679880e-02, ...,\n",
       "          1.06084846e-01, -2.92281540e-01,  1.88627077e-02],\n",
       "        ...,\n",
       "        [-4.07596543e-06, -9.69253066e-03,  4.13857307e-03, ...,\n",
       "          2.41377540e-03,  5.96281535e-03, -1.17499627e-02],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-2.96772658e-02, -2.11436290e-02,  1.52917951e-02, ...,\n",
       "          2.58696723e-02, -7.98174474e-02,  7.64472614e-03],\n",
       "        [-1.44640839e-01, -6.38516212e-02,  5.74278910e-02, ...,\n",
       "          1.24633249e-01, -3.60016528e-01,  3.23129192e-03],\n",
       "        [ 3.36344855e-02,  1.27558158e-02, -1.65793108e-02, ...,\n",
       "         -3.75920112e-02,  8.37419466e-02, -8.02636734e-03],\n",
       "        ...,\n",
       "        [-2.40101043e-03,  1.64624496e-03,  8.94632479e-03, ...,\n",
       "         -3.35659377e-03,  6.15473078e-03,  4.53012665e-04],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-4.48995833e-02, -3.56385338e-02,  2.09525033e-02, ...,\n",
       "          4.27530049e-02, -1.22105705e-01,  1.72309641e-02],\n",
       "        [-1.02322495e-01, -5.37571932e-02,  5.57046168e-02, ...,\n",
       "          1.04004700e-01, -2.98770539e-01,  4.13809238e-03],\n",
       "        [-1.13564416e-01, -5.38812517e-02,  5.48243283e-02, ...,\n",
       "          1.15806884e-01, -3.14910714e-01,  2.06600997e-02],\n",
       "        ...,\n",
       "        [-3.24856347e-03, -2.76034729e-03, -1.64187299e-03, ...,\n",
       "         -3.29605584e-03,  2.51453932e-03, -5.72536662e-03],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lstm_unit(encoded_data)\n",
    "\n",
    "loss = 0\n",
    "for t in range(sample.shape[1]-1):\n",
    "    loss += cross_entropy_loss(sample[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "\n",
    "errors = np.zeros(shape=(5, MAX_LENGTH-1, len(token_to_id)))\n",
    "for t in range(errors.shape[1]-1):\n",
    "    errors[:, t, :] = cross_entropy_loss_derivative(sample[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "\n",
    "lstm_unit.backward(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8ab9b7-5e1d-4f80-b475-c61dd6c24145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Dual Recurrent Attention Units for Visual Question Answering ; We propose an architecture for VQA which utilizes recurrent layer\\n',\n",
       " ' Sequential Short-Text Classification with Recurrent and Convolutional   Neural Networks ; Recent approaches based on artificial \\n',\n",
       " ' Multiresolution Recurrent Neural Networks: An Application to Dialogue   Response Generation ; We introduce the multiresolution r\\n',\n",
       " ' Learning what to share between loosely related tasks ; Multi-task learning is motivated by the observation that humans bring to \\n',\n",
       " ' A Deep Reinforcement Learning Chatbot ; We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Inst\\n',\n",
       " ' Generating Sentences by Editing Prototypes ; We propose a new generative model of sentences that first samples a prototype sente\\n',\n",
       " ' A Deep Reinforcement Learning Chatbot (Short Version) ; We present MILABOT: a deep reinforcement learning chatbot developed by t\\n',\n",
       " ' Document Image Coding and Clustering for Script Discrimination ; The paper introduces a new method for discrimination of documen\\n',\n",
       " ' Tutorial on Answering Questions about Images with Deep Learning ; Together with the development of more accurate methods in Comp\\n',\n",
       " ' pix2code: Generating Code from a Graphical User Interface Screenshot ; Transforming a graphical user interface screenshot create\\n',\n",
       " ' A Unified Deep Neural Network for Speaker and Language Recognition ; Learned feature representations and sub-phoneme posteriors \\n',\n",
       " ' Efficient Neural Architecture Search via Parameter Sharing ; We propose Efficient Neural Architecture Search (ENAS), a fast and \\n',\n",
       " ' Building Machines That Learn and Think Like People ; Recent progress in artificial intelligence (AI) has renewed interest in bui\\n',\n",
       " ' Towards Bayesian Deep Learning: A Survey ; While perception tasks such as visual object recognition and text understanding play \\n',\n",
       " ' Hierarchical Deep Reinforcement Learning: Integrating Temporal   Abstraction and Intrinsic Motivation ; Learning goal-directed b\\n',\n",
       " ' Learning Features by Watching Objects Move ; This paper presents a novel yet intuitive approach to unsupervised feature learning\\n',\n",
       " ' Domain Adaptive Neural Networks for Object Recognition ; We propose a simple neural network model to deal with the domain adapta\\n',\n",
       " ' Beyond Temporal Pooling: Recurrence and Temporal Convolutions for   Gesture Recognition in Video ; Recent studies have demonstra\\n',\n",
       " ' Telugu OCR Framework using Deep Learning ; In this paper, we address the task of Optical Character Recognition(OCR) for the Telu\\n',\n",
       " ' Adversarial Feature Learning ; The ability of the Generative Adversarial Networks (GANs) framework to learn generative models ma\\n',\n",
       " ' The Mythos of Model Interpretability ; Supervised machine learning models boast remarkable predictive capabilities. But can you \\n',\n",
       " ' Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a   Changing World ; In this paper, we focus on online repre\\n',\n",
       " ' Borrowing Treasures from the Wealthy: Deep Transfer Learning through   Selective Joint Fine-tuning ; Deep neural networks requir\\n',\n",
       " ' Aligned Image-Word Representations Improve Inductive Transfer Across   Vision-Language Tasks ; An important goal of computer vis\\n',\n",
       " ' Universal Adversarial Perturbations Against Semantic Image Segmentation ; While deep learning is remarkably successful on percep\\n',\n",
       " ' The loss surface of deep and wide neural networks ; While the optimization problem behind deep neural networks is highly non-con\\n',\n",
       " ' Semantically Decomposing the Latent Spaces of Generative Adversarial   Networks ; We propose a new algorithm for training genera\\n',\n",
       " ' Variants of RMSProp and Adagrad with Logarithmic Regret Bounds ; Adaptive gradient methods have become recently very popular, in\\n',\n",
       " ' ALICE: Towards Understanding Adversarial Learning for Joint Distribution   Matching ; We investigate the non-identifiability iss\\n',\n",
       " ' A systematic study of the class imbalance problem in convolutional   neural networks ; In this study, we systematically investig\\n',\n",
       " ' Regularization for Deep Learning: A Taxonomy ; Regularization is one of the crucial ingredients of deep learning, yet the term r\\n',\n",
       " ' Clustering with Deep Learning: Taxonomy and New Methods ; Clustering is a fundamental machine learning method. The quality of it\\n',\n",
       " ' Coarse to fine non-rigid registration: a chain of scale-specific neural   networks for multimodal image alignment with applicati\\n',\n",
       " ' Describing Videos by Exploiting Temporal Structure ; Recent progress in using recurrent neural networks (RNNs) for image descrip\\n',\n",
       " ' Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in   the Blanks ; Hybrid methods that utilize both content\\n',\n",
       " ' Sentiment Classification using Images and Label Embeddings ; In this project we analysed how much semantic information images ca\\n',\n",
       " ' Natural-Parameter Networks: A Class of Probabilistic Neural Networks ; Neural networks (NN) have achieved state-of-the-art perfo\\n',\n",
       " ' Learning to Perform Physics Experiments via Deep Reinforcement Learning ; When encountering novel objects, humans are able to in\\n',\n",
       " ' A Network-based End-to-End Trainable Task-oriented Dialogue System ; Teaching machines to accomplish tasks by conversing natural\\n',\n",
       " ' A Factorization Machine Framework for Testing Bigram Embeddings in   Knowledgebase Completion ; Embedding-based Knowledge Base C\\n',\n",
       " ' Neural Networks for Joint Sentence Classification in Medical Paper   Abstracts ; Existing models based on artificial neural netw\\n',\n",
       " ' De-identification of Patient Notes with Recurrent Neural Networks ; Objective: Patient notes in electronic health records (EHRs)\\n',\n",
       " ' Reasoning with Memory Augmented Neural Networks for Language   Comprehension ; Hypothesis testing is an important cognitive proc\\n',\n",
       " ' Automatic Rule Extraction from Long Short Term Memory Networks ; Although deep learning models have proven effective at solving \\n',\n",
       " ' Comparing Rule-Based and Deep Learning Models for Patient Phenotyping ; Objective: We investigate whether deep learning techniqu\\n',\n",
       " ' MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks ; Over 50 million scholarly articles have \\n',\n",
       " ' Transfer Learning for Named-Entity Recognition with Neural Networks ; Recent approaches based on artificial neural networks (ANN\\n',\n",
       " ' Adversarial Generation of Natural Language ; Generative Adversarial Networks (GANs) have gathered a lot of attention from the co\\n',\n",
       " ' Explaining Recurrent Neural Network Predictions in Sentiment Analysis ; Recently, a technique called Layer-wise Relevance Propag\\n',\n",
       " ' Text Compression for Sentiment Analysis via Evolutionary Algorithms ; Can textual data be compressed intelligently without losin\\n',\n",
       " ' Building competitive direct acoustics-to-word models for English   conversational speech recognition ; Direct acoustics-to-word \\n',\n",
       " ' Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for   Visual Question Answering ; We address the problem of \\n',\n",
       " ' Task-driven Visual Saliency and Attention-based Visual Question   Answering ; Visual question answering (VQA) has witnessed grea\\n',\n",
       " ' Optimising The Input Window Alignment in CD-DNN Based Phoneme   Recognition for Low Latency Processing ; We present a systematic\\n',\n",
       " ' Bridging LSTM Architecture and the Neural Dynamics during Reading ; Recently, the long short-term memory neural network (LSTM) h\\n',\n",
       " ' Feature Weight Tuning for Recursive Neural Networks ; This paper addresses how a recursive neural network model can automaticall\\n',\n",
       " ' A New Data Representation Based on Training Data Characteristics to   Extract Drug Named-Entity in Medical Text ; One essential \\n',\n",
       " ' DopeLearning: A Computational Approach to Rap Lyrics Generation ; Writing rap lyrics requires both creativity to construct a mea\\n',\n",
       " ' Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN ; Semantic matching, which aims to determine the matching\\n',\n",
       " ' Piecewise Latent Variables for Neural Variational Text Processing ; Advances in neural variational inference have facilitated th\\n',\n",
       " ' Recurrent Neural Networks with External Memory for Language   Understanding ; Recurrent Neural Networks (RNNs) have become incre\\n',\n",
       " ' A Neural Network Approach to Context-Sensitive Generation of   Conversational Responses ; We present a novel response generation\\n',\n",
       " ' The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured   Multi-Turn Dialogue Systems ; This paper introduces t\\n',\n",
       " ' Building End-To-End Dialogue Systems Using Generative Hierarchical   Neural Network Models ; We investigate the task of building\\n',\n",
       " ' End-to-End Attention-based Large Vocabulary Speech Recognition ; Many of the current state-of-the-art Large Vocabulary Continuou\\n',\n",
       " ' Towards Neural Network-based Reasoning ; We propose Neural Reasoner, a framework for neural network-based reasoning over natural\\n',\n",
       " ' What to talk about and how? Selective Generation using LSTMs with   Coarse-to-Fine Alignment ; We propose an end-to-end, domain-\\n',\n",
       " ' Reasoning about Entailment with Neural Attention ; While most approaches to automatically recognizing entailment relations have \\n',\n",
       " ' Highway Long Short-Term Memory RNNs for Distant Speech Recognition ; In this paper, we extend the deep long short-term memory (D\\n',\n",
       " ' Neural Enquirer: Learning to Query Tables with Natural Language ; We proposed Neural Enquirer as a neural network architecture t\\n',\n",
       " ' Sentence Pair Scoring: Towards Unified Framework for Text Comprehension ; We review the task of Sentence Pair Scoring, popular i\\n',\n",
       " ' Incorporating Copying Mechanism in Sequence-to-Sequence Learning ; We address an important problem in sequence-to-sequence (Seq2\\n',\n",
       " ' Generating Factoid Questions With Recurrent Neural Networks: The 30M   Factoid Question-Answer Corpus ; Over the past decade, la\\n',\n",
       " ' How NOT To Evaluate Your Dialogue System: An Empirical Study of   Unsupervised Evaluation Metrics for Dialogue Response Generati\\n',\n",
       " ' A Hierarchical Latent Variable Encoder-Decoder Model for Generating   Dialogues ; Sequential data often possesses a hierarchical\\n',\n",
       " ' Neural Associative Memory for Dual-Sequence Modeling ; Many important NLP problems can be posed as dual-sequence or sequence-to-\\n',\n",
       " ' Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior   Knowledge ; We introduce LL-RNNs (Log-Linear RNNs), an \\n',\n",
       " ' Embracing data abundance: BookTest Dataset for Reading Comprehension ; There is a practically unlimited amount of natural langua\\n',\n",
       " ' Quasi-Recurrent Neural Networks ; Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence\\n',\n",
       " ' Input Switched Affine Networks: An RNN Architecture Designed for   Interpretability ; There exist many problem domains where the\\n',\n",
       " ' Frustratingly Short Attention Spans in Neural Language Modeling ; Neural language models predict the next token using a latent r\\n',\n",
       " ' A Structured Self-attentive Sentence Embedding ; This paper proposes a new model for extracting an interpretable sentence embedd\\n',\n",
       " ' A Recurrent Neural Model with Attention for the Recognition of Chinese   Implicit Discourse Relations ; We introduce an attentio\\n',\n",
       " ' Event Representations for Automated Story Generation with Deep Neural   Nets ; Automated story generation is the problem of auto\\n',\n",
       " ' A Joint Model for Question Answering and Question Generation ; We propose a generative machine comprehension model that learns j\\n',\n",
       " ' Learning Intrinsic Sparse Structures within Long Short-Term Memory ; Model compression is significant for the wide adoption of R\\n',\n",
       " ' Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational   Compositional Operators for Analogy Detection ; Represen\\n',\n",
       " ' Object-oriented Neural Programming (OONP) for Document Understanding ; We propose Object-oriented Neural Programming (OONP), a f\\n',\n",
       " ' A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering ; This paper proposes a novel neural machine reading mode\\n',\n",
       " ' Improving speech recognition by revising gated recurrent units ; Speech recognition is largely taking advantage of deep learning\\n',\n",
       " ' Integrating planning for task-completion dialogue policy learning ; Training a task-completion dialogue agent with real users vi\\n',\n",
       " ' Building DNN Acoustic Models for Large Vocabulary Speech Recognition ; Deep neural networks (DNNs) are now a central component o\\n',\n",
       " ' Deep Recurrent Neural Networks for Acoustic Modelling ; We present a novel deep Recurrent Neural Network (RNN) model for acousti\\n',\n",
       " ' Regularizing RNNs by Stabilizing Activations ; We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing th\\n',\n",
       " ' Outrageously Large Neural Networks: The Sparsely-Gated   Mixture-of-Experts Layer ; The capacity of a neural network to absorb i\\n',\n",
       " ' Discourse-Based Objectives for Fast Unsupervised Sentence Representation   Learning ; This work presents a novel objective funct\\n',\n",
       " ' Learning Convolutional Text Representations for Visual Question   Answering ; Visual question answering is a recently proposed a\\n',\n",
       " ' Learning Phrase Representations using RNN Encoder-Decoder for   Statistical Machine Translation ; In this paper, we propose a no\\n',\n",
       " ' Recurrent Neural Network Training with Dark Knowledge Transfer ; Recurrent neural networks (RNNs), particularly long short-term \\n',\n",
       " ' Long Short-Term Memory Based Recurrent Neural Network Architectures for   Large Vocabulary Speech Recognition ; Long Short-Term \\n',\n",
       " ' Monitoring Term Drift Based on Semantic Consistency in an Evolving   Vector Field ; Based on the Aristotelian concept of potenti\\n',\n",
       " ' Towards better decoding and language model integration in sequence to   sequence models ; The recently proposed Sequence-to-Sequ\\n',\n",
       " ' Neural Machine Translation by Jointly Learning to Align and Translate ; Neural machine translation is a recently proposed approa\\n',\n",
       " ' Overcoming the Curse of Sentence Length for Neural Machine Translation   using Automatic Segmentation ; The authors of (Cho et a\\n',\n",
       " ' Transferring Knowledge from a RNN to a DNN ; Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art result\\n',\n",
       " ' Correlational Neural Networks ; Common Representation Learning (CRL), wherein different descriptions (or views) of the data are \\n',\n",
       " ' Attention-Based Models for Speech Recognition ; Recurrent sequence generators conditioned on input data through an attention mec\\n',\n",
       " ' Fast and Accurate Recurrent Neural Network Acoustic Models for Speech   Recognition ; We have recently shown that deep Long Shor\\n',\n",
       " ' Listen, Attend and Spell ; We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utteranc\\n',\n",
       " ' BlackOut: Speeding up Recurrent Neural Network Language Models With Very   Large Vocabularies ; We propose BlackOut, an approxim\\n',\n",
       " ' Character-based Neural Machine Translation ; Neural Machine Translation (MT) has reached state-of-the-art results. However, one \\n',\n",
       " ' A Latent Variable Recurrent Neural Network for Discourse Relation   Language Models ; This paper presents a novel latent variabl\\n',\n",
       " ' Multi-task Recurrent Model for Speech and Speaker Recognition ; Although highly correlated, speech and speaker recognition have \\n',\n",
       " ' Hierarchical Memory Networks ; Memory networks are neural networks with an explicit memory component that can be both read and w\\n',\n",
       " ' Sequence-to-Sequence Learning as Beam-Search Optimization ; Sequence-to-Sequence (seq2seq) modeling has rapidly become an import\\n',\n",
       " ' Grounded Recurrent Neural Networks ; In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural n\\n',\n",
       " ' Latent Intention Dialogue Models ; Developing a dialogue agent that is capable of making autonomous decisions and communicating \\n',\n",
       " ' Transfer Learning for Speech Recognition on a Budget ; End-to-end training of automated speech recognition (ASR) systems require\\n',\n",
       " ' Optimizing expected word error rate via sampling for speech recognition ; State-level minimum Bayes risk (sMBR) training has bec\\n',\n",
       " ' Neural Networks Compression for Language Modeling ; In this paper, we consider several compression techniques for the language m\\n',\n",
       " \" Avoiding Your Teacher's Mistakes: Training Neural Networks with   Controlled Weak Supervision ; Training deep neural networks re\\n\",\n",
       " ' Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy   Optimisation ; In statistical dialogue management, th\\n',\n",
       " ' On Extended Long Short-term Memory and Dependent Bidirectional Recurrent   Neural Network ; In this work, we investigate the mem\\n',\n",
       " ' Learning to Answer Questions From Image Using Convolutional Neural   Network ; In this paper, we propose to employ the convoluti\\n',\n",
       " ' Stacked Attention Networks for Image Question Answering ; This paper presents stacked attention networks (SANs) that learn to an\\n',\n",
       " ' Neural Module Networks ; Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\\n',\n",
       " ' Symbol Grounding Association in Multimodal Sequences with Missing   Elements ; In this paper, we extend a symbolic association f\\n',\n",
       " ' Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe   Noise ; The growing importance of massive datasets wit\\n',\n",
       " ' Describing Multimedia Content using Attention-based Encoder--Decoder   Networks ; Whereas deep neural networks were first mostly\\n',\n",
       " ' Multilingual Image Description with Neural Sequence Models ; In this paper we present an approach to multi-language image descri\\n',\n",
       " ' Deep Embedding for Spatial Role Labeling ; This paper introduces the visually informed embedding of word (VIEW), a continuous ve\\n',\n",
       " ' Image-to-Markup Generation with Coarse-to-Fine Attention ; We present a neural encoder-decoder model to convert images into pres\\n',\n",
       " ' Teaching Machines to Code: Neural Markup Generation with Visual   Attention ; We present a deep recurrent neural network model w\\n',\n",
       " ' Evolution in Groups: A deeper look at synaptic cluster driven evolution   of deep neural networks ; A promising paradigm for ach\\n',\n",
       " ' Mesh Learning for Classifying Cognitive Processes ; A relatively recent advance in cognitive neuroscience has been multi-voxel p\\n',\n",
       " ' Synthesizing Deep Neural Network Architectures using Biological Synaptic   Strength Distributions ; In this work, we perform an \\n',\n",
       " ' A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters   Optimization ; Addressing the issue of SVMs parameters op\\n',\n",
       " ' Density estimation using Real NVP ; Unsupervised learning of probabilistic models is a central yet challenging problem in machin\\n',\n",
       " ' Evolution Strategies as a Scalable Alternative to Reinforcement Learning ; We explore the use of Evolution Strategies (ES), a cl\\n',\n",
       " ' QMDP-Net: Deep Learning for Planning under Partial Observability ; This paper introduces the QMDP-net, a neural network architec\\n',\n",
       " ' TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep   Reinforcement Learning ; Combining deep model-free reinforce\\n',\n",
       " ' Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent   Networks ; A major drawback of backpropagation throug\\n',\n",
       " ' Stochastic Deep Learning in Memristive Networks ; We study the performance of stochastically trained deep neural networks (DNNs)\\n',\n",
       " ' PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction ; Multi-step-ahead time series prediction is one of the \\n',\n",
       " ' Norm-Based Capacity Control in Neural Networks ; We investigate the capacity, convexity and characterization of a general family\\n',\n",
       " ' Improving the Performance of Neural Networks in Regression Tasks Using   Drawering ; The method presented extends a given regres\\n',\n",
       " ' Learning unbiased features ; A key element in transfer learning is representation learning; if representations can be developed \\n',\n",
       " ' Compatible Value Gradients for Reinforcement Learning of Continuous Deep   Policies ; This paper proposes GProp, a deep reinforc\\n',\n",
       " ' Learning dynamic Boltzmann machines with spike-timing dependent   plasticity ; We propose a particularly structured Boltzmann ma\\n',\n",
       " ' Gated Graph Sequence Neural Networks ; Graph-structured data appears frequently in domains including chemistry, natural language\\n',\n",
       " ' Deep Reinforcement Learning in Large Discrete Action Spaces ; Being able to reason in an environment with a large number of disc\\n',\n",
       " ' Value Iteration Networks ; We introduce the value iteration network (VIN): a fully differentiable neural network with a `plannin\\n',\n",
       " ' Recurrent Orthogonal Networks and Long-Memory Tasks ; Although RNNs have been shown to be powerful tools for processing sequenti\\n',\n",
       " ' Learning values across many orders of magnitude ; Most learning algorithms are not invariant to the scale of the function that i\\n',\n",
       " ' Genetic Architect: Discovering Genomic Structure with Learned Neural   Architectures ; Each human genome is a 3 billion base pai\\n',\n",
       " ' Deep Successor Reinforcement Learning ; Learning robust value functions given raw observations and rewards is now possible with \\n',\n",
       " ' RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning ; Deep reinforcement learning (deep RL) has been successful \\n',\n",
       " ' Capacity and Trainability in Recurrent Neural Networks ; Two potential bottlenecks on the expressiveness of recurrent neural net\\n',\n",
       " ' Causal Regularization ; In application domains such as healthcare, we want accurate predictive models that are also causally int\\n',\n",
       " ' On the Behavior of Convolutional Nets for Feature Extraction ; Deep neural networks are representation learning techniques. Duri\\n',\n",
       " ' Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in   Generative Models ; Adversarial learning of probabilistic m\\n',\n",
       " ' Filtering Variational Objectives ; When used as a surrogate objective for maximum likelihood estimation in latent variable model\\n',\n",
       " ' Kernel Implicit Variational Inference ; Recent progress in variational inference has paid much attention to the flexibility of v\\n',\n",
       " ' Non-Markovian Control with Gated End-to-End Memory Policy Networks ; Partially observable environments present an important open\\n',\n",
       " ' Automated Problem Identification: Regression vs Classification via   Evolutionary Deep Networks ; Regression or classification? \\n',\n",
       " ' A Simple Neural Attentive Meta-Learner ; Deep neural networks excel in regimes with large amounts of data, but tend to struggle \\n',\n",
       " ' Kafnets: kernel-based non-parametric activation functions for neural   networks ; Neural networks are generally built by interle\\n',\n",
       " ' Learning model-based planning from scratch ; Conventional wisdom holds that model-based planning is a powerful approach to seque\\n',\n",
       " ' Recurrent Ladder Networks ; We propose a recurrent extension of the Ladder networks whose structure is motivated by the inferenc\\n',\n",
       " ' Generalization in Deep Learning ; With a direct analysis of neural networks, this paper presents a mathematically tight generali\\n',\n",
       " ' Parametrizing filters of a CNN with a GAN ; It is commonly agreed that the use of relevant invariances as a good statistical bia\\n',\n",
       " ' Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence   Learning ; Long Short-Term Memory (LSTM) is a popular appr\\n',\n",
       " ' Learning and Real-time Classification of Hand-written Digits With   Spiking Neural Networks ; We describe a novel spiking neural\\n',\n",
       " ' Overcoming catastrophic forgetting with hard attention to the task ; Catastrophic forgetting occurs when a neural network loses \\n',\n",
       " ' Detecting and Correcting for Label Shift with Black Box Predictors ; Faced with distribution shift between training and test set\\n',\n",
       " ' Generalization in Machine Learning via Analytical Learning Theory ; This paper introduces a novel measure-theoretic learning the\\n',\n",
       " ' Sensitivity and Generalization in Neural Networks: an Empirical Study ; In practice it is often found that large over-parameteri\\n',\n",
       " ' On the importance of single directions for generalization ; Despite their ability to memorize large datasets, deep neural networ\\n',\n",
       " ' Maximin affinity learning of image segmentation ; Images can be segmented by first using a classifier to predict an affinity gra\\n',\n",
       " ' A General Framework for Development of the Cortex-like Visual Object   Recognition System: Waves of Spikes, Predictive Coding an\\n',\n",
       " ' Handwritten Digit Recognition with a Committee of Deep Neural Nets on   GPUs ; The competitive MNIST handwritten digit recogniti\\n',\n",
       " ' Eclectic Extraction of Propositional Rules from Neural Networks ; Artificial Neural Network is among the most popular algorithm \\n',\n",
       " ' Message Passing Multi-Agent GANs ; Communicating and sharing intelligence among agents is an important facet of achieving Artifi\\n',\n",
       " ' Mode Regularized Generative Adversarial Networks ; Although Generative Adversarial Networks achieve state-of-the-art results on \\n',\n",
       " ' Layer-Specific Adaptive Learning Rates for Deep Networks ; The increasing complexity of deep learning architectures is resulting\\n',\n",
       " ' Return of Frustratingly Easy Domain Adaptation ; Unlike human learning, machine learning often fails to handle changes between t\\n',\n",
       " ' Origami: A 803 GOp/s/W Convolutional Network Accelerator ; An ever increasing number of computer vision and image/video processi\\n',\n",
       " ' Option Discovery in Hierarchical Reinforcement Learning using   Spatio-Temporal Clustering ; This paper introduces an automated \\n',\n",
       " ' Residual Networks Behave Like Ensembles of Relatively Shallow Networks ; In this work we propose a novel interpretation of resid\\n',\n",
       " ' Synthesizing the preferred inputs for neurons in neural networks via   deep generator networks ; Deep neural networks (DNNs) hav\\n',\n",
       " ' Structured Convolution Matrices for Energy-efficient Deep learning ; We derive a relationship between network representation in \\n',\n",
       " ' Deep CORAL: Correlation Alignment for Deep Domain Adaptation ; Deep neural networks are able to learn powerful representations f\\n',\n",
       " ' Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition ; 3D action recognition - analysis of human actions based \\n',\n",
       " ' Generalized Dropout ; Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer t\\n',\n",
       " ' Parsimonious Inference on Convolutional Neural Networks: Learning and   applying on-line kernel activation rules ; A new, radica\\n',\n",
       " ' Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks ; We propose an algorithm for meta-learning that is model-agno\\n',\n",
       " ' WRPN: Training and Inference using Wide Reduced-Precision Networks ; For computer vision applications, prior works have shown th\\n',\n",
       " ' Deep Learning is Robust to Massive Label Noise ; Deep neural networks trained on large supervised datasets have led to impressiv\\n',\n",
       " ' Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object   Rotation ; Content-invariance in mapping codes learned\\n',\n",
       " ' Deep Learning for Sensor-based Activity Recognition: A Survey ; Sensor-based activity recognition seeks the profound high-level \\n',\n",
       " ' On the Importance of Consistency in Training Deep Neural Networks ; We explain that the difficulties of training deep neural net\\n',\n",
       " ' UI-Net: Interactive Artificial Neural Networks for Iterative Image   Segmentation Based on a User Model ; For complex segmentati\\n',\n",
       " ' Lightweight Neural Networks ; Most of the weights in a Lightweight Neural Network have a value of zero, while the remaining ones\\n',\n",
       " ' Tensor Field Networks: Rotation- and Translation-Equivariant Neural   Networks for 3D Point Clouds ; We introduce tensor field n\\n',\n",
       " ' Knowledge Matters: Importance of Prior Information for Optimization ; We explore the effect of introducing prior information int\\n',\n",
       " ' Zero-bias autoencoders and the benefits of co-adapting features ; Regularized training of an autoencoder typically results in hi\\n',\n",
       " ' Theory and Tools for the Conversion of Analog to Spiking Convolutional   Neural Networks ; Deep convolutional neural networks (C\\n',\n",
       " ' Stacked Generative Adversarial Networks ; In this paper, we propose a novel generative model named Stacked Generative Adversaria\\n',\n",
       " ' Self-informed neural network structure learning ; We study the problem of large scale, multi-label visual recognition with a lar\\n',\n",
       " ' Learning Activation Functions to Improve Deep Neural Networks ; Artificial neural networks typically have a fixed, non-linear ac\\n',\n",
       " ' Denoising autoencoder with modulated lateral connections learns   invariant representations of natural images ; Suitable lateral\\n',\n",
       " ' A Probabilistic Theory of Deep Learning ; A grand challenge in machine learning is the development of computational algorithms t\\n',\n",
       " ' Integrated Inference and Learning of Neural Factors in Structural   Support Vector Machines ; Tackling pattern recognition probl\\n',\n",
       " ' What Happened to My Dog in That Network: Unraveling Top-down Generators   in Convolutional Neural Networks ; Top-down informatio\\n',\n",
       " ' Virtual Worlds as Proxy for Multi-Object Tracking Analysis ; Modern computer vision algorithms typically require expensive data \\n',\n",
       " ' Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet ; Video sequences contain rich dynamic patterns, such as dy\\n',\n",
       " ' Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural   Networks ; Taking inspiration from biological evolution, we e\\n',\n",
       " ' Alternating Back-Propagation for Generator Network ; This paper proposes an alternating back-propagation algorithm for learning \\n',\n",
       " ' Hyperparameter Transfer Learning through Surrogate Alignment for   Efficient Deep Neural Network Training ; Recently, several op\\n',\n",
       " ' Towards Bayesian Deep Learning: A Framework and Some Existing Methods ; While perception tasks such as visual object recognition\\n',\n",
       " ' Deciding How to Decide: Dynamic Routing in Artificial Neural Networks ; We propose and systematically evaluate three strategies \\n',\n",
       " ' Pixel Deconvolutional Networks ; Deconvolutional layers have been widely used in a variety of deep models for up-sampling, inclu\\n',\n",
       " ' Gaussian Prototypical Networks for Few-Shot Learning on Omniglot ; We propose a novel architecture for $k$-shot classification o\\n',\n",
       " ' Super-Convergence: Very Fast Training of Residual Networks Using Large   Learning Rates ; In this paper, we show a phenomenon, w\\n',\n",
       " ' Generative learning for deep networks ; Learning, taking into account full distribution of the data, referred to as generative, \\n',\n",
       " ' Hierarchical Representations for Efficient Architecture Search ; We explore efficient neural architecture search methods and sho\\n',\n",
       " ' Data Augmentation Generative Adversarial Networks ; Effective training of neural networks requires much data. In the low-data re\\n',\n",
       " ' DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the   Jigsaw Puzzle Problem ; This paper introduces the first dee\\n',\n",
       " ' DeepPainter: Painter Classification Using Deep Convolutional   Autoencoders ; In this paper we describe the problem of painter c\\n',\n",
       " ' DeepBrain: Functional Representation of Neural In-Situ Hybridization   Images for Gene Ontology Classification Using Deep Convol\\n',\n",
       " ' Generative Adversarial Perturbations ; In this paper, we propose novel generative models for creating adversarial examples, slig\\n',\n",
       " ' A Rotation and a Translation Suffice: Fooling CNNs with Simple   Transformations ; We show that simple transformations, namely t\\n',\n",
       " ' Peephole: Predicting Network Performance Before Training ; The quest for performant networks has been a significant force that d\\n',\n",
       " ' An Architecture Combining Convolutional Neural Network (CNN) and Support   Vector Machine (SVM) for Image Classification ; Convo\\n',\n",
       " ' Benchmarking Decoupled Neural Interfaces with Synthetic Gradients ; Artifical Neural Networks are a particular class of learning\\n',\n",
       " ' Segmentation hiérarchique faiblement supervisée ; Image segmentation is the process of partitioning an image into a set of meani\\n',\n",
       " ' Training wide residual networks for deployment using a single bit for   each weight ; For fast and energy-efficient deployment o\\n',\n",
       " ' Deep Learning using Rectified Linear Units (ReLU) ; We introduce the use of rectified linear units (ReLU) as the classification \\n',\n",
       " ' Rectified Factor Networks ; We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-d\\n',\n",
       " ' From Maxout to Channel-Out: Encoding Information on Sparse Pathways ; Motivated by an important insight from neural science, we \\n',\n",
       " ' Competitive Learning with Feedforward Supervisory Signal for Pre-trained   Multilayered Networks ; We propose a novel learning m\\n',\n",
       " ' Deeply-Supervised Nets ; Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while ma\\n',\n",
       " ' Path-SGD: Path-Normalized Optimization in Deep Neural Networks ; We revisit the choice of SGD for training deep neural networks \\n',\n",
       " ' Adapting Resilient Propagation for Deep Learning ; The Resilient Propagation (Rprop) algorithm has been very popular for backpro\\n',\n",
       " ' Convolutional Neural Network for Stereotypical Motor Movement Detection   in Autism ; Autism Spectrum Disorders (ASDs) are often\\n',\n",
       " ' Resnet in Resnet: Generalizing Residual Architectures ; Residual networks (ResNets) have recently achieved state-of-the-art on c\\n',\n",
       " ' Evolutionary Synthesis of Deep Neural Networks via Synaptic   Cluster-driven Genetic Encoding ; There has been significant recen\\n',\n",
       " ' Neural Photo Editing with Introspective Adversarial Networks ; The increasingly photorealistic sample quality of generative imag\\n',\n",
       " ' Adaptive Neural Networks for Efficient Inference ; We present an approach to adaptively utilize deep neural networks in order to\\n',\n",
       " ' Spatial Variational Auto-Encoding via Matrix-Variate Normal   Distributions ; The key idea of variational auto-encoders (VAEs) r\\n',\n",
       " ' Dense Transformer Networks ; The key idea of current deep learning methods for dense prediction is to apply a model on a regular\\n',\n",
       " ' Progressive Learning for Systematic Design of Large Neural Networks ; We develop an algorithm for systematic design of a large a\\n',\n",
       " ' A Classification-Based Perspective on GAN Distributions ; A fundamental, and still largely unanswered, question in the context o\\n',\n",
       " ' Learning Visual Reasoning Without Strong Priors ; Achieving artificial visual reasoning - the ability to answer image-related qu\\n',\n",
       " ' Men Also Like Shopping: Reducing Gender Bias Amplification using   Corpus-level Constraints ; Language is increasingly being use\\n',\n",
       " ' Acquiring Common Sense Spatial Knowledge through Implicit Spatial   Templates ; Spatial understanding is a fundamental problem w\\n',\n",
       " ' FiLM: Visual Reasoning with a General Conditioning Layer ; We introduce a general-purpose conditioning method for neural network\\n',\n",
       " ' Unsupervised Induction of Semantic Roles within a Reconstruction-Error   Minimization Framework ; We introduce a new approach to\\n',\n",
       " ' Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word   Embeddings ; The blind application of machine learning \\n',\n",
       " ' TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency ; In this paper, we propose TopicRNN, a recurrent neura\\n',\n",
       " ' Gaussian Attention Model and Its Application to Knowledge Base Embedding   and Question Answering ; We propose the Gaussian atte\\n',\n",
       " ' Variable Computation in Recurrent Neural Networks ; Recurrent neural networks (RNNs) have been used extensively and with increas\\n',\n",
       " ' Learning to Learn from Weak Supervision by Full Supervision ; In this paper, we propose a method for training neural networks wh\\n',\n",
       " ' SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for   Predicting Chemical Properties ; Chemical databases store\\n',\n",
       " ' Sample Efficient Deep Reinforcement Learning for Dialogue Systems with   Large Action Spaces ; In spoken dialogue systems, we ai\\n',\n",
       " ' High-Dimensional Vector Semantics ; In this paper we explore the \"vector semantics\" problem from the perspective of \"almost orth\\n',\n",
       " ' Learning Semantic Script Knowledge with Event Embeddings ; Induction of common sense knowledge about prototypical sequences of e\\n',\n",
       " ' Mathematical Language Processing: Automatic Grading and Feedback for   Open Response Mathematical Questions ; While computer and\\n',\n",
       " ' Nonparametric Bayesian Double Articulation Analyzer for Direct Language   Acquisition from Continuous Speech Signals ; Human inf\\n',\n",
       " ' Harnessing Deep Neural Networks with Logic Rules ; Combining deep neural networks with structured logic rules is desirable to ha\\n',\n",
       " ' Toward Controlled Generation of Text ; Generic generation and manipulation of text is challenging and has limited success compar\\n',\n",
       " ' Adversarial Connective-exploiting Networks for Implicit Discourse   Relation Classification ; Implicit discourse relation classi\\n',\n",
       " ' Abstract Syntax Networks for Code Generation and Semantic Parsing ; Tasks like code generation and semantic parsing require mapp\\n',\n",
       " ' Multimodal Word Distributions ; Word embeddings provide point representations of words containing useful semantic information. W\\n',\n",
       " ' Guiding Reinforcement Learning Exploration Using Natural Language ; In this work we present a technique to use natural language \\n',\n",
       " ' Robust Task Clustering for Deep Many-Task Learning ; We investigate task clustering for deep-learning based multi-task and few-s\\n',\n",
       " ' Natural Language Multitasking: Analyzing and Improving Syntactic   Saliency of Hidden Representations ; We train multi-task auto\\n',\n",
       " ' Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement   Learning ; With the increasing popularity of video shar\\n',\n",
       " ' A Supervised Approach to Extractive Summarisation of Scientific Papers ; Automatic summarisation is a popular approach to reduce\\n',\n",
       " ' Language Models for Image Captioning: The Quirks and What Works ; Two recent approaches have achieved state-of-the-art results i\\n',\n",
       " ' Exploring Models and Data for Image Question Answering ; This work aims to address the problem of image-based question-answering\\n',\n",
       " ' Making the V in VQA Matter: Elevating the Role of Image Understanding in   Visual Question Answering ; Problems at the intersect\\n',\n",
       " ' A Multi-World Approach to Question Answering about Real-World Scenes   based on Uncertain Input ; We propose a method for automa\\n',\n",
       " ' Hard to Cheat: A Turing Test based on Answering Questions about Images ; Progress in language and image understanding by machine\\n',\n",
       " ' Analyzing the Behavior of Visual Question Answering Models ; Recently, a number of deep-learning based models have been proposed\\n',\n",
       " ' Sort Story: Sorting Jumbled Images and Captions into Stories ; Temporal common sense has applications in AI tasks such as QA, mu\\n',\n",
       " ' Mean Box Pooling: A Rich Image Representation and Output Embedding for   the Visual Madlibs Task ; We present Mean Box Pooling, \\n',\n",
       " ' Learning to generalize to new compositions in image understanding ; Recurrent neural networks have recently been used for learni\\n',\n",
       " ' Measuring Machine Intelligence Through Visual Question Answering ; As machines have become more intelligent, there has been a re\\n',\n",
       " ' Towards Transparent AI Systems: Interpreting Visual Question Answering   Models ; Deep neural networks have shown striking progr\\n',\n",
       " ' Visual Dialog ; We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in na\\n',\n",
       " ' Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic   Speech Recognition ; Multi-task learning (MTL) involves\\n',\n",
       " ' Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning ; We introduce the first goal-driven training for v\\n',\n",
       " ' Being Negative but Constructively: Lessons Learnt from Creating Better   Visual Question Answering Datasets ; Visual question an\\n',\n",
       " ' C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0   Dataset ; Visual Question Answering (VQA) has receive\\n',\n",
       " ' Deep learning evaluation using deep linguistic processing ; We discuss problems with the standard approaches to evaluation for t\\n',\n",
       " ' meProp: Sparsified Back Propagation for Accelerated Deep Learning with   Reduced Overfitting ; We propose a simple yet effective\\n',\n",
       " ' Towards Crafting Text Adversarial Samples ; Adversarial samples are strategically modified samples, which are crafted with the p\\n',\n",
       " ' Reinforced Video Captioning with Entailment Rewards ; Sequence-to-sequence models have shown promising improvements on the tempo\\n',\n",
       " ' Hierarchically-Attentive RNN for Album Summarization and Storytelling ; We address the problem of end-to-end visual storytelling\\n',\n",
       " ' Generating Natural Adversarial Examples ; Due to their complex nature, it is hard to characterize the ways in which machine lear\\n',\n",
       " ' Training Simplification and Model Simplification for Deep Learning: A   Minimal Effort Back Propagation Method ; We propose a si\\n',\n",
       " ' Embodied Question Answering ; We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned \\n',\n",
       " \" Don't Just Assume; Look and Answer: Overcoming Priors for Visual   Question Answering ; A number of studies have found that toda\\n\",\n",
       " ' CoDraw: Visual Dialog for Collaborative Drawing ; In this work, we propose a goal-driven collaborative task that contains vision\\n',\n",
       " \" Answerer in Questioner's Mind for Goal-Oriented Visual Dialogue ; Goal-oriented dialogue has been paid attention for its numerou\\n\",\n",
       " ' Resource Constrained Structured Prediction ; We study the problem of structured prediction under test-time budget constraints. W\\n',\n",
       " ' Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to   Action Sequences ; We propose a neural sequence-to-se\\n',\n",
       " ' Coupling Distributed and Symbolic Execution for Natural Language Queries ; Building neural networks to query a knowledge base (a\\n',\n",
       " ' An agent-driven semantical identifier using radial basis neural networks   and reinforcement learning ; Due to the huge availabi\\n',\n",
       " ' Where is my forearm? Clustering of body parts from simultaneous tactile   and linguistic input using sequential mapping ; Humans\\n',\n",
       " ' Improvements to deep convolutional neural networks for LVCSR ; Deep Convolutional Neural Networks (CNNs) are more powerful than \\n',\n",
       " ' Collaborative Deep Learning for Recommender Systems ; Collaborative filtering (CF) is a successful approach commonly used by man\\n',\n",
       " ' Explaining Predictions of Non-Linear Classifiers in NLP ; Layer-wise relevance propagation (LRP) is a recently proposed techniqu\\n',\n",
       " ' Tensor network language model ; We propose a new statistical model suitable for machine learning of systems with long distance c\\n',\n",
       " ' Language as a matrix product state ; We propose a statistical model for natural language that begins by considering language as \\n',\n",
       " ' Accelerating Hessian-free optimization for deep neural networks by   implicit preconditioning and sampling ; Hessian-free traini\\n',\n",
       " ' Is a Picture Worth Ten Thousand Words in a Review Dataset? ; While textual reviews have become prominent in many recommendation-\\n',\n",
       " ' Validation of nonlinear PCA ; Linear principal component analysis (PCA) can be extended to a nonlinear PCA by using artificial n\\n',\n",
       " ' Graph Approximation and Clustering on a Budget ; We consider the problem of learning from a similarity matrix (such as spectral \\n',\n",
       " ' ShareBoost: Efficient Multiclass Learning with Feature Sharing ; Multiclass prediction is the problem of classifying an object i\\n',\n",
       " ' Functional Principal Component Analysis and Randomized Sparse Clustering   Algorithm for Medical Image Analysis ; Due to advance\\n',\n",
       " ' Jointly Learning Multiple Measures of Similarities from Triplet   Comparisons ; Similarity between objects is multi-faceted and \\n',\n",
       " ' Variational Inference for Uncertainty on the Inputs of Gaussian Process   Models ; The Gaussian process latent variable model (G\\n',\n",
       " ' Conditional Generative Adversarial Nets ; Generative Adversarial Nets [8] were recently introduced as a novel way to train gener\\n',\n",
       " ' Visual Causal Feature Learning ; We provide a rigorous definition of the visual cause of a behavior that is broadly applicable t\\n',\n",
       " ' In Search of the Real Inductive Bias: On the Role of Implicit   Regularization in Deep Learning ; We present experiments demonst\\n',\n",
       " ' Domain Generalization for Object Recognition with Multi-task   Autoencoders ; The problem of domain generalization is to take kn\\n',\n",
       " ' Data-Efficient Learning of Feedback Policies from Image Pixels using   Deep Dynamical Models ; Data-efficient reinforcement lear\\n',\n",
       " ' Scatter Component Analysis: A Unified Framework for Domain Adaptation   and Domain Generalization ; This paper addresses classif\\n',\n",
       " ' Robust Subspace Clustering via Tighter Rank Approximation ; Matrix rank minimization problem is in general NP-hard. The nuclear \\n',\n",
       " ' Recognizing Semantic Features in Faces using Deep Learning ; The human face constantly conveys information, both consciously and\\n',\n",
       " ' Deep Reconstruction-Classification Networks for Unsupervised Domain   Adaptation ; In this paper, we propose a novel unsupervise\\n',\n",
       " ' A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation ; Finding the most effective way to aggregate multi-subject \\n',\n",
       " ' Feedback-Controlled Sequential Lasso Screening ; One way to solve lasso problems when the dictionary does not fit into available\\n',\n",
       " ' The Symmetry of a Simple Optimization Problem in Lasso Screening ; Recently dictionary screening has been proposed as an effecti\\n',\n",
       " ' Hard Negative Mining for Metric Learning Based Zero-Shot Classification ; Zero-Shot learning has been shown to be an efficient s\\n',\n",
       " ' Pose-Selective Max Pooling for Measuring Similarity ; In this paper, we deal with two challenges for measuring the similarity of\\n',\n",
       " ' Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble   of Autoencoders ; A fall is an abnormal activity that\\n',\n",
       " ' Generalization Error of Invariant Classifiers ; This paper studies the generalization error of invariant classifiers. In particu\\n',\n",
       " ' Universal adversarial perturbations ; Given a state-of-the-art deep neural network classifier, we show the existence of a univer\\n',\n",
       " ' Linear Disentangled Representation Learning for Facial Actions ; Limited annotated data available for the recognition of facial \\n',\n",
       " ' On Detecting Adversarial Perturbations ; Machine learning and deep learning in particular has advanced tremendously on perceptua\\n',\n",
       " ' Activation Maximization Generative Adversarial Nets ; Class labels have been empirically shown useful in improving the sample qu\\n',\n",
       " ' Interpretable Explanations of Black Boxes by Meaningful Perturbation ; As machine learning algorithms are increasingly applied t\\n',\n",
       " ' A General Theory for Training Learning Machine ; Though the deep learning is pushing the machine learning to a new stage, basic \\n',\n",
       " ' A Generalization of Convolutional Neural Networks to Graph-Structured   Data ; This paper introduces a generalization of Convolu\\n',\n",
       " ' Formal Guarantees on the Robustness of a Classifier against Adversarial   Manipulation ; Recent work has shown that state-of-the\\n',\n",
       " ' Classification regions of deep neural networks ; The goal of this paper is to analyze the geometric properties of deep neural ne\\n',\n",
       " ' Analysis of universal adversarial perturbations ; Deep networks have recently been shown to be vulnerable to universal perturbat\\n',\n",
       " ' Bayesian GAN ; Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which\\n',\n",
       " ' Unsupervised Learning of Disentangled Representations from Video ; We present a new model DrNET that learns disentangled image r\\n',\n",
       " ' Dualing GANs ; Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is howe\\n',\n",
       " ' Wavelet Residual Network for Low-Dose CT via Deep Convolutional   Framelets ; Model based iterative reconstruction (MBIR) algori\\n',\n",
       " ' 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks ; The success of various applications including robotics, di\\n',\n",
       " ' Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x))   Alternative ; In this article, we mathematically study sev\\n',\n",
       " ' A Brief Survey of Deep Reinforcement Learning ; Deep reinforcement learning is poised to revolutionise the field of AI and repre\\n',\n",
       " ' CirCNN: Accelerating and Compressing Deep Neural Networks Using   Block-CirculantWeight Matrices ; Large-scale deep neural netwo\\n',\n",
       " ' XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual   Classification ; We propose two multimodal deep learning archite\\n',\n",
       " ' Context Embedding Networks ; Low dimensional embeddings that capture the main variations of interest in collections of data are \\n',\n",
       " ' How Much Chemistry Does a Deep Neural Network Need to Know to Make   Accurate Predictions? ; The meteoric rise of deep learning \\n',\n",
       " ' Variational Inference of Disentangled Latent Concepts from Unlabeled   Observations ; Disentangled representations, where the hi\\n',\n",
       " ' Three Factors Influencing Minima in SGD ; We study the properties of the endpoint of stochastic gradient descent (SGD). By appro\\n',\n",
       " ' Learning to Play Othello with Deep Neural Networks ; Achieving superhuman playing level by AlphaGo corroborated the capabilities\\n',\n",
       " ' Deep Learning Can Reverse Photon Migration for Diffuse Optical   Tomography ; Can artificial intelligence (AI) learn complicated\\n',\n",
       " ' Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for   Transferable Chemical Property Prediction ; With access to\\n',\n",
       " ' Deep Learning in RF Sub-sampled B-mode Ultrasound Imaging ; In portable, three dimensional, and ultra-fast ultrasound (US) imagi\\n',\n",
       " ' Deep Learning Interior Tomography for Region-of-Interest Reconstruction ; Interior tomography for the region-of-interest (ROI) i\\n',\n",
       " ' Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner ; For homeland and transportation security applications, \\n',\n",
       " ' Effective Building Block Design for Deep Convolutional Neural Networks   using Search ; Deep learning has shown promising result\\n',\n",
       " ' TVAE: Triplet-Based Variational Autoencoder using Metric Learning ; Deep metric learning has been demonstrated to be highly effe\\n',\n",
       " ' Learning to Play with Intrinsically-Motivated Self-Aware Agents ; Infants are experts at playing, with an amazing ability to gen\\n',\n",
       " ' Emergence of Structured Behaviors from Curiosity-Based Intrinsic   Motivation ; Infants are experts at playing, with an amazing \\n',\n",
       " ' Stochastic Video Generation with a Learned Prior ; Generating video frames that accurately predict future world states is challe\\n',\n",
       " ' Multi-Evidence Filtering and Fusion for Multi-Label Classification,   Object Detection and Semantic Segmentation Based on Weakly\\n',\n",
       " ' Neural Networks Should Be Wide Enough to Learn Disconnected Decision   Regions ; In the recent literature the important role of \\n',\n",
       " \" Visual Explanations From Deep 3D Convolutional Neural Networks for   Alzheimer's Disease Classification ; We develop three effic\\n\",\n",
       " ' Averaging Weights Leads to Wider Optima and Better Generalization ; Deep neural networks are typically trained by optimizing a l\\n',\n",
       " ' SENNS: Sparse Extraction Neural NetworkS for Feature Extraction ; By drawing on ideas from optimisation theory, artificial neura\\n',\n",
       " ' Generative Models and Model Criticism via Optimized Maximum Mean   Discrepancy ; We propose a method to optimize the representat\\n',\n",
       " ' Deep Learning Approximation for Stochastic Control Problems ; Many real world stochastic control problems suffer from the \"curse\\n',\n",
       " ' Generating Focussed Molecule Libraries for Drug Discovery with Recurrent   Neural Networks ; In de novo drug design, computation\\n',\n",
       " ' Parameter Space Noise for Exploration ; Deep reinforcement learning (RL) methods generally engage in exploratory behavior throug\\n',\n",
       " ' On The Robustness of a Neural Network ; With the development of neural networks based machine learning and their usage in missio\\n',\n",
       " ' ZhuSuan: A Library for Bayesian Deep Learning ; In this paper we introduce ZhuSuan, a python probabilistic programming library f\\n',\n",
       " ' Using Parameterized Black-Box Priors to Scale Up Model-Based Policy   Search for Robotics ; The most data-efficient algorithms f\\n',\n",
       " ' Bayesian Optimization with Automatic Prior Selection for Data-Efficient   Direct Policy Search ; One of the most interesting fea\\n',\n",
       " ' Bounding and Counting Linear Regions of Deep Neural Networks ; In this paper, we study the representational power of deep neural\\n',\n",
       " ' Deep Rewiring: Training very sparse deep networks ; Neuromorphic hardware tends to pose limits on the connectivity of deep netwo\\n',\n",
       " ' Comparing heterogeneous entities using artificial neural networks of   trainable weighted structural components and machine-lear\\n',\n",
       " ' Active Learning of Inverse Models with Intrinsically Motivated Goal   Exploration in Robots ; We introduce the Self-Adaptive Goa\\n',\n",
       " ' End-to-End Tracking and Semantic Segmentation Using Recurrent Neural   Networks ; In this work we present a novel end-to-end fra\\n',\n",
       " ' Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks ; This paper presents to the best of our knowledge the first\\n',\n",
       " ' Deep Predictive Coding Networks for Video Prediction and Unsupervised   Learning ; While great strides have been made in using d\\n',\n",
       " ' Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient   Convolutional Neural Networks ; This paper proposes a comp\\n',\n",
       " ' On Convergence and Stability of GANs ; We propose studying GAN training dynamics as regret minimization, which is in contrast to\\n',\n",
       " ' Imitation from Observation: Learning to Imitate Behaviors from Raw Video   via Context Translation ; Imitation learning is an ef\\n',\n",
       " ' Convergence rates for pretraining and dropout: Guiding learning   parameters using network structure ; Unsupervised pretraining \\n',\n",
       " ' Learning Discriminative Features via Label Consistent Neural Network ; Deep Convolutional Neural Networks (CNN) enforces supervi\\n',\n",
       " ' Out-of-Sample Extension for Dimensionality Reduction of Noisy Time   Series ; This paper proposes an out-of-sample extension fra\\n',\n",
       " ' Adversarial Examples for Semantic Image Segmentation ; Machine learning methods in general and Deep Neural Networks in particula\\n',\n",
       " ' Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box   Machine Learning Models ; Many machine learning algorit\\n',\n",
       " ' Towards Building an Intelligent Anti-Malware System: A Deep Learning   Approach using Support Vector Machine (SVM) for Malware C\\n',\n",
       " ' Feature extraction using Latent Dirichlet Allocation and Neural   Networks: A case study on movie synopses ; Feature extraction \\n',\n",
       " ' A Survey of Available Corpora for Building Data-Driven Dialogue Systems ; During the past decade, several areas of speech and la\\n',\n",
       " ' Generative Topic Embedding: a Continuous Representation of Documents   (Extended Version with Proofs) ; Word embedding maps word\\n',\n",
       " ' Fine-Grained Entity Typing with High-Multiplicity Assignments ; As entity type systems become richer and more fine-grained, we e\\n',\n",
       " ' Towards a Visual Turing Challenge ; As language and visual understanding by machines progresses rapidly, we are observing an inc\\n',\n",
       " ' Interactive Robot Learning of Gestures, Language and Affordances ; A growing field in robotics and Artificial Intelligence (AI) \\n',\n",
       " ' Visual Features for Context-Aware Speech Recognition ; Automatic transcriptions of consumer-generated multi-media content such a\\n',\n",
       " ' Examining Cooperation in Visual Dialog Models ; In this work we propose a blackbox intervention method for visual dialog models,\\n',\n",
       " ' Video Highlight Prediction Using Audience Chat Reactions ; Sports channel video portals offer an exciting domain for research on\\n',\n",
       " ' Invariant Representations for Noisy Speech Recognition ; Modern automatic speech recognition (ASR) systems need to be robust und\\n',\n",
       " ' Self-Supervised Vision-Based Detection of the Active Speaker as a   Prerequisite for Socially-Aware Language Acquisition ; This \\n',\n",
       " ' Product Characterisation towards Personalisation: Learning Attributes   from Unstructured Data to Recommend Fashion Products ; I\\n',\n",
       " ' The Self-Organization of Speech Sounds ; The speech code is a vehicle of language: it defines a set of forms used by a community\\n',\n",
       " \" What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes ; The F-measure or F-score is one of the most commonly \\n\",\n",
       " ' A Machine Learning Perspective on Predictive Coding with PAQ ; PAQ8 is an open source lossless data compression algorithm that c\\n',\n",
       " ' A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale   SVM Training ; Recently, there has been a renewed inte\\n',\n",
       " ' Semi-supervised Vocabulary-informed Learning ; Despite significant progress in object categorization, in recent years, a number \\n',\n",
       " ' Submodular meets Structured: Finding Diverse Subsets in   Exponentially-Large Structured Item Sets ; To cope with the high level\\n',\n",
       " ' ZM-Net: Real-time Zero-shot Image Manipulation Network ; Many problems in image processing and computer vision (e.g. colorizatio\\n',\n",
       " ' Multi-Agent Diverse Generative Adversarial Networks ; We propose an intuitive generalization to the Generative Adversarial Netwo\\n',\n",
       " ' Geometric GAN ; Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has i\\n',\n",
       " ' A Data and Model-Parallel, Distributed and Scalable Framework for   Training of Deep Networks in Apache Spark ; Training deep ne\\n',\n",
       " ' Understanding and Comparing Deep Neural Networks for Age and Gender   Classification ; Recently, deep neural networks have demon\\n',\n",
       " ' When is a Convolutional Filter Easy To Learn? ; We analyze the convergence of (stochastic) gradient descent algorithm for learni\\n',\n",
       " ' Learning Sparse Visual Representations with Leaky Capped Norm   Regularizers ; Sparsity inducing regularization is an important \\n',\n",
       " ' ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection,   Adversarial Examples and Model Criticism ; ConvNets and I\\n',\n",
       " \" Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of   Spurious Local Minima ; We consider the problem of learning a\\n\",\n",
       " ' Curiosity-driven Exploration by Self-supervised Prediction ; In many real-world scenarios, rewards extrinsic to the agent are ex\\n',\n",
       " ' Houdini: Fooling Deep Structured Prediction Models ; Generating adversarial examples is a critical step for evaluating and impro\\n',\n",
       " ' Recent Advances in Zero-shot Recognition ; With the recent renaissance of deep convolution neural networks, encouraging breakthr\\n',\n",
       " ' The loss surface and expressivity of deep convolutional neural networks ; We analyze the expressiveness and loss surface of prac\\n',\n",
       " ' Physics-guided Neural Networks (PGNN): An Application in Lake   Temperature Modeling ; This paper introduces a novel framework f\\n',\n",
       " ' Unified Spectral Clustering with Optimal Graph ; Spectral clustering has found extensive use in many areas. Most traditional spe\\n',\n",
       " ' On the Inductive Bias of Dropout ; Dropout is a simple but effective technique for learning in neural networks and other setting\\n',\n",
       " ' Surprising properties of dropout in deep networks ; We analyze dropout in deep networks with rectified linear units and the quad\\n',\n",
       " ' Training Probabilistic Spiking Neural Networks with First-to-spike   Decoding ; Third-generation neural networks, or Spiking Neu\\n',\n",
       " ' A Novel Clustering Algorithm Based on Quantum Games ; Enormous successes have been made by quantum algorithms during the last de\\n',\n",
       " ' Exact solutions to the nonlinear dynamics of learning in deep linear   neural networks ; Despite the widespread practical succes\\n',\n",
       " ' Entropy of Overcomplete Kernel Dictionaries ; In signal analysis and synthesis, linear approximation theory considers a linear d\\n',\n",
       " ' Rotation-invariant convolutional neural networks for galaxy morphology   prediction ; Measuring the morphological parameters of \\n',\n",
       " ' Kernel Nonnegative Matrix Factorization Without the Curse of the   Pre-image - Application to Unmixing Hyperspectral Images ; Th\\n',\n",
       " ' Approximation errors of online sparsification criteria ; Many machine learning frameworks, such as resource-allocating networks,\\n',\n",
       " ' Discrete Deep Feature Extraction: A Theory and New Architectures ; First steps towards a mathematical theory of deep convolution\\n',\n",
       " ' Neural Responding Machine for Short-Text Conversation ; We propose Neural Responding Machine (NRM), a neural network-based respo\\n',\n",
       " ' Deep Active Learning for Dialogue Generation ; We propose an online, end-to-end, neural generative conversational model for open\\n',\n",
       " ' Teaching Machines to Read and Comprehend ; Teaching machines to read natural language documents remains an elusive challenge. Ma\\n',\n",
       " ' Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models   of Meaning ; Deep compositional models of meaning actin\\n',\n",
       " ' A Deep Architecture for Semantic Matching with Multiple Positional   Sentence Representations ; Matching natural language senten\\n',\n",
       " ' LSTM Neural Reordering Feature for Statistical Machine Translation ; Artificial neural networks are powerful models, which have \\n',\n",
       " ' Learning Natural Language Inference with LSTM ; Natural language inference (NLI) is a fundamentally important task in natural la\\n',\n",
       " ' Quantifying the vanishing gradient and long distance dependency problem   in recursive neural networks and recursive LSTMs ; Rec\\n',\n",
       " ' Implicit Discourse Relation Classification via Multi-Task Neural   Networks ; Without discourse connectives, classifying implici\\n',\n",
       " ' Enhancing Sentence Relation Modeling with Auxiliary Character-level   Embedding ; Neural network based approaches for sentence r\\n',\n",
       " ' Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks   with Feedback Negative Sampling ; Previous studies in\\n',\n",
       " ' Question Answering over Knowledge Base with Neural Attention Combining   Global Knowledge Information ; With the rapid growth of\\n',\n",
       " ' Generating Natural Language Inference Chains ; The ability to reason with natural language is a fundamental prerequisite for man\\n',\n",
       " ' MuFuRU: The Multi-Function Recurrent Unit ; Recurrent neural networks such as the GRU and LSTM found wide adoption in natural la\\n',\n",
       " ' LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in   Recurrent Neural Networks ; Recurrent neural networks, and in \\n',\n",
       " ' Compression of Neural Machine Translation Models via Pruning ; Neural Machine Translation (NMT), like many other deep learning d\\n',\n",
       " ' Constructing a Natural Language Inference Dataset using Generative   Neural Networks ; Natural Language Inference is an importan\\n',\n",
       " ' Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain   Factoid Question Answering ; While question answering (QA\\n',\n",
       " ' Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM   Encoder-Decoder ; We present Tweet2Vec, a novel method for\\n',\n",
       " ' Online Segment to Segment Neural Transduction ; We introduce an online neural sequence to sequence model that learns to alternat\\n',\n",
       " ' Semantic Parsing with Semi-Supervised Sequential Autoencoders ; We present a novel semi-supervised approach for sequence transdu\\n',\n",
       " ' Exploiting Sentence and Context Representations in Deep Neural Models   for Spoken Language Understanding ; This paper presents \\n',\n",
       " ' The Neural Noisy Channel ; We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent \\n',\n",
       " ' Generative Deep Neural Networks for Dialogue: A Short Review ; Researchers have recently started investigating deep neural netwo\\n',\n",
       " ' Learning Python Code Suggestion with a Sparse Pointer Network ; To enhance developer productivity, all modern integrated develop\\n',\n",
       " ' OpenNMT: Open-Source Toolkit for Neural Machine Translation ; We describe an open-source toolkit for neural machine translation \\n',\n",
       " ' Making Neural QA as Simple as Possible but not Simpler ; Recent development of large-scale question answering (QA) datasets trig\\n',\n",
       " ' Survey of the State of the Art in Natural Language Generation: Core   tasks, applications and evaluation ; This paper surveys th\\n',\n",
       " ' A Constrained Sequence-to-Sequence Neural Model for Sentence   Simplification ; Sentence simplification reduces semantic complex\\n',\n",
       " ' Improved Neural Relation Detection for Knowledge Base Question Answering ; Relation detection is a core component for many NLP a\\n',\n",
       " ' ASR error management for improving spoken language understanding ; This paper addresses the problem of automatic speech recognit\\n',\n",
       " ' Dynamic Integration of Background Knowledge in Neural NLU Systems ; Common-sense or background knowledge is required to understa\\n',\n",
       " ' Rethinking Skip-thought: A Neighborhood based Approach ; We study the skip-thought model with neighborhood information as weak s\\n',\n",
       " ' Neural Domain Adaptation for Biomedical Question Answering ; Factoid question answering (QA) has recently benefited from the dev\\n',\n",
       " ' Neural Models for Key Phrase Detection and Question Generation ; We propose a two-stage neural model to tackle question generati\\n',\n",
       " ' Neural Question Answering at BioASQ 5B ; This paper describes our submission to the 2017 BioASQ challenge. We participated in Ta\\n',\n",
       " ' A Deep Network with Visual Text Composition Behavior ; While natural languages are compositional, how state-of-the-art neural mo\\n',\n",
       " ' Semi-supervised emotion lexicon expansion with label propagation and   specialized word embeddings ; There exist two main approa\\n',\n",
       " ' Modelling Protagonist Goals and Desires in First-Person Narrative ; Many genres of natural language text are narratively structu\\n',\n",
       " ' Understanding Grounded Language Learning Agents ; Neural network-based systems can now learn to locate the referents of words an\\n',\n",
       " ' Just ASK: Building an Architecture for Extensible Self-Service Spoken   Language Understanding ; This paper presents the design \\n',\n",
       " ' The NarrativeQA Reading Comprehension Challenge ; Reading comprehension (RC)---in contrast to information retrieval---requires i\\n',\n",
       " ' Cognitive Database: A Step towards Endowing Relational Databases with   Artificial Intelligence Capabilities ; We propose Cognit\\n',\n",
       " ' Feudal Reinforcement Learning for Dialogue Management in Large Domains ; Reinforcement learning (RL) is a promising approach to \\n',\n",
       " ' An Analysis of Neural Language Modeling at Multiple Scales ; Many of the leading approaches in language modeling introduce novel\\n',\n",
       " ' Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy   and Reverberant Environments ; We propose a spatial dif\\n',\n",
       " ' Character-Aware Neural Language Models ; We describe a simple neural language model that relies only on character-level inputs. \\n',\n",
       " ' Neural-based machine translation for medical text domain. Based on   European Medicines Agency leaflet texts ; The quality of ma\\n',\n",
       " ' Conditional Generation and Snapshot Learning in Neural Dialogue Systems ; Recently a variety of LSTM-based conditional language \\n',\n",
       " ' Dialog state tracking, a machine reading approach using Memory Network ; In an end-to-end dialog system, the aim of dialog state\\n',\n",
       " ' A Physical Metaphor to Study Semantic Drift ; In accessibility tests for digital preservation, over time we experience drifts of\\n',\n",
       " ' Optimizing Neural Network Hyperparameters with Gaussian Processes for   Dialog Act Classification ; Systems based on artificial \\n',\n",
       " ' A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder ; Speech Translation has always been about giving source \\n',\n",
       " ' Learning to Reason With Adaptive Computation ; Multi-hop inference is necessary for machine learning systems to successfully sol\\n',\n",
       " ' Feature-Augmented Neural Networks for Patient Note De-identification ; Patient notes contain a wealth of information of potentia\\n',\n",
       " ' Direct Acoustics-to-Word Models for English Conversational Speech   Recognition ; Recent work on end-to-end automatic speech rec\\n',\n",
       " ' Factorization tricks for LSTM networks ; We present two simple ways of reducing the number of parameters and accelerating the tr\\n',\n",
       " ' NeuroNER: an easy-to-use program for named-entity recognition based on   neural networks ; Named-entity recognition (NER) aims a\\n',\n",
       " ' Syllable-aware Neural Language Models: A Failure to Beat Character-aware   Ones ; Syllabification does not seem to improve word-\\n',\n",
       " ' A Benchmarking Environment for Reinforcement Learning Based Task   Oriented Dialogue Management ; Dialogue assistants are rapidl\\n',\n",
       " ' Reusing Weights in Subword-aware Neural Language Models ; We propose several ways of reusing subword embeddings and other weight\\n',\n",
       " ' Multi-task Learning of Pairwise Sequence Classification Tasks Over   Disparate Label Spaces ; We combine multi-task learning and\\n',\n",
       " ' Dynamic Memory Networks for Visual and Textual Question Answering ; Neural network architectures with memory and attention mecha\\n',\n",
       " ' Picture It In Your Mind: Generating High Level Visual Representations   From Textual Descriptions ; In this paper we tackle the \\n',\n",
       " ' Where to put the Image in an Image Caption Generator ; When a recurrent neural network language model is used for caption genera\\n',\n",
       " ' A Focused Dynamic Attention Model for Visual Question Answering ; Visual Question and Answering (VQA) problems are attracting in\\n',\n",
       " ' Simple Image Description Generator via a Linear Phrase-Based Approach ; Generating a novel textual description of an image is an\\n',\n",
       " ' Multimodal Convolutional Neural Networks for Matching Image and Sentence ; In this paper, we propose multimodal convolutional ne\\n',\n",
       " ' Learning to Compose Neural Networks for Question Answering ; We describe a question answering model that applies to both images \\n',\n",
       " ' Signer-independent Fingerspelling Recognition with Deep Neural Network   Adaptation ; We study the problem of recognition of fin\\n',\n",
       " ' Full-Network Embedding in a Multimodal Embedding Pipeline ; The current state-of-the-art for image annotation and image retrieva\\n',\n",
       " ' What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption   Generator? ; In neural image captioning systems, a re\\n',\n",
       " ' A Fixed-Size Encoding Method for Variable-Length Sequences with its   Application to Neural Network Language Models ; In this pa\\n',\n",
       " ' Transition-Based Dependency Parsing with Stack Long Short-Term Memory ; We propose a technique for learning representations of p\\n',\n",
       " ' A Semisupervised Approach for Language Identification based on Ladder   Networks ; In this study we address the problem of train\\n',\n",
       " ' First-Pass Large Vocabulary Continuous Speech Recognition using   Bi-Directional Recurrent DNNs ; We present a method to perform\\n',\n",
       " ' Applying deep learning techniques on medical corpora from the World Wide   Web: a prototypical system and evaluation ; BACKGROUN\\n',\n",
       " ' Syntax-based Deep Matching of Short Texts ; Many tasks in natural language processing, ranging from machine translation to quest\\n',\n",
       " ' Ensemble of Generative and Discriminative Techniques for Sentiment   Analysis of Movie Reviews ; Sentiment analysis is a common \\n',\n",
       " ' Diverse Embedding Neural Network Language Models ; We propose Diverse Embedding Neural Network (DENN), a novel architecture for \\n',\n",
       " ' Learning linearly separable features for speech recognition using   convolutional neural networks ; Automatic speech recognition\\n',\n",
       " ' Learning to Transduce with Unbounded Memory ; Recently, strong results have been demonstrated by Deep Recurrent Neural Networks \\n',\n",
       " ' Feedforward Sequential Memory Neural Networks without Recurrent Feedback ; We introduce a new structure for memory neural networ\\n',\n",
       " ' Towards Structured Deep Neural Network for Automatic Speech Recognition ; In this paper we propose the Structured Deep Neural Ne\\n',\n",
       " ' Character-Level Incremental Speech Recognition with Recurrent Neural   Networks ; In real-time speech recognition applications, \\n',\n",
       " ' Globally Normalized Transition-Based Neural Networks ; We introduce a globally normalized transition-based neural network model \\n',\n",
       " ' Clinical Information Extraction via Convolutional Neural Network ; We report an implementation of a clinical information extract\\n',\n",
       " ' Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations ; We propose zoneout, a novel method for regularizing RNNs.\\n',\n",
       " ' Stance Detection with Bidirectional Conditional Encoding ; Stance detection is the task of classifying the attitude expressed in\\n',\n",
       " ' SMS Spam Filtering using Probabilistic Topic Modelling and Stacked   Denoising Autoencoder ; In This paper we present a novel ap\\n',\n",
       " ' Bidirectional Recurrent Neural Networks for Medical Event Detection in   Electronic Health Records ; Sequence labeling for extra\\n',\n",
       " ' Sequence Training and Adaptation of Highway Deep Neural Networks ; Highway deep neural network (HDNN) is a type of depth-gated f\\n',\n",
       " ' Recurrent Highway Networks ; Many sequential processing tasks require complex nonlinear transition functions from one step to th\\n',\n",
       " ' Towards cross-lingual distributed representations without parallel text   trained with adversarial autoencoders ; Current approa\\n',\n",
       " ' Memory Visualization for Gated Recurrent Neural Networks in Speech   Recognition ; Recurrent neural networks (RNNs) have shown c\\n',\n",
       " ' Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large   Vocabulary Speech Recognition ; We present results that show i\\n',\n",
       " ' Unsupervised Pretraining for Sequence to Sequence Learning ; This work presents a general unsupervised learning method to improv\\n',\n",
       " ' Structured Attention Networks ; Attention networks have proven to be an effective approach for embedding categorical inference w\\n',\n",
       " ' End-to-End Multi-View Networks for Text Classification ; We propose a multi-view network for text classification. Our method aut\\n',\n",
       " ' Differentiable Scheduled Sampling for Credit Assignment ; We demonstrate that a continuous relaxation of the argmax operation ca\\n',\n",
       " ' Phone-aware Neural Language Identification ; Pure acoustic neural models, particularly the LSTM-RNN model, have shown great pote\\n',\n",
       " ' Detecting Off-topic Responses to Visual Prompts ; Automated methods for essay scoring have made great progress in recent years, \\n',\n",
       " ' Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation   Functions in Quasi-Recurrent Neural Networks ; In this\\n',\n",
       " ' Fidelity-Weighted Learning ; Training deep neural networks requires many training samples, but in practice training labels are e\\n',\n",
       " ' Feature Learning in Deep Neural Networks - Studies on Speech Recognition   Tasks ; Recent studies have shown that deep neural ne\\n',\n",
       " ' Estimating Phoneme Class Conditional Probabilities from Raw Speech   Signal using Convolutional Neural Networks ; In hybrid hidd\\n',\n",
       " ' Recursive Neural Networks Can Learn Logical Semantics ; Tree-structured recursive neural networks (TreeRNNs) for sentence meanin\\n',\n",
       " ' A Re-ranking Model for Dependency Parser with Recursive Convolutional   Neural Network ; In this work, we address the problem to\\n',\n",
       " ' Deep Speaker Vectors for Semi Text-independent Speaker Verification ; Recent research shows that deep neural networks (DNNs) can\\n',\n",
       " ' Advances in Very Deep Convolutional Neural Networks for LVCSR ; Very deep CNNs with small 3x3 kernels have recently been shown t\\n',\n",
       " ' Learning Compact Recurrent Neural Networks ; Recurrent neural networks (RNNs), including long short-term memory (LSTM) RNNs, hav\\n',\n",
       " ' Dependency Parsing with LSTMs: An Empirical Evaluation ; We propose a transition-based dependency parser using Recurrent Neural \\n',\n",
       " ' Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis   and Application to Information Retrieval ; This paper \\n',\n",
       " ' Encoding Source Language with Convolutional Neural Network for Machine   Translation ; The recently proposed neural network join\\n',\n",
       " ' Maximum a Posteriori Adaptation of Network Parameters in Deep Models ; We present a Bayesian approach to adapting parameters of \\n',\n",
       " ' Context-Dependent Translation Selection Using Convolutional Neural   Network ; We propose a novel method for translation selecti\\n',\n",
       " ' Convolutional Neural Network Architectures for Matching Natural Language   Sentences ; Semantic matching is of central importanc\\n',\n",
       " ' Long Short-Term Memory Over Tree Structures ; The chain-structured long short-term memory (LSTM) has showed to be effective in a\\n',\n",
       " ' Improving the Performance of Neural Machine Translation Involving   Morphologically Rich Languages ; The advent of the attention\\n',\n",
       " ' A recurrent neural network without chaos ; We introduce an exceptionally simple gated recurrent neural network (RNN) that achiev\\n',\n",
       " ' End-to-end Phoneme Sequence Recognition using Convolutional Neural   Networks ; Most phoneme recognition state-of-the-art system\\n',\n",
       " ' A Deep Learning Approach to Data-driven Parameterizations for   Statistical Parametric Speech Synthesis ; Nearly all Statistical\\n',\n",
       " ' Addressing the Rare Word Problem in Neural Machine Translation ; Neural Machine Translation (NMT) is a new approach to machine t\\n',\n",
       " ' Investigating the Role of Prior Disambiguation in Deep-learning   Compositional Models of Meaning ; This paper aims to explore t\\n',\n",
       " ' Deep Speech: Scaling up end-to-end speech recognition ; We present a state-of-the-art speech recognition system developed using \\n',\n",
       " ' Incremental Adaptation Strategies for Neural Network Language Models ; It is today acknowledged that neural network language mod\\n',\n",
       " ' Joint RNN-Based Greedy Parsing and Word Composition ; This paper introduces a greedy parser based on neural networks, which leve\\n',\n",
       " ' Efficient Exact Gradient Update for training Deep Networks with Very   Large Sparse Targets ; An important class of problems inv\\n',\n",
       " ' Discriminative Neural Sentence Modeling by Tree-Based Convolution ; This paper proposes a tree-based convolutional neural networ\\n',\n",
       " ' Self-Adaptive Hierarchical Sentence Model ; The ability to accurately model a sentence at varying stages (e.g., word-phrase-sent\\n',\n",
       " ' Classifying Relations by Ranking with Convolutional Neural Networks ; Relation classification is an important semantic processin\\n',\n",
       " ' Lexical Translation Model Using a Deep Neural Network Architecture ; In this paper we combine the advantages of a model using gl\\n',\n",
       " ' Visualizing and Understanding Recurrent Networks ; Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-\\n',\n",
       " ' A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for   Unsupervised Discovery of Linguistic Units and Generatio\\n',\n",
       " ' Author Identification using Multi-headed Recurrent Neural Networks ; Recurrent neural networks (RNNs) are very good at modelling\\n',\n",
       " ' A Deep Memory-based Architecture for Sequence-to-Sequence Learning ; We propose DEEPMEMORY, a novel deep architecture for sequen\\n',\n",
       " ' Ask Me Anything: Dynamic Memory Networks for Natural Language Processing ; Most tasks in natural language processing can be cast\\n',\n",
       " ' Improved Deep Speaker Feature Learning for Text-Dependent Speaker   Recognition ; A deep learning approach has been proposed rec\\n',\n",
       " ' Grid Long Short-Term Memory ; This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidime\\n',\n",
       " ' A Dependency-Based Neural Network for Relation Classification ; Previous research on relation classification has verified the ef\\n',\n",
       " ' PTE: Predictive Text Embedding through Large-scale Heterogeneous Text   Networks ; Unsupervised text embedding methods, such as \\n',\n",
       " ' Relation Classification via Recurrent Neural Network ; Deep learning has gained much success in sentence-level relation classifi\\n',\n",
       " ' Learning from LDA using Deep Neural Networks ; Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian model fo\\n',\n",
       " ' Online Representation Learning in Recurrent Neural Language Models ; We investigate an extension of continuous online learning i\\n',\n",
       " \" A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional   Neural Networks for Sentence Classification ; Convolutio\\n\",\n",
       " ' Prediction-Adaptation-Correction Recurrent Neural Networks for   Low-Resource Language Speech Recognition ; In this paper, we in\\n',\n",
       " ' Generating Text with Deep Reinforcement Learning ; We introduce a novel schema for sequence to sequence learning with a Deep Q-N\\n',\n",
       " ' Detecting Interrogative Utterances with Recurrent Neural Networks ; In this paper, we explore different neural network architect\\n',\n",
       " ' A Neural Transducer ; Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitabl\\n',\n",
       " ' Skip-Thought Memory Networks ; Question Answering (QA) is fundamental to natural language processing in that most nlp problems c\\n',\n",
       " ' Named Entity Recognition with Bidirectional LSTM-CNNs ; Named entity recognition is a challenging task that has traditionally re\\n',\n",
       " ' Generating News Headlines with Recurrent Neural Networks ; We describe an application of an encoder-decoder recurrent neural net\\n',\n",
       " ' Words are not Equal: Graded Weighting Model for building Composite   Document Vectors ; Despite the success of distributional se\\n',\n",
       " ' Small-footprint Deep Neural Networks with Highway Connections for Speech   Recognition ; For speech recognition, deep neural net\\n',\n",
       " ' Backward and Forward Language Modeling for Constrained Sentence   Generation ; Recent language models, especially those based on\\n',\n",
       " ' Online Keyword Spotting with a Character-Level Recurrent Neural Network ; In this paper, we propose a context-aware keyword spot\\n',\n",
       " ' Domain Specific Author Attribution Based on Feedforward Neural Network   Language Models ; Authorship attribution refers to the \\n',\n",
       " ' Segmental Recurrent Neural Networks for End-to-end Speech Recognition ; We study the segmental recurrent neural network for end-\\n',\n",
       " ' How Transferable are Neural Networks in NLP Applications? ; Transfer learning is aimed to make use of valuable knowledge in a so\\n',\n",
       " ' Recurrent Neural Network Encoder with Attention for Community Question   Answering ; We apply a general recurrent neural network\\n',\n",
       " ' Recursive Neural Language Architecture for Tag Prediction ; We consider the problem of learning distributed representations for \\n',\n",
       " ' On the Compression of Recurrent Neural Networks with an Application to   LVCSR acoustic modeling for Embedded Speech Recognition\\n',\n",
       " ' Pointing the Unknown Words ; The problem of rare and unknown words is an important issue that can potentially influence the perf\\n',\n",
       " ' Learning Multiscale Features Directly From Waveforms ; Deep learning has dramatically improved the performance of speech recogni\\n',\n",
       " ' Joint Learning of Sentence Embeddings for Relevance and Entailment ; We consider the problem of Recognizing Textual Entailment w\\n',\n",
       " ' Deep API Learning ; Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. \\n',\n",
       " ' Does Multimodality Help Human and Machine for Translation and Image   Captioning? ; This paper presents the systems developed by\\n',\n",
       " ' Very Deep Convolutional Networks for Text Classification ; The dominant approach for many NLP tasks are recurrent neural network\\n',\n",
       " ' Improving Recurrent Neural Networks For Sequence Labelling ; In this paper we study different types of Recurrent Neural Networks\\n',\n",
       " ' Sentence Similarity Measures for Fine-Grained Estimation of Topical   Relevance in Learner Essays ; We investigate the task of a\\n',\n",
       " ' Deep CNNs along the Time Axis with Intermap Pooling for Robustness to   Spectral Variations ; Convolutional neural networks (CNN\\n',\n",
       " ' Automatic Text Scoring Using Neural Networks ; Automated Text Scoring (ATS) provides a cost-effective and consistent alternative\\n',\n",
       " ' A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic   Modeling in Speech Recognition ; We present a comprehensive\\n',\n",
       " ' Sequence-Level Knowledge Distillation ; Neural machine translation (NMT) offers a novel alternative formulation of translation t\\n',\n",
       " ' Learning Semantically Coherent and Reusable Kernels in Convolution   Neural Nets for Sentence Classification ; The state-of-the-\\n',\n",
       " ' RETURNN: The RWTH Extensible Training framework for Universal Recurrent   Neural Networks ; In this work we release our extensib\\n',\n",
       " ' Character-Level Language Modeling with Hierarchical Recurrent Neural   Networks ; Recurrent neural network (RNN) based character\\n',\n",
       " ' Multi-task Recurrent Model for True Multilingual Speech Recognition ; Research on multilingual speech recognition remains attrac\\n',\n",
       " ' Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep   Recurrent models ; Sentiment Analysis (SA) is an acti\\n',\n",
       " ' Attending to Characters in Neural Sequence Labeling Models ; Sequence labeling architectures use word embeddings for capturing s\\n',\n",
       " ' Visualizing and Understanding Curriculum Learning for Long Short-Term   Memory Networks ; Curriculum Learning emphasizes the ord\\n',\n",
       " ' Dense Prediction on Sequences with Time-Dilated Convolutions for Speech   Recognition ; In computer vision pixelwise dense predi\\n',\n",
       " ' End-to-End ASR-free Keyword Search from Speech ; End-to-end (E2E) systems have achieved competitive results compared to conventi\\n',\n",
       " ' Training Language Models Using Target-Propagation ; While Truncated Back-Propagation through Time (BPTT) is the most popular app\\n',\n",
       " ' Deep Voice: Real-time Neural Text-to-Speech ; We present Deep Voice, a production-quality text-to-speech system constructed enti\\n',\n",
       " ' Improved Variational Autoencoders for Text Modeling using Dilated   Convolutions ; Recent work on generative modeling of text ha\\n',\n",
       " ' Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence   Labelling ; Most existing sequence labelling models r\\n',\n",
       " ' Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model) ; We examine Memory Networks for the task of question answerin\\n',\n",
       " ' Simplified End-to-End MMI Training and Voting for ASR ; A simplified speech recognition system that uses the maximum mutual info\\n',\n",
       " ' Learning to Generate Reviews and Discovering Sentiment ; We explore the properties of byte-level recurrent language models. When\\n',\n",
       " ' Semi-supervised Multitask Learning for Sequence Labeling ; We propose a sequence labeling framework with a secondary training ob\\n',\n",
       " ' Going Wider: Recurrent Neural Network With Parallel Cells ; Recurrent Neural Network (RNN) has been widely applied for sequence \\n',\n",
       " ' Phonetic Temporal Neural Model for Language Identification ; Deep neural models, particularly the LSTM-RNN model, have shown gre\\n',\n",
       " ' Relevance-based Word Embedding ; Learning a high-dimensional dense representation for vocabulary terms, also known as a word emb\\n',\n",
       " ' Deriving Neural Architectures from Sequence and Graph Kernels ; The design of neural architectures for structured objects is typ\\n',\n",
       " ' On Multilingual Training of Neural Dependency Parsers ; We show that a recently proposed neural dependency parser can be improve\\n',\n",
       " ' Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks ; Ladder networks are a notable new concept in the field of s\\n',\n",
       " ' Adversarially Regularized Autoencoders ; While autoencoders are a key technique in representation learning for continuous struct\\n',\n",
       " ' Auxiliary Objectives for Neural Error Detection Models ; We investigate the utility of different auxiliary objectives and traini\\n',\n",
       " ' An Error-Oriented Approach to Word Embedding Pre-Training ; We propose a novel word embedding pre-training approach that exploit\\n',\n",
       " ' A Continuous Relaxation of Beam Search for End-to-end Training of Neural   Sequence Models ; Beam search is a desirable choice o\\n',\n",
       " ' Regularizing and Optimizing LSTM Language Models ; Recurrent neural networks (RNNs), such as long short-term memory networks (LS\\n',\n",
       " ' Supervised Speech Separation Based on Deep Learning: An Overview ; Speech separation is the task of separating target speech fro\\n',\n",
       " ' Grasping the Finer Point: A Supervised Similarity Network for Metaphor   Detection ; The ubiquity of metaphor in our everyday co\\n',\n",
       " ' Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words ; Distributed word embeddings have shown superior perfo\\n',\n",
       " ' KeyVec: Key-semantics Preserving Document Representations ; Previous studies have demonstrated the empirical success of word emb\\n',\n",
       " ' Exploring Asymmetric Encoder-Decoder Structure for Context-based   Sentence Representation Learning ; Context information plays \\n',\n",
       " ' CNN Is All You Need ; The Convolution Neural Network (CNN) has demonstrated the unique advantage in audio, image and text learni\\n',\n",
       " ' Combining Representation Learning with Logic for Language Processing ; The current state-of-the-art in many natural language pro\\n',\n",
       " ' A Note on Topology Preservation in Classification, and the Construction   of a Universal Neuron Grid ; It will be shown that acc\\n',\n",
       " ' Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On   Boltzmann Machines ; One conjecture in both deep learni\\n',\n",
       " ' Mapping Temporal Variables into the NeuCube for Improved Pattern   Recognition, Predictive Modelling and Understanding of Stream\\n',\n",
       " ' An Evolutionary Algorithm to Learn SPARQL Queries for   Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia \\n',\n",
       " ' A Geometric Framework for Convolutional Neural Networks ; In this paper, a geometric framework for neural networks is proposed. \\n',\n",
       " ' A Novel Representation of Neural Networks ; Deep Neural Networks (DNNs) have become very popular for prediction in many areas. T\\n',\n",
       " ' Converting Cascade-Correlation Neural Nets into Probabilistic Generative   Models ; Humans are not only adept in recognizing wha\\n',\n",
       " ' On the Performance of Network Parallel Training in Artificial Neural   Networks ; Artificial Neural Networks (ANNs) have receive\\n',\n",
       " ' Programmable Agents ; We build deep RL agents that execute declarative programs expressed in formal language. The agents learn t\\n',\n",
       " ' Explainable Artificial Intelligence: Understanding, Visualizing and   Interpreting Deep Learning Models ; With the availability \\n',\n",
       " ' Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games? ; Deep reinforcement learning has achieved many recent succ\\n',\n",
       " ' Emergence of grid-like representations by training recurrent neural   networks to perform spatial localization ; Decades of rese\\n',\n",
       " ' Dimensionality Reduction and Reconstruction using Mirroring Neural   Networks and Object Recognition based on Reduced Dimension \\n',\n",
       " ' Parcellation of fMRI Datasets with ICA and PLS-A Data Driven Approach ; Inter-subject parcellation of functional Magnetic Resona\\n',\n",
       " ' Iris Codes Classification Using Discriminant and Witness Directions ; The main topic discussed in this paper is how to use intel\\n',\n",
       " ' Algorithms for Image Analysis and Combination of Pattern Classifiers   with Application to Medical Diagnosis ; Medical Informati\\n',\n",
       " ' Deep Neural Networks are Easily Fooled: High Confidence Predictions for   Unrecognizable Images ; Deep neural networks (DNNs) ha\\n',\n",
       " ' Homogeneous Spiking Neuromorphic System for Real-World Pattern   Recognition ; A neuromorphic chip that combines CMOS analog spi\\n',\n",
       " ' Crowd Behavior Analysis: A Review where Physics meets Biology ; Although the traits emerged in a mass gathering are often non-de\\n',\n",
       " ' Can Pretrained Neural Networks Detect Anatomy? ; Convolutional neural networks demonstrated outstanding empirical results in com\\n',\n",
       " ' Metaheuristic Algorithms for Convolution Neural Network ; A typical modern optimization technique is usually either heuristic or\\n',\n",
       " ' Hadamard Product for Low-rank Bilinear Pooling ; Bilinear models provide rich representations compared with linear models. They \\n',\n",
       " ' Incremental Network Quantization: Towards Lossless CNNs with   Low-Precision Weights ; This paper presents incremental network q\\n',\n",
       " ' LesionSeg: Semantic segmentation of skin lesions using Deep   Convolutional Neural Network ; We present a method for skin lesion\\n',\n",
       " ' Convolutional Spike Timing Dependent Plasticity based Feature Learning   in Spiking Neural Networks ; Brain-inspired learning mo\\n',\n",
       " ' Adversarial Transformation Networks: Learning to Generate Adversarial   Examples ; Multiple different approaches of generating a\\n',\n",
       " ' Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced   Attentive Response Approach for Explaining and Visual\\n',\n",
       " ' Fast YOLO: A Fast You Only Look Once System for Real-time Embedded   Object Detection in Video ; Object detection is considered \\n',\n",
       " ' NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm ; Neural networks (NNs) have begun to have a pervasive \\n',\n",
       " ' Analysis of supervised and semi-supervised GrowCut applied to   segmentation of masses in mammography images ; Breast cancer is \\n',\n",
       " ' Empirical Explorations in Training Networks with Discrete Activations ; We present extensive experiments training and testing hi\\n',\n",
       " ' Regularized Evolution for Image Classifier Architecture Search ; The effort devoted to hand-crafting image classifiers has motiv\\n',\n",
       " ' Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network   for Real-time Embedded Object Detection ; Object dete\\n',\n",
       " ' Inferencing Based on Unsupervised Learning of Disentangled   Representations ; Combining Generative Adversarial Networks (GANs) \\n',\n",
       " ' The Parameter-Less Self-Organizing Map algorithm ; The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network algori\\n',\n",
       " ' Simplified firefly algorithm for 2D image key-points search ; In order to identify an object, human eyes firstly search the fiel\\n',\n",
       " ' Deep-Plant: Plant Identification with convolutional neural networks ; This paper studies convolutional neural networks (CNN) to \\n',\n",
       " ' Adapting Deep Network Features to Capture Psychological Representations ; Deep neural networks have become increasingly successf\\n',\n",
       " ' Large-Scale Evolution of Image Classifiers ; Neural networks have proven effective at solving difficult problems but designing t\\n',\n",
       " ' A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification   and Domain Adaptation ; Recently, DNN model compression \\n',\n",
       " ' Identifying Spatial Relations in Images using Convolutional Neural   Networks ; Traditional approaches to building a large scale\\n',\n",
       " ' Hierarchical Attentive Recurrent Tracking ; Class-agnostic object tracking is particularly difficult in cluttered environments a\\n',\n",
       " ' PSIque: Next Sequence Prediction of Satellite Images using a   Convolutional Sequence-to-Sequence Network ; Predicting unseen we\\n',\n",
       " \" Evaluation of Alzheimer's Disease by Analysis of MR Images using   Multilayer Perceptrons and Kohonen SOM Classifiers as an Alte\\n\",\n",
       " ' Neural tuning size is a key factor underlying holistic face processing ; Faces are a class of visual stimuli with unique signifi\\n',\n",
       " ' Distribution of the search of evolutionary product unit neural networks   for classification ; This paper deals with the distrib\\n',\n",
       " ' Correlation Alignment for Unsupervised Domain Adaptation ; In this chapter, we present CORrelation ALignment (CORAL), a simple y\\n',\n",
       " \" CITlab ARGUS for historical handwritten documents ; We describe CITlab's recognition system for the HTRtS competition attached t\\n\",\n",
       " ' Generalized Haar Filter based Deep Networks for Real-Time Object   Detection in Traffic Scene ; Vision-based object detection is\\n',\n",
       " ' Autoencoder Regularized Network For Driving Style Representation   Learning ; In this paper, we study learning generalized drivi\\n',\n",
       " ' Fashioning with Networks: Neural Style Transfer to Design Clothes ; Convolutional Neural Networks have been highly successful in\\n',\n",
       " ' GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from   Remote Sensing Imagery ; Advances in remote sensing tech\\n',\n",
       " ' Improving Efficiency in Convolutional Neural Network with Multilinear   Filters ; The excellent performance of deep neural netwo\\n',\n",
       " ' Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided   Diagnosis of Diabetic Retinopathy ; Objective: Radiomics-drive\\n',\n",
       " ' HP-GAN: Probabilistic 3D human motion prediction via GAN ; Predicting and understanding human motion dynamics has many applicati\\n',\n",
       " ' Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro   Gesture ; In the research of the impact of gestures usin\\n',\n",
       " ' Nature vs. Nurture: The Role of Environmental Resources in Evolutionary   Deep Intelligence ; Evolutionary deep intelligence syn\\n',\n",
       " ' A stochastic model of human visual attention with a dynamic Bayesian   network ; Recent studies in the field of human vision sci\\n',\n",
       " ' Smart Content Recognition from Images Using a Mixture of Convolutional   Neural Networks ; With rapid development of the Interne\\n',\n",
       " ' Cortical spatio-temporal dimensionality reduction for visual grouping ; The visual systems of many mammals, including humans, is\\n',\n",
       " ' Visual Sentiment Prediction with Deep Convolutional Neural Networks ; Images have become one of the most popular types of media \\n',\n",
       " ' Correntropy Maximization via ADMM - Application to Robust Hyperspectral   Unmixing ; In hyperspectral images, some spectral band\\n',\n",
       " ' Identifying individual facial expressions by deconstructing a neural   network ; This paper focuses on the problem of explaining\\n',\n",
       " ' Object Boundary Detection and Classification with Image-level Labels ; Semantic boundary and edge detection aims at simultaneous\\n',\n",
       " ' Evolving Spatially Aggregated Features from Satellite Imagery for   Regional Modeling ; Satellite imagery and remote sensing pro\\n',\n",
       " ' Pillar Networks++: Distributed non-parametric deep and wide networks ; In recent work, it was shown that combining multi-kernel \\n',\n",
       " ' Market-Based Reinforcement Learning in Partially Observable Worlds ; Unlike traditional reinforcement learning (RL), market-base\\n',\n",
       " ' Controlled hierarchical filtering: Model of neocortical sensory   processing ; A model of sensory information processing is pres\\n',\n",
       " ' When Do Differences Matter? On-Line Feature Extraction Through Cognitive   Economy ; For an intelligent agent to be truly autono\\n',\n",
       " ' Applying Policy Iteration for Training Recurrent Neural Networks ; Recurrent neural networks are often used for learning time-se\\n',\n",
       " ' A Neural-Network Technique to Learn Concepts from Electroencephalograms ; A new technique is presented developed to learn multi-\\n',\n",
       " ' Empirical learning aided by weak domain knowledge in the form of feature   importance ; Standard hybrid learners that use domain\\n',\n",
       " ' Evolutionary Algorithms for Reinforcement Learning ; There are two distinct approaches to solving reinforcement learning problem\\n',\n",
       " ' On Training Deep Boltzmann Machines ; The deep Boltzmann machine (DBM) has been an important development in the quest for powerf\\n',\n",
       " ' Memristive fuzzy edge detector ; Fuzzy inference systems always suffer from the lack of efficient structures or platforms for th\\n',\n",
       " ' Echo State Queueing Network: a new reservoir computing learning tool ; In the last decade, a new computational paradigm was intr\\n',\n",
       " ' The Predictron: End-To-End Learning and Planning ; One of the key challenges of artificial intelligence is to learn models that \\n',\n",
       " ' Quadratically constrained quadratic programming for classification using   particle swarms and applications ; Particle swarm opt\\n',\n",
       " ' Learning to Execute ; Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are\\n',\n",
       " ' Bitwise Neural Networks ; Based on the assumption that there exists a neural network that efficiently represents a set of Boolea\\n',\n",
       " ' Graying the black box: Understanding DQNs ; In recent years there is a growing interest in using deep representations for reinfo\\n',\n",
       " ' Evaluation of a Tree-based Pipeline Optimization Tool for Automating   Data Science ; As the field of data science continues to \\n',\n",
       " ' Probabilistic Reasoning via Deep Learning: Neural Association Models ; In this paper, we propose a new deep learning approach, c\\n',\n",
       " ' Deep Reinforcement Learning With Macro-Actions ; Deep reinforcement learning has been shown to be a powerful framework for learn\\n',\n",
       " ' RETAIN: An Interpretable Predictive Model for Healthcare using Reverse   Time Attention Mechanism ; Accuracy and interpretabilit\\n',\n",
       " ' A High Speed Multi-label Classifier based on Extreme Learning Machines ; In this paper a high speed neural network classifier ba\\n',\n",
       " ' An Online Universal Classifier for Binary, Multi-class and Multi-label   Classification ; Classification involves the learning o\\n',\n",
       " ' Adaptive Online Sequential ELM for Concept Drift Tackling ; A machine learning method needs to adapt to over time changes in the\\n',\n",
       " ' Adaptive Convolutional ELM For Concept Drift Handling in Online Stream   Data ; In big data era, the data continuously generated\\n',\n",
       " ' Particle Swarm Optimization for Generating Interpretable Fuzzy   Reinforcement Learning Policies ; Fuzzy controllers are efficie\\n',\n",
       " ' A Growing Long-term Episodic & Semantic Memory ; The long-term memory of most connectionist systems lies entirely in the weights\\n',\n",
       " ' Cognitive Discriminative Mappings for Rapid Learning ; Humans can learn concepts or recognize items from just a handful of examp\\n',\n",
       " ' Towards a Mathematical Understanding of the Difficulty in Learning with   Feedforward Neural Networks ; Training deep neural net\\n',\n",
       " ' An effective algorithm for hyperparameter optimization of neural   networks ; A major challenge in designing neural network (NN)\\n',\n",
       " ' Evolutionary Training of Sparse Artificial Neural Networks: A Network   Science Perspective ; Through the success of deep learni\\n',\n",
       " ' Attend and Predict: Understanding Gene Regulation by Selective Attention   on Chromatin ; The past decade has seen a revolution \\n',\n",
       " ' Parallelizing Linear Recurrent Neural Nets Over Sequence Length ; Recurrent neural networks (RNNs) are widely used to model sequ\\n',\n",
       " ' Feature learning in feature-sample networks using multi-objective   optimization ; Data and knowledge representation are fundame\\n',\n",
       " ' Meta-Learning and Universality: Deep Representations and Gradient   Descent can Approximate any Learning Algorithm ; Learning to\\n',\n",
       " ' Hindsight policy gradients ; Goal-conditional policies allow reinforcement learning agents to pursue specific goals during diffe\\n',\n",
       " ' SquishedNets: Squishing SqueezeNet further for edge device scenarios via   deep evolutionary synthesis ; While deep neural netwo\\n',\n",
       " ' Autonomous development and learning in artificial intelligence and   robotics: Scaling up deep learning to human--like learning \\n',\n",
       " ' Learning from Scarce Experience ; Searching the space of policies directly for the optimal policy has been one popular method fo\\n',\n",
       " ' Fitness inheritance in the Bayesian optimization algorithm ; This paper describes how fitness inheritance can be used to estimat\\n',\n",
       " ' The Combined Technique for Detection of Artifacts in Clinical   Electroencephalograms of Sleeping Newborns ; In this paper we de\\n',\n",
       " ' Evolving Classifiers: Methods for Incremental Learning ; The ability of a classifier to take on new information and classes by e\\n',\n",
       " ' Automatic Pattern Classification by Unsupervised Learning Using   Dimensionality Reduction of Data with Mirroring Neural Network\\n',\n",
       " ' Improving the Performance of PieceWise Linear Separation Incremental   Algorithms for Practical Hardware Implementations ; In th\\n',\n",
       " ' A Novel Rough Set Reduct Algorithm for Medical Domain Based on Bee   Colony Optimization ; Feature selection refers to the probl\\n',\n",
       " ' Automated Query Learning with Wikipedia and Genetic Programming ; Most of the existing information retrieval systems are based o\\n',\n",
       " ' Scaling Up Estimation of Distribution Algorithms For Continuous   Optimization ; Since Estimation of Distribution Algorithms (ED\\n',\n",
       " ' Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA ; An automated technique has recently been proposed to tra\\n',\n",
       " ' Discrete Dynamical Genetic Programming in XCS ; A number of representation schemes have been presented for use within Learning C\\n',\n",
       " ' Fuzzy Dynamical Genetic Programming in XCSF ; A number of representation schemes have been presented for use within Learning Cla\\n',\n",
       " ' Learning-Based Procedural Content Generation ; Procedural content generation (PCG) has recently become one of the hottest topics\\n',\n",
       " ' Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in   the Othello League ; N-tuple networks have been successf\\n',\n",
       " ' Towards a Self-Organized Agent-Based Simulation Model for Exploration of   Human Synaptic Connections ; In this paper, the early\\n',\n",
       " ' Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural   Network ; The paper presents the electronic design and m\\n',\n",
       " ' Learning Bayesian Network Equivalence Classes with Ant Colony   Optimization ; Bayesian networks are a useful tool in the repres\\n',\n",
       " ' Probabilistic Neural Programs ; We present probabilistic neural programs, a framework for program induction that permits flexibl\\n',\n",
       " ' Cognitive Deep Machine Can Train Itself ; Machine learning is making substantial progress in diverse applications. The success i\\n',\n",
       " ' Summary - TerpreT: A Probabilistic Programming Language for Program   Induction ; We study machine learning formulations of indu\\n',\n",
       " ' Learning in the Machine: Random Backpropagation and the Deep Learning   Channel ; Random backpropagation (RBP) is a variant of t\\n',\n",
       " ' Highway and Residual Networks learn Unrolled Iterative Estimation ; The past year saw the introduction of new architectures such\\n',\n",
       " ' Deep neural heart rate variability analysis ; Despite of the pain and limited accuracy of blood tests for early recognition of c\\n',\n",
       " ' A neural network approach to ordinal regression ; Ordinal regression is an important type of learning, which has properties of b\\n',\n",
       " ' Computational Model of Music Sight Reading: A Reinforcement Learning   Approach ; Although the Music Sight Reading process has b\\n',\n",
       " ' Using Artificial Bee Colony Algorithm for MLP Training on Earthquake   Time Series Data Prediction ; Nowadays, computer scientis\\n',\n",
       " ' Multiple chaotic central pattern generators with learning for legged   locomotion and malfunction compensation ; An originally c\\n',\n",
       " ' Teaching Deep Convolutional Neural Networks to Play Go ; Mastering the game of Go has remained a long standing challenge to the \\n',\n",
       " ' Polyphonic Music Generation by Modeling Temporal Dependencies Using a   RNN-DBN ; In this paper, we propose a generic technique \\n',\n",
       " ' Massively Parallel Methods for Deep Reinforcement Learning ; We present the first massively distributed architecture for deep re\\n',\n",
       " ' A genetic algorithm for autonomous navigation in partially observable   domain ; The problem of autonomous navigation is one of \\n',\n",
       " ' Distributed Deep Q-Learning ; We propose a distributed deep learning model to successfully learn control policies directly from \\n',\n",
       " ' Lifted Relational Neural Networks ; We propose a method combining relational-logic representations with neural network learning.\\n',\n",
       " ' Giraffe: Using Deep Reinforcement Learning to Play Chess ; This report presents Giraffe, a chess engine that uses self-play to d\\n',\n",
       " ' Attention with Intention for a Neural Network Conversation Model ; In a conversation or a dialogue process, attention and intent\\n',\n",
       " ' Deep Reinforcement Learning in Parameterized Action Space ; Recent work has shown that deep neural networks are capable of appro\\n',\n",
       " ' MazeBase: A Sandbox for Learning from Games ; This paper introduces MazeBase: an environment for simple 2D games, designed as a \\n',\n",
       " ' On Learning to Think: Algorithmic Information Theory for Novel   Combinations of Reinforcement Learning Controllers and Recurren\\n',\n",
       " ' An Empirical Comparison of Neural Architectures for Reinforcement   Learning in Partially Observable Environments ; This paper e\\n',\n",
       " ' Predicting Clinical Events by Combining Static and Dynamic Information   Using Recurrent Neural Networks ; In clinical data sets\\n',\n",
       " ' Weight Normalization: A Simple Reparameterization to Accelerate Training   of Deep Neural Networks ; We present weight normaliza\\n',\n",
       " ' Bounded Rational Decision-Making in Feedforward Neural Networks ; Bounded rational decision-makers transform sensory input into \\n',\n",
       " ' Lie Access Neural Turing Machine ; Following the recent trend in explicit neural memory structures, we present a new design of a\\n',\n",
       " ' Towards Machine Intelligence ; There exists a theory of a single general-purpose learning algorithm which could explain the prin\\n',\n",
       " ' Dynamic Frame skip Deep Q Network ; Deep Reinforcement Learning methods have achieved state of the art performance in learning c\\n',\n",
       " ' Programming with a Differentiable Forth Interpreter ; Given that in practice training data is scarce for all but a small set of \\n',\n",
       " ' Generative Choreography using Deep Learning ; Recent advances in deep learning have enabled the extraction of high-level feature\\n',\n",
       " ' Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and   Knowledge ; We propose Logic Tensor Networks: a unifo\\n',\n",
       " ' Identifying and Harnessing the Building Blocks of Machine Learning   Pipelines for Sensible Initialization of a Data Science Aut\\n',\n",
       " ' Neuroevolution-Based Inverse Reinforcement Learning ; The problem of Learning from Demonstration is targeted at learning to perf\\n',\n",
       " ' TerpreT: A Probabilistic Programming Language for Program Induction ; We study machine learning formulations of inductive progra\\n',\n",
       " ' Multi-Label Classification Method Based on Extreme Learning Machines ; In this paper, an Extreme Learning Machine (ELM) based te\\n',\n",
       " ' A Novel Online Real-time Classifier for Multi-label Data Streams ; In this paper, a novel extreme learning machine based online \\n',\n",
       " ' A Novel Progressive Learning Technique for Multi-class Classification ; In this paper, a progressive learning technique for mult\\n',\n",
       " ' A novel online multi-label classifier for high-speed streaming data   applications ; In this paper, a high-speed online neural n\\n',\n",
       " ' Ternary Neural Networks for Resource-Efficient AI Applications ; The computation and storage requirements for Deep Neural Networ\\n',\n",
       " ' Fitted Learning: Models with Awareness of their Limits ; Though deep learning has pushed the boundaries of classification forwar\\n',\n",
       " ' Learning to learn with backpropagation of Hebbian plasticity ; Hebbian plasticity is a powerful principle that allows biological\\n',\n",
       " ' Learning by Stimulation Avoidance: A Principle to Control Spiking Neural   Networks Dynamics ; Learning based on networks of rea\\n',\n",
       " ' Surprisal-Driven Zoneout ; We propose a novel method of regularization for recurrent neural networks called suprisal-driven zone\\n',\n",
       " ' Neural Architecture Search with Reinforcement Learning ; Neural networks are powerful and flexible models that work well for man\\n',\n",
       " ' Emergence of foveal image sampling from learning to attend in visual   scenes ; We describe a neural attention model with a lear\\n',\n",
       " ' Long Timescale Credit Assignment in NeuralNetworks with External Memory ; Credit assignment in traditional recurrent neural netw\\n',\n",
       " ' Energy Saving Additive Neural Network ; In recent years, machine learning techniques based on neural networks for mobile computi\\n',\n",
       " ' Learning to Repeat: Fine Grained Action Repetition for Deep   Reinforcement Learning ; Reinforcement Learning algorithms can lea\\n',\n",
       " ' Survey of reasoning using Neural networks ; Reason and inference require process as well as memory skills by humans. Neural netw\\n',\n",
       " ' One-Shot Imitation Learning ; Imitation learning has been commonly applied to solve different tasks in isolation. This usually r\\n',\n",
       " ' Deep Learning for Explicitly Modeling Optimization Landscapes ; In all but the most trivial optimization problems, the structure\\n',\n",
       " ' Stochastic Neural Networks for Hierarchical Reinforcement Learning ; Deep reinforcement learning has achieved many impressive re\\n',\n",
       " ' Batch Reinforcement Learning on the Industrial Benchmark: First   Experiences ; The Particle Swarm Optimization Policy (PSO-P) h\\n',\n",
       " ' End-to-End Differentiable Proving ; We introduce neural networks for end-to-end differentiable proving of queries to knowledge b\\n',\n",
       " ' Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments ; We explore deep reinforcement learning methods for mul\\n',\n",
       " ' Getting deep recommenders fit: Bloom embeddings for sparse binary   input/output networks ; Recommendation algorithms that incor\\n',\n",
       " ' Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural   Network and Long-Term Evaluation ; Monte Carlo tree se\\n',\n",
       " ' Hindsight Experience Replay ; Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We pr\\n',\n",
       " ' Trial without Error: Towards Safe Reinforcement Learning via Human   Intervention ; AI systems are increasingly applied to compl\\n',\n",
       " ' Reverse Curriculum Generation for Reinforcement Learning ; Many relevant tasks require an agent to reach a certain state, or to \\n',\n",
       " ' Ideological Sublations: Resolution of Dialectic in Population-based   Optimization ; A population-based optimization algorithm w\\n',\n",
       " ' ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural   Projections ; Deep neural networks have become ubiquito\\n',\n",
       " ' A Flow Model of Neural Networks ; Based on a natural connection between ResNet and transport equation or its characteristic equa\\n',\n",
       " ' Multimodal Content Analysis for Effective Advertisements on YouTube ; The rapid advances in e-commerce and Web 2.0 technologies \\n',\n",
       " ' Overcoming Exploration in Reinforcement Learning with Demonstrations ; Exploration in environments with sparse rewards has been \\n',\n",
       " ' Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency   for Sequence Modeling ; Recurrent neural networks hav\\n',\n",
       " ' Scalable Recollections for Continual Lifelong Learning ; Given the recent success of Deep Learning applied to a variety of singl\\n',\n",
       " ' Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data ; The paper introduces the Hidden Tree Markov Network (H\\n',\n",
       " ' Hierarchical Actor-Critic ; The ability to learn at different resolutions in time may help overcome one of the main challenges i\\n',\n",
       " ' Proximodistal Exploration in Motor Learning as an Emergent Property of   Optimization ; To harness the complexity of their high-\\n',\n",
       " ' Null Dynamical State Models of Human Cognitive Dysfunction ; The hard problem in artificial intelligence asks how the shuffling \\n',\n",
       " \" Accelerating Deep Learning with Memcomputing ; Restricted Boltzmann machines (RBMs) and their extensions, called 'deep-belief ne\\n\",\n",
       " ' mvn2vec: Preservation and Collaboration in Multi-View Network Embedding ; Multi-view networks are ubiquitous in real-world appli\\n',\n",
       " ' Granger-causal Attentive Mixtures of Experts ; Several methods have recently been proposed to detect salient input features for \\n',\n",
       " ' Memorize or generalize? Searching for a compositional RNN in a haystack ; Neural networks are very powerful learning systems, bu\\n',\n",
       " ' Continual Reinforcement Learning with Complex Synapses ; Unlike humans, who are capable of continual learning over their lifetim\\n',\n",
       " ' Meta-Reinforcement Learning of Structured Exploration Strategies ; Exploration is a fundamental challenge in reinforcement learn\\n',\n",
       " ' Approximation Algorithms for Cascading Prediction Models ; We present an approximation algorithm that takes a pool of pre-traine\\n',\n",
       " ' Coloring black boxes: visualization of neural network decisions ; Neural networks are commonly regarded as black boxes performin\\n',\n",
       " ' Relational Neural Expectation Maximization: Unsupervised Discovery of   Objects and their Interactions ; Common-sense physical r\\n',\n",
       " ' A Bayesian Model for Activities Recommendation and Event Structure   Optimization Using Visitors Tracking ; In events that are c\\n',\n",
       " ' The Lottery Ticket Hypothesis: Training Pruned Neural Networks ; Recent work on neural network pruning indicates that, at traini\\n',\n",
       " ' Learning recurrent dynamics in spiking networks ; Spiking activity of neurons engaged in learning and performing a task show com\\n',\n",
       " ' Principal Graphs and Manifolds ; In many physical, statistical, biological and other investigations it is desirable to approxima\\n',\n",
       " ' Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint ; Deep Belief Networks (DBN) have been successfully appl\\n',\n",
       " ' Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary   Independent Stochastic Neurons ; In this paper, a simpl\\n',\n",
       " ' Locally Imposing Function for Generalized Constraint Neural Networks - A   Study on Equality Constraints ; This work is a furthe\\n',\n",
       " ' Evolution of Covariance Functions for Gaussian Process Regression using   Genetic Programming ; In this contribution we describe\\n',\n",
       " ' Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image   Statistics ; We present a theoretical analysis of Gaus\\n',\n",
       " ' Training Restricted Boltzmann Machine by Perturbation ; A new approach to maximum likelihood learning of discrete graphical mode\\n',\n",
       " ' Multilayer bootstrap networks ; Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from botto\\n',\n",
       " ' Invariant backpropagation: how to train a transformation-invariant   neural network ; In many classification problems a classifi\\n',\n",
       " ' Shared latent subspace modelling within Gaussian-Binary Restricted   Boltzmann Machines for NIST i-Vector Challenge 2014 ; This \\n',\n",
       " ' A Neural Transfer Function for a Smooth and Differentiable Transition   Between Additive and Multiplicative Interactions ; Exist\\n',\n",
       " ' A Probabilistic Framework for Deep Learning ; We develop a probabilistic framework for deep learning based on the Deep Rendering\\n',\n",
       " ' Neurogenesis Deep Learning ; Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable succe\\n',\n",
       " ' Deep learning for neuroimaging: a validation study ; Deep learning methods have recently made notable advances in the tasks of c\\n',\n",
       " ' Improving Deep Neural Networks with Probabilistic Maxout Units ; We present a probabilistic variant of the recently introduced m\\n',\n",
       " ' How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need? ; In numerous applicative contexts, data are too rich and\\n',\n",
       " ' Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures ; Model-based methods and deep neural networks have both bee\\n',\n",
       " ' Learning deep dynamical models from image pixels ; Modeling dynamical systems is important in many disciplines, e.g., control, r\\n',\n",
       " ' From neural PCA to deep unsupervised learning ; A network supporting deep unsupervised learning is presented. The network is an \\n',\n",
       " ' Qualitatively characterizing neural network optimization problems ; Training neural networks involves solving large-scale non-co\\n',\n",
       " ' Why does Deep Learning work? - A perspective from Group Theory ; Why does Deep Learning work? What representations does it captu\\n',\n",
       " ' ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient ; Stochastic gradient algorithms have been the main focus of la\\n',\n",
       " ' A Unified Perspective on Multi-Domain and Multi-Task Learning ; In this paper, we provide a new neural-network based perspective\\n',\n",
       " ' A Neural Network Anomaly Detector Using the Random Cluster Model ; The random cluster model is used to define an upper bound on \\n',\n",
       " ' A Group Theoretic Perspective on Unsupervised Deep Learning ; Why does Deep Learning work? What representations does it capture?\\n',\n",
       " ' A Generative Model for Deep Convolutional Learning ; A generative model is developed for deep (multi-layered) convolutional dict\\n',\n",
       " ' Knowledge Transfer Pre-training ; Pre-training is crucial for learning deep neural networks. Most of existing pre-training metho\\n',\n",
       " ' Stacked What-Where Auto-encoders ; We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integr\\n',\n",
       " ' Training recurrent networks online without backtracking ; We introduce the \"NoBackTrack\" algorithm to train the parameters of dy\\n',\n",
       " ' Deep clustering: Discriminative embeddings for segmentation and   separation ; We address the problem of acoustic source separat\\n',\n",
       " ' Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural   Networks ; Several popular graph embedding techniques f\\n',\n",
       " ' Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A   Systematic Study ; This paper presents Rudra, a parameter s\\n',\n",
       " ' Convolutional Networks on Graphs for Learning Molecular Fingerprints ; We introduce a convolutional neural network that operates\\n',\n",
       " ' Population-Contrastive-Divergence: Does Consistency help with RBM   training? ; Estimating the log-likelihood gradient with resp\\n',\n",
       " ' AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery ; Deep convolutional\\n',\n",
       " ' Distillation as a Defense to Adversarial Perturbations against Deep   Neural Networks ; Deep learning algorithms have been shown\\n',\n",
       " ' The Variational Gaussian Process ; Variational inference is a powerful tool for approximate inference, and it has been recently \\n',\n",
       " ' Partial Reinitialisation for Optimisers ; Heuristic optimisers which search for an optimal configuration of variables relative t\\n',\n",
       " ' Efficient Representation of Low-Dimensional Manifolds using Deep   Networks ; We consider the ability of deep neural networks to\\n',\n",
       " ' Enhanced perceptrons using contrastive biclusters ; Perceptrons are neuronal devices capable of fully discriminating linearly se\\n',\n",
       " ' Alternating optimization method based on nonnegative matrix   factorizations for deep neural networks ; The backpropagation algo\\n',\n",
       " ' Robust Large Margin Deep Neural Networks ; The generalization error of deep neural networks via their classification margin is s\\n',\n",
       " ' No bad local minima: Data independent training error guarantees for   multilayer neural networks ; We use smoothed analysis tech\\n',\n",
       " ' Learning Structured Sparsity in Deep Neural Networks ; High demand for computation resources severely hinders deployment of larg\\n',\n",
       " ' Depth-Width Tradeoffs in Approximating Natural Functions with Neural   Networks ; We provide several new depth-based separation \\n',\n",
       " ' Tensor Switching Networks ; We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes th\\n',\n",
       " ' Survey of Expressivity in Deep Neural Networks ; We survey results on neural network expressivity described in \"On the Expressiv\\n',\n",
       " ' Precise Recovery of Latent Vectors from Generative Adversarial Networks ; Generative adversarial networks (GANs) transform laten\\n',\n",
       " ' Predicting Surgery Duration with Neural Heteroscedastic Regression ; Scheduling surgeries is a challenging task due to the funda\\n',\n",
       " ' Depth Creates No Bad Local Minima ; In deep learning, \\\\textit{depth}, as well as \\\\textit{nonlinearity}, create non-convex loss s\\n',\n",
       " ' Deep Semi-Random Features for Nonlinear Function Approximation ; We propose semi-random features for nonlinear function approxim\\n',\n",
       " ' Curriculum Dropout ; Dropout is a very effective way of regularizing neural networks. Stochastically \"dropping out\" units with a\\n',\n",
       " ' The power of deeper networks for expressing natural functions ; It is well-known that neural networks are universal approximator\\n',\n",
       " ' Gradient Descent for Spiking Neural Networks ; Much of studies on neural computation are based on network models of static neuro\\n',\n",
       " ' Unsure When to Stop? Ask Your Semantic Neighbors ; In iterative supervised learning algorithms it is common to reach a point in \\n',\n",
       " ' Anomaly Detection on Graph Time Series ; In this paper, we use variational recurrent neural network to investigate the anomaly d\\n',\n",
       " ' A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and   Support Vector Machine (SVM) for Intrusion Detection in\\n',\n",
       " ' DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in   Neural Networks ; Deep neural networks have become wid\\n',\n",
       " ' A Method of Generating Random Weights and Biases in Feedforward Neural   Networks with Random Hidden Nodes ; Neural networks wit\\n',\n",
       " ' Rotational Unit of Memory ; The concepts of unitary evolution matrices and associative memory have boosted the field of Recurren\\n',\n",
       " ' Progressive Growing of GANs for Improved Quality, Stability, and   Variation ; We describe a new training methodology for genera\\n',\n",
       " ' Generative Adversarial Source Separation ; Generative source separation methods such as non-negative matrix factorization (NMF) \\n',\n",
       " ' A Supervised STDP-based Training Algorithm for Living Neural Networks ; Neural networks have shown great potential in many appli\\n',\n",
       " ' Improving Factor-Based Quantitative Investing by Forecasting Company   Fundamentals ; On a periodic basis, publicly traded compa\\n',\n",
       " ' Genetic Algorithms for Mentor-Assisted Evaluation Function Optimization ; In this paper we demonstrate how genetic algorithms ca\\n',\n",
       " ' Block Neural Network Avoids Catastrophic Forgetting When Learning   Multiple Task ; In the present work we propose a Deep Feed F\\n',\n",
       " ' A Scalable Deep Neural Network Architecture for Multi-Building and   Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinti\\n',\n",
       " ' Dynamic Boltzmann Machines for Second Order Moments and Generalized   Gaussian Distributions ; Dynamic Boltzmann Machine (DyBM) \\n',\n",
       " ' Multi-timescale memory dynamics in a reinforcement learning network with   attention-gated memory ; Learning and memory are inte\\n',\n",
       " ' Weighted Contrastive Divergence ; Learning algorithms for energy based Boltzmann architectures that rely on gradient descent are\\n',\n",
       " ' Dynamic Optimization of Neural Network Structures Using Probabilistic   Modeling ; Deep neural networks (DNNs) are powerful mach\\n',\n",
       " ' Pruning Techniques for Mixed Ensembles of Genetic Programming Models ; The objective of this paper is to define an effective str\\n',\n",
       " ' Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines ; This paper introduces the Metric-Free Natural Gradient (\\n',\n",
       " ' Stochastic Pooling for Regularization of Deep Convolutional Neural   Networks ; We introduce a simple and effective method for r\\n',\n",
       " ' Training Neural Networks with Stochastic Hessian-Free Optimization ; Hessian-free (HF) optimization has been successfully used f\\n',\n",
       " ' Reversible Jump MCMC Simulated Annealing for Neural Networks ; We propose a novel reversible jump Markov chain Monte Carlo (MCMC\\n',\n",
       " ' Predicting Parameters in Deep Learning ; We demonstrate that there is significant redundancy in the parameterization of several \\n',\n",
       " ' Disentangling Factors of Variation via Generative Entangling ; Here we propose a novel model family with the objective of learni\\n',\n",
       " ' Neural Networks for Complex Data ; Artificial neural networks are simple and efficient machine learning tools. Defined originall\\n',\n",
       " ' Multi-task Neural Networks for QSAR Predictions ; Although artificial neural networks have occasionally been used for Quantitati\\n',\n",
       " ' A Hybrid Latent Variable Neural Network Model for Item Recommendation ; Collaborative filtering is used to recommend items to a \\n',\n",
       " ' Techniques for Learning Binary Stochastic Feedforward Neural Networks ; Stochastic binary hidden units in a multi-layer perceptr\\n',\n",
       " ' Learning ELM network weights using linear discriminant analysis ; We present an alternative to the pseudo-inverse method for det\\n',\n",
       " ' Exponentially Increasing the Capacity-to-Computation Ratio for   Conditional Computation in Deep Learning ; Many state-of-the-ar\\n',\n",
       " ' Soft-Deep Boltzmann Machines ; We present a layered Boltzmann machine (BM) that can better exploit the advantages of a distribut\\n',\n",
       " ' Domain-Adversarial Training of Neural Networks ; We introduce a new representation learning approach for domain adaptation, in w\\n',\n",
       " ' Deep Online Convex Optimization with Gated Games ; Methods from convex optimization are widely used as building blocks for deep \\n',\n",
       " ' Churn analysis using deep convolutional neural networks and autoencoders ; Customer temporal behavioral data was represented as \\n',\n",
       " ' Developing an ICU scoring system with interaction terms using a genetic   algorithm ; ICU mortality scoring systems attempt to p\\n',\n",
       " ' Scale Normalization ; One of the difficulties of training deep neural networks is caused by improper scaling between layers. Sca\\n',\n",
       " ' Layer-wise learning of deep generative models ; When using deep, multi-layered architectures to build generative models of data,\\n',\n",
       " ' Distributed optimization of deeply nested systems ; In science and engineering, intelligent processing of complex signals such a\\n',\n",
       " ' Understanding Boltzmann Machine and Deep Learning via A Confident   Information First Principle ; Typical dimensionality reducti\\n',\n",
       " ' Canonical dual solutions to nonconvex radial basis neural network   optimization problem ; Radial Basis Functions Neural Network\\n',\n",
       " ' On the Number of Linear Regions of Deep Neural Networks ; We study the complexity of functions computable by deep feedforward ne\\n',\n",
       " ' Geometry and Expressive Power of Conditional Restricted Boltzmann   Machines ; Conditional restricted Boltzmann machines are und\\n',\n",
       " ' Is Joint Training Better for Deep Auto-Encoders? ; Traditionally, when generative models of data are developed via deep architec\\n',\n",
       " ' Massively Multitask Networks for Drug Discovery ; Massively multitask neural architectures provide a learning framework for drug\\n',\n",
       " ' Gated Feedback Recurrent Neural Networks ; In this work, we propose a novel recurrent neural network (RNN) architecture. The pro\\n',\n",
       " ' Deep Learning with Limited Numerical Precision ; Training of large-scale deep neural networks is often constrained by the availa\\n',\n",
       " ' MADE: Masked Autoencoder for Distribution Estimation ; There has been a lot of recent interest in designing neural network model\\n',\n",
       " ' Simple, Efficient, and Neural Algorithms for Sparse Coding ; Sparse coding is a basic task in many fields including signal proce\\n',\n",
       " ' Toxicity Prediction using Deep Learning ; Everyday we are exposed to various chemicals via food additives, cleaning and cosmetic\\n',\n",
       " ' To Drop or Not to Drop: Robustness, Consistency and Differential Privacy   Properties of Dropout ; Training deep belief networks\\n',\n",
       " ' Distilling the Knowledge in a Neural Network ; A very simple way to improve the performance of almost any machine learning algor\\n',\n",
       " ' A mathematical motivation for complex-valued convolutional networks ; A complex-valued convolutional network (convnet) implement\\n',\n",
       " ' Optimizing Neural Networks with Kronecker-factored Approximate Curvature ; We propose an efficient method for approximating natu\\n',\n",
       " ' Unsupervised model compression for multilayer bootstrap networks ; Recently, multilayer bootstrap network (MBN) has demonstrated\\n',\n",
       " ' Positive blood culture detection in time series data using a BiLSTM   network ; The presence of bacteria or fungi in the bloodst\\n',\n",
       " ' Known Unknowns: Uncertainty Quality in Bayesian Neural Networks ; We evaluate the uncertainty quality in neural networks using a\\n',\n",
       " ' Semi-Supervised Learning with the Deep Rendering Mixture Model ; Semi-supervised learning algorithms reduce the high cost of acq\\n',\n",
       " ' Self-calibrating Neural Networks for Dimensionality Reduction ; Recently, a novel family of biologically plausible online algori\\n',\n",
       " ' Tunable Efficient Unitary Neural Networks (EUNN) and their application   to RNNs ; Using unitary (instead of general) matrices i\\n',\n",
       " ' Sequence Transduction with Recurrent Neural Networks ; Many machine learning tasks can be expressed as the transformation---or \\\\\\n',\n",
       " ' On Fast Dropout and its Applicability to Recurrent Networks ; Recurrent Neural Networks (RNNs) are rich models for the processin\\n',\n",
       " ' Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks ; In this paper we propose and investigate a novel nonli\\n',\n",
       " ' Missing Value Imputation With Unsupervised Backpropagation ; Many data mining and data analysis techniques operate on dense matr\\n',\n",
       " ' Stochastic Gradient Estimate Variance in Contrastive Divergence and   Persistent Contrastive Divergence ; Contrastive Divergence\\n',\n",
       " ' How to Construct Deep Recurrent Neural Networks ; In this paper, we explore different ways to extend a recurrent neural network \\n',\n",
       " ' Neuronal Synchrony in Complex-Valued Deep Networks ; Deep learning has recently led to great successes in tasks such as image re\\n',\n",
       " ' An empirical analysis of dropout in piecewise linear networks ; The recently introduced dropout training criterion for neural ne\\n',\n",
       " ' An Empirical Investigation of Catastrophic Forgetting in Gradient-Based   Neural Networks ; Catastrophic forgetting is a problem\\n',\n",
       " ' Unsupervised Domain Adaptation by Backpropagation ; Top-performing deep architectures are trained on massive amounts of labeled \\n',\n",
       " ' Deep Directed Generative Autoencoders ; For discrete data, the likelihood $P(x)$ can be rewritten exactly and parametrized into \\n',\n",
       " ' An exact mapping between the Variational Renormalization Group and Deep   Learning ; Deep learning is a broad set of techniques \\n',\n",
       " ' Non-parametric Bayesian Learning with Deep Learning Structure and Its   Applications in Wireless Networks ; In this paper, we pr\\n',\n",
       " ' Parallel training of DNNs with Natural Gradient and Parameter Averaging ; We describe the neural-network training framework used\\n',\n",
       " ' End-to-end Continuous Speech Recognition using Attention-based Recurrent   NN: First Results ; We replace the Hidden Markov Mode\\n',\n",
       " ' Provable Methods for Training Neural Networks with Sparse Connectivity ; We provide novel guaranteed approaches for training fee\\n',\n",
       " ' Domain-Adversarial Neural Networks ; We introduce a new representation learning algorithm suited to the context of domain adapta\\n',\n",
       " ' Learning with Pseudo-Ensembles ; We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models \\n',\n",
       " ' Random Walk Initialization for Training Very Deep Feedforward Networks ; Training very deep networks is an important open proble\\n',\n",
       " ' Variational Recurrent Auto-Encoders ; In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variati\\n',\n",
       " ' Neural Network Regularization via Robust Weight Factorization ; Regularization is essential when training large neural networks.\\n',\n",
       " ' A Bayesian encourages dropout ; Dropout is one of the key techniques to prevent the learning from overfitting. It is explained t\\n',\n",
       " ' Deep Fried Convnets ; The fully connected layers of a deep convolutional neural network typically contain over 90% of the networ\\n',\n",
       " ' Lateral Connections in Denoising Autoencoders Support Supervised   Learning ; We show how a deep denoising autoencoder with late\\n',\n",
       " ' Deep Neural Networks with Random Gaussian Weights: A Universal   Classification Strategy? ; Three important properties of a clas\\n',\n",
       " ' Imaging Time-Series to Improve Classification and Imputation ; Inspired by recent successes of deep learning in computer vision,\\n',\n",
       " ' Blocks and Fuel: Frameworks for deep learning ; We introduce two Python frameworks to train neural networks on large datasets: B\\n',\n",
       " ' Adaptive Normalized Risk-Averting Training For Deep Neural Networks ; This paper proposes a set of new error criteria and learni\\n',\n",
       " ' Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer   Free Energy ; Restricted Boltzmann machines are undire\\n',\n",
       " ' Pointer Networks ; We introduce a new neural architecture to learn the conditional probability of an output sequence with elemen\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3df85ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_size = 16\n",
    "emb = Embedding(len(token_to_id), emb_size)\n",
    "emb.set_optimizer(ADAM(learning_rate=0.0001))\n",
    "\n",
    "lstm_unit = LSTM(\n",
    "    n_input=emb_size,\n",
    "    n_hidden=64,\n",
    "    n_output=len(token_to_id),\n",
    "    bptt_trunc=15\n",
    ")\n",
    "lstm_unit.set_optimizer(ADAM(learning_rate=0.0001))\n",
    "\n",
    "batch_size = 64\n",
    "history = []\n",
    "for i in range(1000):\n",
    "    batch_ix = to_matrix(np.random.choice(lines, size=batch_size, replace=False), token_to_id, max_len=MAX_LENGTH)\n",
    "    \n",
    "    encoded_data = emb(batch_ix)\n",
    "    \n",
    "    pred = lstm_unit(encoded_data)\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(batch_ix.shape[1]-1):\n",
    "        loss += cross_entropy_loss(batch_ix[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "        \n",
    "    errors = np.zeros(shape=(batch_size, MAX_LENGTH-1, len(token_to_id)))\n",
    "    for t in range(errors.shape[1]-1):\n",
    "        errors[:, t, :] = cross_entropy_loss_derivative(batch_ix[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "    \n",
    "    err = lstm_unit.backward(errors)\n",
    "    emb.backward(err)\n",
    "    \n",
    "    # visualizing training process\n",
    "    history.append(loss / batch_size)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aab9b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.94967026594122"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c9e164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    phrase = copy.copy(seed_phrase)\n",
    "    \n",
    "    for t in range(len(seed_phrase)-1, max_length-len(seed_phrase)):\n",
    "        x_sequence = to_matrix([phrase], token_to_id, max_len=max_length)\n",
    "        encoded_data = emb(x_sequence)\n",
    "\n",
    "        pred = char_rnn(encoded_data)\n",
    "        probs = softmax(pred[:, t] / temperature).ravel()\n",
    "        next_ix = np.random.choice(len(tokens), p=probs)\n",
    "        phrase += tokens[next_ix]\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a17cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " On A intione ; Dearnion ; The teprof tion an ; Antion in in ; Lece on ert a A A pro trol Re wor on iopror Ses ; The Pransing semt\n",
      " Stion of Tineror Lamwor anent ;  al Recing  Ere tore The Eve ering ; in the ose The of on ; Mope ; A int sure wert tor Prork in o\n",
      " A Rusent oning Alising an ale on arion ; Qontal sertion of Teruss for Pror Are are tesure sining ; Mear ion ; Corsing ; cons or P\n",
      " Delin   Lovic bisitionion of Sesianing ; Aatif End intion Lide Prol seralliving ; ders tor ; Moprol sering Dearsical Aution ; in \n",
      " A tor Artion for Leltion of Afal corpror  pers ; pre are Nody bericis Core pere ; ; The of Fecar in in   Pres ; Mualtion or Sine \n",
      " Meral A aling Seer A Bo ere fove wor   Momere tre In ; Neere tectes ; caped and on ; on on or Talting ; Araling of for   Ness ; A\n",
      " Rearne misice ; Meuwes in lise pre o pro mor Contion tre ; Aert con sertion of Ses sice Aution aning Ler pre   Ses Reming the ; A\n",
      " Ferien Cers ( fetwof des ; A pre In an are Mase Aluer of Deersing on for Cor ors ; Disetion on for ares tit an ; dering Deg ; Rea\n",
      " Sel sumrition of Appror ; Wece ; Deprorising ond ; fopers heche for pre  is on ar Dears nere for The on on serice ; Appror ; tent\n",
      " A Suce of of Seine on in al Sable for a In of Che Dinem sert ve a ing Mere for Contion ; Tor ; Fertion ; The wor de lepere Are ; \n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(lstm_unit, seed_phrase=' ', temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b6bf58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([121, 130, 130, 131,  68,  27,  23, 113, 132,  23,  23, 131,  68,\n",
       "       132,  77,  62, 130, 131, 130, 113,  77, 131,  77, 130,  77,  77,\n",
       "        23,  77, 132,  77,  62,  68, 131,  68,  12,  77,  62, 131, 130,\n",
       "       130, 130, 113, 113,  68,  77,  23, 130,  77, 132,  23, 113,  68,\n",
       "        77,  27,  77,  62, 131,  42, 130, 131,  27,  77,  62, 131,  42,\n",
       "        23, 113, 130,  77,  77,  77,  62,  77,  95, 131, 131, 130,  77,\n",
       "        77,  77,  27,  77,  62, 131, 130,  52,  77,  62,  68,  77,  77,\n",
       "       113,  68,  77,  77,  77,  77,  77,  23,  77, 132,  77,  62,  68,\n",
       "        77,  62,  68, 121, 113, 130,  90,  23, 130,  77, 113, 113, 131,\n",
       "        68,  12,  68,  23, 132,  77,  33, 130, 113, 132, 131,  23,  77],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_unit(encoded_data)[1].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efd3604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30, 131, 106,  23,  33, 132, 130,  23,  42,  77, 122, 113,  68,\n",
       "       131,  77, 121,  12, 131,  77, 121,  52,  52, 131,  52,  52,  32,\n",
       "       131,  68, 132,  77,  82,  52,  23,  68,  12,  77,  92, 131, 131,\n",
       "        90,  77, 103, 113,  68,  56, 113,  27,  28, 132,  23, 113,  68,\n",
       "        33,  27,  77,  98, 131,  28, 130,  33,  27,  77,  98, 131, 132,\n",
       "        35, 113, 130,   0,  52,  77,  62,  77,  85,   0, 131,  27, 131,\n",
       "       132,  33,  27,  77,  91, 113,  68, 131,  77,  33,  12, 131,  77,\n",
       "        33,  52,  52, 131,  52,  52,  32, 131,  68, 132,  77,  23,  52,\n",
       "        77,  33,  77,  42, 113,  32,  32, 113,  68,  77,  42,  27,  23,\n",
       "        68,  23,  42,  33,  27,  77,  90, 130,  33,  42, 132,  80])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ix[1, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e074c3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div #notebook {\n",
       "    background-color: #FFF9EE;\n",
       "    margin: auto;\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "    padding: 15px;\n",
       "    background-color: #FFFAFA;\n",
       "    min-height: 0;\n",
       "    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width to about 80 chars */\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "\n",
       "div.cell.border-box-sizing.code_cell.running { \n",
       "    border: 3px solid #111;\n",
       "}\n",
       "\n",
       "div.cell.code_cell {\n",
       "    background-color: #FFFAFA ;\n",
       "    border-radius: 5px;\n",
       "    padding: 1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Times New Roman';\n",
       "    color: #B8860B\n",
       "}\n",
       "\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 40pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 700;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 25pt;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: italic;\n",
       "    color: #8B4513;\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 20pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 16pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 10pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render p {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 15pt;\n",
       "    color: black;\n",
       "    text-align: justify;\n",
       "    text-justify: inter-word;\n",
       "    line-height: 1.5;\n",
       "}\n",
       "\n",
       "mark {\n",
       "  background: #D5EAFF;\n",
       "  color: black;\n",
       "}\n",
       "\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:2000px;  /* your desired max-height here */\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./style.css') as f:\n",
    "    style = f.read()\n",
    "HTML(style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_39",
   "language": "python",
   "name": "data_science_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
