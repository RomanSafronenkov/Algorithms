{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbda81a",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16265124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union\n",
    "from IPython.display import HTML, clear_output\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4455dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS, EOS = ' ', '\\n'\n",
    "\n",
    "data = pd.read_json(\"./data/arxivData.json\")\n",
    "lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:128], axis=1) \\\n",
    "            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n",
    "            .tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54e6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens =  136\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(''.join(lines)))\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print('num_tokens = ', num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b1e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 0,\n",
       " '2': 1,\n",
       " '%': 2,\n",
       " 'c': 3,\n",
       " 'x': 4,\n",
       " 'b': 5,\n",
       " ' ': 6,\n",
       " 'r': 7,\n",
       " 'ï': 8,\n",
       " '=': 9,\n",
       " '.': 10,\n",
       " 'f': 11,\n",
       " 'u': 12,\n",
       " '!': 13,\n",
       " '}': 14,\n",
       " 'N': 15,\n",
       " 'y': 16,\n",
       " 'ô': 17,\n",
       " 'Y': 18,\n",
       " 'Q': 19,\n",
       " 'l': 20,\n",
       " 'á': 21,\n",
       " 'õ': 22,\n",
       " 'τ': 23,\n",
       " 'i': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " 'H': 27,\n",
       " 'ś': 28,\n",
       " 'm': 29,\n",
       " 'I': 30,\n",
       " '8': 31,\n",
       " '-': 32,\n",
       " 'A': 33,\n",
       " '5': 34,\n",
       " 'L': 35,\n",
       " 'v': 36,\n",
       " 'a': 37,\n",
       " '$': 38,\n",
       " ')': 39,\n",
       " 'Ö': 40,\n",
       " 'K': 41,\n",
       " ',': 42,\n",
       " 'ê': 43,\n",
       " 'ü': 44,\n",
       " 'g': 45,\n",
       " 'o': 46,\n",
       " 'F': 47,\n",
       " '<': 48,\n",
       " 'S': 49,\n",
       " 'ã': 50,\n",
       " 'k': 51,\n",
       " 'α': 52,\n",
       " 'W': 53,\n",
       " '\\x7f': 54,\n",
       " '?': 55,\n",
       " 'í': 56,\n",
       " '@': 57,\n",
       " 'â': 58,\n",
       " 'γ': 59,\n",
       " 'e': 60,\n",
       " 'Z': 61,\n",
       " '{': 62,\n",
       " '#': 63,\n",
       " 'Π': 64,\n",
       " 'd': 65,\n",
       " 'D': 66,\n",
       " 'Ω': 67,\n",
       " 'ρ': 68,\n",
       " 'z': 69,\n",
       " '1': 70,\n",
       " 'ä': 71,\n",
       " '&': 72,\n",
       " 'P': 73,\n",
       " 'R': 74,\n",
       " '+': 75,\n",
       " 'χ': 76,\n",
       " '\"': 77,\n",
       " 'à': 78,\n",
       " '(': 79,\n",
       " '|': 80,\n",
       " 'n': 81,\n",
       " '/': 82,\n",
       " '3': 83,\n",
       " 'h': 84,\n",
       " '7': 85,\n",
       " \"'\": 86,\n",
       " 'ö': 87,\n",
       " 'O': 88,\n",
       " 'ν': 89,\n",
       " 'ó': 90,\n",
       " 'ω': 91,\n",
       " 'V': 92,\n",
       " '4': 93,\n",
       " '*': 94,\n",
       " 'q': 95,\n",
       " 'j': 96,\n",
       " '\\n': 97,\n",
       " 'J': 98,\n",
       " 'λ': 99,\n",
       " 'æ': 100,\n",
       " 's': 101,\n",
       " 'B': 102,\n",
       " 'σ': 103,\n",
       " '\\\\': 104,\n",
       " 'É': 105,\n",
       " '9': 106,\n",
       " '°': 107,\n",
       " '[': 108,\n",
       " '>': 109,\n",
       " 'é': 110,\n",
       " '~': 111,\n",
       " 'X': 112,\n",
       " 'w': 113,\n",
       " '^': 114,\n",
       " '_': 115,\n",
       " 'è': 116,\n",
       " 't': 117,\n",
       " 'Σ': 118,\n",
       " 'p': 119,\n",
       " 'ő': 120,\n",
       " 'Ü': 121,\n",
       " 'U': 122,\n",
       " 'G': 123,\n",
       " 'M': 124,\n",
       " '0': 125,\n",
       " ']': 126,\n",
       " 'E': 127,\n",
       " 'C': 128,\n",
       " 'μ': 129,\n",
       " 'Ł': 130,\n",
       " 'β': 131,\n",
       " 'ε': 132,\n",
       " 'ç': 133,\n",
       " '`': 134,\n",
       " '6': 135}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185191cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems alright!\n"
     ]
    }
   ],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e75a93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first=True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, data))\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[c] for c in data[i]]\n",
    "        data_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        data_ix = np.transpose(data_ix)\n",
    "\n",
    "    return data_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537348a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dual Recurrent Attention Units for Visual Question Answering ; We propose an architecture for VQA which utilizes recurrent layer\n",
      "\n",
      " Exploring Deep and Recurrent Architectures for Optimal Control ; Sophisticated multilayer neural networks have achieved state of\n",
      "\n",
      " Space and camera path reconstruction for omni-directional vision ; In this paper, we address the inverse problem of reconstructi\n",
      "\n",
      " Provable Algorithms for Inference in Topic Models ; Recently, there has been considerable progress on designing algorithms with \n",
      "\n",
      " Fixing Weight Decay Regularization in Adam ; L$_2$ regularization and weight decay regularization are equivalent for standard st\n",
      "\n",
      " Query-Efficient Imitation Learning for End-to-End Autonomous Driving ; One way to approach end-to-end autonomous driving is to l\n",
      "\n",
      " Multiple Kernel Learning and Automatic Subspace Relevance Determination   for High-dimensional Neuroimaging Data ; Alzheimer's d\n",
      "\n",
      " A Search for Improved Performance in Regular Expressions ; The primary aim of automated performance improvement is to reduce the\n",
      "\n",
      " Solving General Arithmetic Word Problems ; This paper presents a novel approach to automatically solving arithmetic word problem\n",
      "\n",
      " BDD-based reasoning in the fluent calculus - first results ; The paper reports on first preliminary results and insights gained \n",
      "\n",
      " Balancing bike sharing systems (BBSS): instance generation from the   CitiBike NYC data ; Bike sharing systems are a very popula\n",
      "\n",
      " Variational Bayesian inference for linear and logistic regression ; The article describe the model, derivation, and implementati\n",
      "\n",
      " Fusion of Range and Thermal Images for Person Detection ; Detecting people in images is a challenging problem. Differences in po\n",
      "\n",
      " Spontaneous Facial Micro-Expression Recognition using Discriminative   Spatiotemporal Local Binary Pattern with an Improved Inte\n",
      "\n",
      " CAD Priors for Accurate and Flexible Instance Reconstruction ; We present an efficient and automatic approach for accurate recon\n",
      "\n",
      " An Automatic Diagnosis Method of Facial Acne Vulgaris Based on   Convolutional Neural Network ; In this paper, we present a new \n",
      "\n",
      " Bayesian Multitask Learning with Latent Hierarchies ; We learn multiple hypotheses for related tasks under a latent hierarchical\n",
      "\n",
      " PACE: Pattern Accurate Computationally Efficient Bootstrapping for   Timely Discovery of Cyber-Security Concepts ; Public disclo\n",
      "\n",
      " Quantum decision theory as quantum theory of measurement ; We present a general theory of quantum information processing devices\n",
      "\n",
      " Fast Robust Methods for Singular State-Space Models ; State-space models are used in a wide range of time series analysis formul\n",
      "\n",
      " Objectness Scoring and Detection Proposals in Forward-Looking Sonar   Images with Convolutional Neural Networks ; Forward-lookin\n",
      "\n",
      "[[  6  66  12 ...  60   7  97]\n",
      " [  6 127   4 ...  46  11  97]\n",
      " [  6  49 119 ... 117  24  97]\n",
      " ...\n",
      " [  6  19  12 ...  60 101  97]\n",
      " [  6  47  37 ...  12  20  97]\n",
      " [  6  88   5 ...  24  81  97]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(lines[::2000]))\n",
    "print(to_matrix(lines[::2000], token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfba1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, lines))\n",
    "sample = to_matrix(np.random.choice(lines, size=5), token_to_id, max_len=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c90cd4d-85b7-404b-8049-eee1fc3a20d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6fa5bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 130, 136)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy\n",
    "encoded_data = np.zeros(shape=(5, MAX_LENGTH, len(token_to_id)))\n",
    "\n",
    "for text_i, text in enumerate(encoded_data):\n",
    "    for letter_i, letter in enumerate(text):\n",
    "        encoded_data[text_i, letter_i, sample[text_i, letter_i]] = 1\n",
    "\n",
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d61fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        return self.forward(x, grad)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "\n",
    "class LinearLSTM(BaseLayer):\n",
    "    \"\"\"\n",
    "    Linear class permorms ordinary FC layer in neural networks\n",
    "    Parameters:\n",
    "    n_input - size of input neurons\n",
    "    n_output - size of output neurons\n",
    "    Methods:\n",
    "    set_optimizer(optimizer) - is used for setting an optimizer for gradient descent\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input: int, n_output: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.w = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(n_input, n_output))\n",
    "        self.b = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(1, n_output))\n",
    "\n",
    "        self.w_optimizer = None\n",
    "        self.b_optimizer = None\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.w_optimizer = copy.copy(optimizer)\n",
    "        self.b_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.w_optimizer.set_weight(self.w)\n",
    "        self.b_optimizer.set_weight(self.b)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return x.dot(self.w) + self.b\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        assert self.w_optimizer is not None and self.b_optimizer is not None, 'You should set an optimizer'\n",
    "        w_grad = self.input.T.dot(output_error)\n",
    "        b_grad = np.ones((1, len(output_error))).dot(output_error)\n",
    "        input_error = output_error.dot(self.w.T)\n",
    "\n",
    "        return w_grad, b_grad, input_error\n",
    "\n",
    "    def step(self, w_grad, b_grad):\n",
    "        self.w = self.w_optimizer.step(w_grad)\n",
    "        self.b = self.b_optimizer.step(b_grad)\n",
    "\n",
    "\n",
    "class Activation(BaseLayer):\n",
    "    \"\"\"\n",
    "    Activation class is used for activation function of the FC layer\n",
    "    Params:\n",
    "    activation_function - activation function (e.g. sigmoid, RElU, tanh)\n",
    "    activation_derivative - derivative of the activation function\n",
    "    Methods:\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_function: callable, activation_derivative: callable) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.activation = activation_function\n",
    "        self.derivative = activation_derivative\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return self.activation(x)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return output_error * self.derivative(self.input)\n",
    "\n",
    "class BaseOptimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "class ADAM(BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm.\n",
    "\n",
    "    learning_rate (float, optional) – learning rate (default: 1e-3)\n",
    "    beta1, beta2 (Tuple[float, float], optional) –\n",
    "    coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "    eps (float, optional) – term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8,\n",
    "                 learning_rate: float = 3e-4, weight_decay: float = 0) -> None:\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.EMA1 = None\n",
    "        self.EMA2 = None\n",
    "\n",
    "        self.weight = None\n",
    "\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        self.weight = weight.copy()\n",
    "        self.EMA1 = np.zeros(shape=self.weight.shape)\n",
    "        self.EMA2 = np.zeros(shape=self.weight.shape)\n",
    "\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        assert self.weight is not None, 'You should set the weight'\n",
    "        grad = grad.copy() + self.weight_decay * self.weight\n",
    "        self.EMA1 = (1 - self.beta1) * grad + self.beta1 * self.EMA1\n",
    "        self.EMA2 = (1 - self.beta2) * grad ** 2 + self.beta2 * self.EMA2\n",
    "        self.weight -= self.learning_rate * self.EMA1 / (np.sqrt(self.EMA2) + self.eps)\n",
    "\n",
    "        return self.weight.copy()\n",
    "\n",
    "class Embedding(BaseLayer):\n",
    "    def __init__(self, n_input, emb_dim, pad_idx=None):\n",
    "        self.n_input = n_input\n",
    "        self.emb_dim = emb_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.weights = np.random.normal(scale=np.sqrt(2/(n_input+emb_dim)), size=(n_input, emb_dim))\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.weights_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.weights_optimizer.set_weight(self.weights)\n",
    "\n",
    "    def forward(self, x, grad=True):\n",
    "        self.input = x\n",
    "        return self.weights[x]\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        weights_grad = np.zeros_like(self.weights)\n",
    "        input_shape_len = len(self.input.shape)\n",
    "\n",
    "        if input_shape_len == 2:\n",
    "            for batch_n, s in enumerate(self.input):\n",
    "                for i, emb_i in enumerate(s):\n",
    "                    weights_grad[emb_i] += output_error[batch_n][i]\n",
    "\n",
    "        elif input_shape_len == 1:\n",
    "            for i, emb_i in enumerate(self.input):\n",
    "                weights_grad[emb_i] += output_error[i]\n",
    "\n",
    "        if self.pad_idx is not None:\n",
    "            weights_grad[self.pad_idx] = 0\n",
    "\n",
    "        self.weights = self.weights_optimizer.step(weights_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6375dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Tanh function\n",
    "    \"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Tanh function derivative\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "def sigmoid(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Sigmoid function derivative\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(z: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \"\"\"\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1, 1)\n",
    "\n",
    "def cross_entropy_loss(y_true: np.array, a_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    arg = -a_pred[lenght_y, y_true.ravel()]\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1)\n",
    "    loss = np.sum(arg + np.log(sum_exp))\n",
    "    return loss / len(y_true)\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true: np.array, a_pred: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss derivative for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: np.array with shape [n_samples, n_classes] with CrossEntropyLoss derivatives for each weight\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1).reshape(-1, 1)\n",
    "    loss = np.exp(a_pred.copy()) / sum_exp\n",
    "    loss[lenght_y, y_true.ravel()] -= 1\n",
    "\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04065a",
   "metadata": {},
   "source": [
    "![](./images/lstm.png)\n",
    "![](./images/graph_meanings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "627b9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input,\n",
    "        n_hidden,\n",
    "        n_output,\n",
    "        bptt_trunc=4\n",
    "    ):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.bptt_trunc = bptt_trunc\n",
    "        \n",
    "        self.forget_gate = LinearLSTM(n_input + n_hidden, n_hidden)\n",
    "        self.forget_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        \n",
    "        self.input_layer_gate = LinearLSTM(n_input + n_hidden, n_hidden)\n",
    "        self.input_layer_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        \n",
    "        self.candidate_gate = LinearLSTM(n_input + n_hidden, n_hidden)\n",
    "        self.candidate_gate_activation = Activation(tanh, tanh_derivative)\n",
    "        \n",
    "        self.output_gate = LinearLSTM(n_input + n_hidden, n_hidden)\n",
    "        self.output_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        \n",
    "        self.output_to_logits = LinearLSTM(n_hidden, n_output)\n",
    "        \n",
    "        self.x_and_h = None\n",
    "        self.hidden_states = None\n",
    "        self.cell_states = None\n",
    "        \n",
    "        self.forget_inputs = None\n",
    "        self.forget_states = None\n",
    "        \n",
    "        self.input_inputs = None\n",
    "        self.input_states = None\n",
    "        \n",
    "        self.candidate_inputs = None\n",
    "        self.candidate_states = None\n",
    "        \n",
    "        self.output_input = None\n",
    "        self.output_states = None\n",
    "        \n",
    "        self.outputs = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.forget_gate.set_optimizer(optimizer)\n",
    "        self.input_layer_gate.set_optimizer(optimizer)\n",
    "        self.candidate_gate.set_optimizer(optimizer)\n",
    "        self.output_gate.set_optimizer(optimizer)\n",
    "        self.output_to_logits.set_optimizer(optimizer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        batch_size, timesteps, input_dim = x.shape\n",
    "        \n",
    "        self.x_and_h = np.zeros((batch_size, timesteps, self.n_input+self.n_hidden))\n",
    "        self.hidden_states = np.zeros((batch_size, timesteps+1, self.n_hidden))\n",
    "        self.cell_states = np.zeros((batch_size, timesteps+1, self.n_hidden))\n",
    "        \n",
    "        self.forget_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.forget_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.input_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.input_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.candidate_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.candidate_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.output_input = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.output_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.outputs = np.zeros((batch_size, timesteps, self.n_output))\n",
    "\n",
    "        self.hidden_states[:, -1] = np.zeros((batch_size, self.n_hidden))\n",
    "        self.cell_states[:, -1] = np.zeros((batch_size, self.n_hidden))\n",
    "        for t in range(timesteps):\n",
    "            # соединяем вход и прошлый h\n",
    "            self.x_and_h[:, t] = np.concatenate((x[:, t], self.hidden_states[:, t-1]), axis=1)\n",
    "            \n",
    "            # forget gate проход\n",
    "            self.forget_inputs[:, t] = self.forget_gate(self.x_and_h[:, t])\n",
    "            self.forget_states[:, t] = self.forget_gate_activation(self.forget_inputs[:, t])\n",
    "            \n",
    "            # выбор кандидатов для C\n",
    "            self.input_inputs[:, t] = self.input_layer_gate(self.x_and_h[:, t])\n",
    "            self.input_states[:, t] = self.input_layer_gate_activation(self.input_inputs[:, t])\n",
    "            \n",
    "            self.candidate_inputs[:, t] = self.candidate_gate(self.x_and_h[:, t])\n",
    "            self.candidate_states[:, t] = self.candidate_gate_activation(self.candidate_inputs[:, t])\n",
    "            \n",
    "            self.cell_states[:, t] = self.forget_states[:, t] * self.cell_states[:, t-1]\\\n",
    "            + self.input_states[:, t] * self.candidate_states[:, t]\n",
    "            \n",
    "            self.output_input[:, t] = self.output_gate(self.x_and_h[:, t])\n",
    "            self.output_states[:, t] = self.output_gate_activation(self.output_input[:, t])\n",
    "            \n",
    "            self.hidden_states[:, t] = self.output_states[:, t] * tanh(self.cell_states[:, t])\n",
    "            \n",
    "            # дополнительный слой, не указан на рисунке, для перевода состояния в логиты выхода\n",
    "            self.outputs[:, t] = self.output_to_logits(self.hidden_states[:, t])\n",
    "\n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, output_error):        \n",
    "        _, timesteps, _ = output_error.shape\n",
    "\n",
    "        forgate_gate_w_grad = np.zeros_like(self.forget_gate.w)\n",
    "        forgate_gate_b_grad = np.zeros_like(self.forget_gate.b)\n",
    "\n",
    "        input_layer_gate_w_grad = np.zeros_like(self.input_layer_gate.w)\n",
    "        input_layer_gate_b_grad = np.zeros_like(self.input_layer_gate.b)\n",
    "\n",
    "        candidate_gate_w_grad = np.zeros_like(self.candidate_gate.w)\n",
    "        candidate_gate_b_grad = np.zeros_like(self.candidate_gate.b)\n",
    "\n",
    "        output_gate_w_grad = np.zeros_like(self.output_gate.w)\n",
    "        output_gate_b_grad = np.zeros_like(self.output_gate.b)\n",
    "\n",
    "        output_to_logits_w_grad = np.zeros_like(self.output_to_logits.w)\n",
    "        output_to_logits_b_grad = np.zeros_like(self.output_to_logits.b)\n",
    "        \n",
    "        input_error = np.zeros_like(self.input)\n",
    "        \n",
    "        for t in np.arange(timesteps)[::-1]:\n",
    "            # в разные моменты времени у слоев был разный вход, необходимо искусственно его поменять\n",
    "            self.forget_gate.input = self.x_and_h[:, t]\n",
    "            self.forget_gate_activation.input = self.forget_inputs[:, t]\n",
    "            self.input_layer_gate.input = self.x_and_h[:, t]\n",
    "            self.input_layer_gate_activation.input = self.input_inputs[:, t]\n",
    "            self.candidate_gate.input = self.x_and_h[:, t]\n",
    "            self.candidate_gate_activation.input = self.candidate_inputs[:, t]\n",
    "            self.output_gate.input = self.x_and_h[:, t]\n",
    "            self.output_gate_activation.input = self.output_input[:, t]\n",
    "            self.output_to_logits.input = self.hidden_states[:, t]\n",
    "            \n",
    "            # проход по нижнему уровню\n",
    "            w_grad, b_grad, hidden_error = self.output_to_logits.backward(output_error[:, t])\n",
    "            output_to_logits_w_grad += w_grad\n",
    "            output_to_logits_b_grad += b_grad\n",
    "            \n",
    "            # та, что идет вверх\n",
    "            cell_error = tanh_derivative(self.cell_states[:, t]) * self.output_states[:, t] * hidden_error\n",
    "            \n",
    "            # ошибка идет и вниз и вверх\n",
    "            hidden_error = self.output_gate_activation.backward(hidden_error) * tanh(self.cell_states[:, t])\n",
    "            \n",
    "            # та, что идет вниз\n",
    "            w_grad, b_grad, hidden_error = self.output_gate.backward(hidden_error)\n",
    "            output_gate_w_grad += w_grad\n",
    "            output_gate_b_grad += b_grad\n",
    "            \n",
    "            # идем по верху\n",
    "            hidden_candidate_error = self.candidate_gate_activation.backward(cell_error) * self.input_states[:, t]\n",
    "            w_grad, b_grad, hidden_candidate_error = self.candidate_gate.backward(hidden_candidate_error)\n",
    "            candidate_gate_w_grad += w_grad\n",
    "            candidate_gate_b_grad += b_grad\n",
    "            \n",
    "            hidden_inputs_error = self.input_layer_gate_activation.backward(cell_error) * self.candidate_states[:, t]\n",
    "            w_grad, b_grad, hidden_inputs_error = self.input_layer_gate.backward(hidden_inputs_error)\n",
    "            input_layer_gate_w_grad += w_grad\n",
    "            input_layer_gate_b_grad += b_grad\n",
    "            \n",
    "            hidden_forget_error = self.forget_gate_activation.backward(cell_error) * self.cell_states[:, t-1]\n",
    "            w_grad, b_grad, hidden_forget_error = self.forget_gate.backward(hidden_forget_error)\n",
    "            forgate_gate_w_grad += w_grad\n",
    "            forgate_gate_b_grad += b_grad\n",
    "            \n",
    "            # добавляются ошибки с мест копии\n",
    "            hidden_error += hidden_candidate_error\n",
    "            hidden_error += hidden_inputs_error\n",
    "            hidden_error += hidden_forget_error\n",
    "\n",
    "            # ошибка входа\n",
    "            input_error[:, t] = hidden_error[:, :self.n_input]\n",
    "            # ошибка, которая по времени уходит по низу назад\n",
    "            hidden_error = hidden_error[:, self.n_input:]\n",
    "            # ошибка, которая по времени уходит по верху назад\n",
    "            cell_error = cell_error * self.forget_states[:, t]\n",
    "            \n",
    "            for t_ in np.arange(max(0, t - self.bptt_trunc), t)[::-1]:\n",
    "                # проход по времени\n",
    "                self.forget_gate.input = self.x_and_h[:, t_]\n",
    "                self.forget_gate_activation.input = self.forget_inputs[:, t_]\n",
    "                self.input_layer_gate.input = self.x_and_h[:, t_]\n",
    "                self.input_layer_gate_activation.input = self.input_inputs[:, t_]\n",
    "                self.candidate_gate.input = self.x_and_h[:, t_]\n",
    "                self.candidate_gate_activation.input = self.candidate_inputs[:, t_]\n",
    "                self.output_gate.input = self.x_and_h[:, t_]\n",
    "                self.output_gate_activation.input = self.output_input[:, t_]\n",
    "                \n",
    "                # та, что идет по верху\n",
    "                cell_error += tanh_derivative(self.cell_states[:, t_]) * self.output_states[:, t_]\\\n",
    "                * hidden_error\n",
    "                \n",
    "                hidden_error = self.output_gate_activation.backward(hidden_error) * tanh(self.cell_states[:, t_])\n",
    "                # та, что идет вниз\n",
    "                w_grad, b_grad, hidden_error = self.output_gate.backward(hidden_error)\n",
    "                output_gate_w_grad += w_grad\n",
    "                output_gate_b_grad += b_grad\n",
    "                \n",
    "                hidden_candidate_error = self.candidate_gate_activation.backward(cell_error) * self.input_states[:, t_]\n",
    "                w_grad, b_grad, hidden_candidate_error = self.candidate_gate.backward(hidden_candidate_error)\n",
    "                candidate_gate_w_grad += w_grad\n",
    "                candidate_gate_b_grad += b_grad\n",
    "\n",
    "                hidden_inputs_error = self.input_layer_gate_activation.backward(cell_error) * self.candidate_states[:, t_]\n",
    "                w_grad, b_grad, hidden_inputs_error = self.input_layer_gate.backward(hidden_inputs_error)\n",
    "                input_layer_gate_w_grad += w_grad\n",
    "                input_layer_gate_b_grad += b_grad\n",
    "\n",
    "                hidden_forget_error = self.forget_gate_activation.backward(cell_error) * self.cell_states[:, t_-1]\n",
    "                w_grad, b_grad, hidden_forget_error = self.forget_gate.backward(hidden_forget_error)\n",
    "                forgate_gate_w_grad += w_grad\n",
    "                forgate_gate_b_grad += b_grad\n",
    "                \n",
    "                # добавляются ошибки с мест копии\n",
    "                hidden_error += hidden_candidate_error\n",
    "                hidden_error += hidden_inputs_error\n",
    "                hidden_error += hidden_forget_error\n",
    "\n",
    "                # ошибка входа\n",
    "                input_error[:, t_] += hidden_error[:, :self.n_input]\n",
    "                # ошибка которая по времени уходит по низу назад\n",
    "                hidden_error = hidden_error[:, self.n_input:]\n",
    "                cell_error = cell_error * self.forget_states[:, t_]\n",
    "\n",
    "        self.forget_gate.step(forgate_gate_w_grad, forgate_gate_b_grad)\n",
    "        self.input_layer_gate.step(input_layer_gate_w_grad, input_layer_gate_b_grad)\n",
    "        self.candidate_gate.step(candidate_gate_w_grad, candidate_gate_b_grad)\n",
    "        self.output_gate.step(output_gate_w_grad, output_gate_b_grad)\n",
    "        self.output_to_logits.step(output_to_logits_w_grad, output_to_logits_b_grad)\n",
    "\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b6d16",
   "metadata": {},
   "source": [
    "![](./images/lstm_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e39fce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 130, 16)\n"
     ]
    }
   ],
   "source": [
    "emb_size = 16\n",
    "emb = Embedding(len(token_to_id), emb_size)\n",
    "\n",
    "encoded_data = emb(sample)\n",
    "print(encoded_data.shape)\n",
    "\n",
    "lstm_unit = LSTM(n_input=emb_size,\n",
    "                 n_hidden=64,\n",
    "                 n_output=len(token_to_id),\n",
    "                 bptt_trunc=15)\n",
    "\n",
    "lstm_unit.set_optimizer(ADAM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d57e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 130, 136)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_unit.forward(encoded_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dafb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lstm_unit(encoded_data)\n",
    "\n",
    "loss = 0\n",
    "for t in range(sample.shape[1]-1):\n",
    "    loss += cross_entropy_loss(sample[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "\n",
    "errors = np.zeros(shape=(5, MAX_LENGTH-1, len(token_to_id)))\n",
    "for t in range(errors.shape[1]-1):\n",
    "    errors[:, t, :] = cross_entropy_loss_derivative(sample[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "\n",
    "err = lstm_unit.backward(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f8ab9b7-5e1d-4f80-b475-c61dd6c24145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Dual Recurrent Attention Units for Visual Question Answering ; We propose an architecture for VQA which utilizes recurrent layer\\n',\n",
       " ' Sequential Short-Text Classification with Recurrent and Convolutional   Neural Networks ; Recent approaches based on artificial \\n',\n",
       " ' Multiresolution Recurrent Neural Networks: An Application to Dialogue   Response Generation ; We introduce the multiresolution r\\n',\n",
       " ' Learning what to share between loosely related tasks ; Multi-task learning is motivated by the observation that humans bring to \\n',\n",
       " ' A Deep Reinforcement Learning Chatbot ; We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Inst\\n',\n",
       " ' Generating Sentences by Editing Prototypes ; We propose a new generative model of sentences that first samples a prototype sente\\n',\n",
       " ' A Deep Reinforcement Learning Chatbot (Short Version) ; We present MILABOT: a deep reinforcement learning chatbot developed by t\\n',\n",
       " ' Document Image Coding and Clustering for Script Discrimination ; The paper introduces a new method for discrimination of documen\\n',\n",
       " ' Tutorial on Answering Questions about Images with Deep Learning ; Together with the development of more accurate methods in Comp\\n',\n",
       " ' pix2code: Generating Code from a Graphical User Interface Screenshot ; Transforming a graphical user interface screenshot create\\n',\n",
       " ' A Unified Deep Neural Network for Speaker and Language Recognition ; Learned feature representations and sub-phoneme posteriors \\n',\n",
       " ' Efficient Neural Architecture Search via Parameter Sharing ; We propose Efficient Neural Architecture Search (ENAS), a fast and \\n',\n",
       " ' Building Machines That Learn and Think Like People ; Recent progress in artificial intelligence (AI) has renewed interest in bui\\n',\n",
       " ' Towards Bayesian Deep Learning: A Survey ; While perception tasks such as visual object recognition and text understanding play \\n',\n",
       " ' Hierarchical Deep Reinforcement Learning: Integrating Temporal   Abstraction and Intrinsic Motivation ; Learning goal-directed b\\n',\n",
       " ' Learning Features by Watching Objects Move ; This paper presents a novel yet intuitive approach to unsupervised feature learning\\n',\n",
       " ' Domain Adaptive Neural Networks for Object Recognition ; We propose a simple neural network model to deal with the domain adapta\\n',\n",
       " ' Beyond Temporal Pooling: Recurrence and Temporal Convolutions for   Gesture Recognition in Video ; Recent studies have demonstra\\n',\n",
       " ' Telugu OCR Framework using Deep Learning ; In this paper, we address the task of Optical Character Recognition(OCR) for the Telu\\n',\n",
       " ' Adversarial Feature Learning ; The ability of the Generative Adversarial Networks (GANs) framework to learn generative models ma\\n',\n",
       " ' The Mythos of Model Interpretability ; Supervised machine learning models boast remarkable predictive capabilities. But can you \\n',\n",
       " ' Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a   Changing World ; In this paper, we focus on online repre\\n',\n",
       " ' Borrowing Treasures from the Wealthy: Deep Transfer Learning through   Selective Joint Fine-tuning ; Deep neural networks requir\\n',\n",
       " ' Aligned Image-Word Representations Improve Inductive Transfer Across   Vision-Language Tasks ; An important goal of computer vis\\n',\n",
       " ' Universal Adversarial Perturbations Against Semantic Image Segmentation ; While deep learning is remarkably successful on percep\\n',\n",
       " ' The loss surface of deep and wide neural networks ; While the optimization problem behind deep neural networks is highly non-con\\n',\n",
       " ' Semantically Decomposing the Latent Spaces of Generative Adversarial   Networks ; We propose a new algorithm for training genera\\n',\n",
       " ' Variants of RMSProp and Adagrad with Logarithmic Regret Bounds ; Adaptive gradient methods have become recently very popular, in\\n',\n",
       " ' ALICE: Towards Understanding Adversarial Learning for Joint Distribution   Matching ; We investigate the non-identifiability iss\\n',\n",
       " ' A systematic study of the class imbalance problem in convolutional   neural networks ; In this study, we systematically investig\\n',\n",
       " ' Regularization for Deep Learning: A Taxonomy ; Regularization is one of the crucial ingredients of deep learning, yet the term r\\n',\n",
       " ' Clustering with Deep Learning: Taxonomy and New Methods ; Clustering is a fundamental machine learning method. The quality of it\\n',\n",
       " ' Coarse to fine non-rigid registration: a chain of scale-specific neural   networks for multimodal image alignment with applicati\\n',\n",
       " ' Describing Videos by Exploiting Temporal Structure ; Recent progress in using recurrent neural networks (RNNs) for image descrip\\n',\n",
       " ' Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in   the Blanks ; Hybrid methods that utilize both content\\n',\n",
       " ' Sentiment Classification using Images and Label Embeddings ; In this project we analysed how much semantic information images ca\\n',\n",
       " ' Natural-Parameter Networks: A Class of Probabilistic Neural Networks ; Neural networks (NN) have achieved state-of-the-art perfo\\n',\n",
       " ' Learning to Perform Physics Experiments via Deep Reinforcement Learning ; When encountering novel objects, humans are able to in\\n',\n",
       " ' A Network-based End-to-End Trainable Task-oriented Dialogue System ; Teaching machines to accomplish tasks by conversing natural\\n',\n",
       " ' A Factorization Machine Framework for Testing Bigram Embeddings in   Knowledgebase Completion ; Embedding-based Knowledge Base C\\n',\n",
       " ' Neural Networks for Joint Sentence Classification in Medical Paper   Abstracts ; Existing models based on artificial neural netw\\n',\n",
       " ' De-identification of Patient Notes with Recurrent Neural Networks ; Objective: Patient notes in electronic health records (EHRs)\\n',\n",
       " ' Reasoning with Memory Augmented Neural Networks for Language   Comprehension ; Hypothesis testing is an important cognitive proc\\n',\n",
       " ' Automatic Rule Extraction from Long Short Term Memory Networks ; Although deep learning models have proven effective at solving \\n',\n",
       " ' Comparing Rule-Based and Deep Learning Models for Patient Phenotyping ; Objective: We investigate whether deep learning techniqu\\n',\n",
       " ' MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks ; Over 50 million scholarly articles have \\n',\n",
       " ' Transfer Learning for Named-Entity Recognition with Neural Networks ; Recent approaches based on artificial neural networks (ANN\\n',\n",
       " ' Adversarial Generation of Natural Language ; Generative Adversarial Networks (GANs) have gathered a lot of attention from the co\\n',\n",
       " ' Explaining Recurrent Neural Network Predictions in Sentiment Analysis ; Recently, a technique called Layer-wise Relevance Propag\\n',\n",
       " ' Text Compression for Sentiment Analysis via Evolutionary Algorithms ; Can textual data be compressed intelligently without losin\\n',\n",
       " ' Building competitive direct acoustics-to-word models for English   conversational speech recognition ; Direct acoustics-to-word \\n',\n",
       " ' Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for   Visual Question Answering ; We address the problem of \\n',\n",
       " ' Task-driven Visual Saliency and Attention-based Visual Question   Answering ; Visual question answering (VQA) has witnessed grea\\n',\n",
       " ' Optimising The Input Window Alignment in CD-DNN Based Phoneme   Recognition for Low Latency Processing ; We present a systematic\\n',\n",
       " ' Bridging LSTM Architecture and the Neural Dynamics during Reading ; Recently, the long short-term memory neural network (LSTM) h\\n',\n",
       " ' Feature Weight Tuning for Recursive Neural Networks ; This paper addresses how a recursive neural network model can automaticall\\n',\n",
       " ' A New Data Representation Based on Training Data Characteristics to   Extract Drug Named-Entity in Medical Text ; One essential \\n',\n",
       " ' DopeLearning: A Computational Approach to Rap Lyrics Generation ; Writing rap lyrics requires both creativity to construct a mea\\n',\n",
       " ' Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN ; Semantic matching, which aims to determine the matching\\n',\n",
       " ' Piecewise Latent Variables for Neural Variational Text Processing ; Advances in neural variational inference have facilitated th\\n',\n",
       " ' Recurrent Neural Networks with External Memory for Language   Understanding ; Recurrent Neural Networks (RNNs) have become incre\\n',\n",
       " ' A Neural Network Approach to Context-Sensitive Generation of   Conversational Responses ; We present a novel response generation\\n',\n",
       " ' The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured   Multi-Turn Dialogue Systems ; This paper introduces t\\n',\n",
       " ' Building End-To-End Dialogue Systems Using Generative Hierarchical   Neural Network Models ; We investigate the task of building\\n',\n",
       " ' End-to-End Attention-based Large Vocabulary Speech Recognition ; Many of the current state-of-the-art Large Vocabulary Continuou\\n',\n",
       " ' Towards Neural Network-based Reasoning ; We propose Neural Reasoner, a framework for neural network-based reasoning over natural\\n',\n",
       " ' What to talk about and how? Selective Generation using LSTMs with   Coarse-to-Fine Alignment ; We propose an end-to-end, domain-\\n',\n",
       " ' Reasoning about Entailment with Neural Attention ; While most approaches to automatically recognizing entailment relations have \\n',\n",
       " ' Highway Long Short-Term Memory RNNs for Distant Speech Recognition ; In this paper, we extend the deep long short-term memory (D\\n',\n",
       " ' Neural Enquirer: Learning to Query Tables with Natural Language ; We proposed Neural Enquirer as a neural network architecture t\\n',\n",
       " ' Sentence Pair Scoring: Towards Unified Framework for Text Comprehension ; We review the task of Sentence Pair Scoring, popular i\\n',\n",
       " ' Incorporating Copying Mechanism in Sequence-to-Sequence Learning ; We address an important problem in sequence-to-sequence (Seq2\\n',\n",
       " ' Generating Factoid Questions With Recurrent Neural Networks: The 30M   Factoid Question-Answer Corpus ; Over the past decade, la\\n',\n",
       " ' How NOT To Evaluate Your Dialogue System: An Empirical Study of   Unsupervised Evaluation Metrics for Dialogue Response Generati\\n',\n",
       " ' A Hierarchical Latent Variable Encoder-Decoder Model for Generating   Dialogues ; Sequential data often possesses a hierarchical\\n',\n",
       " ' Neural Associative Memory for Dual-Sequence Modeling ; Many important NLP problems can be posed as dual-sequence or sequence-to-\\n',\n",
       " ' Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior   Knowledge ; We introduce LL-RNNs (Log-Linear RNNs), an \\n',\n",
       " ' Embracing data abundance: BookTest Dataset for Reading Comprehension ; There is a practically unlimited amount of natural langua\\n',\n",
       " ' Quasi-Recurrent Neural Networks ; Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence\\n',\n",
       " ' Input Switched Affine Networks: An RNN Architecture Designed for   Interpretability ; There exist many problem domains where the\\n',\n",
       " ' Frustratingly Short Attention Spans in Neural Language Modeling ; Neural language models predict the next token using a latent r\\n',\n",
       " ' A Structured Self-attentive Sentence Embedding ; This paper proposes a new model for extracting an interpretable sentence embedd\\n',\n",
       " ' A Recurrent Neural Model with Attention for the Recognition of Chinese   Implicit Discourse Relations ; We introduce an attentio\\n',\n",
       " ' Event Representations for Automated Story Generation with Deep Neural   Nets ; Automated story generation is the problem of auto\\n',\n",
       " ' A Joint Model for Question Answering and Question Generation ; We propose a generative machine comprehension model that learns j\\n',\n",
       " ' Learning Intrinsic Sparse Structures within Long Short-Term Memory ; Model compression is significant for the wide adoption of R\\n',\n",
       " ' Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational   Compositional Operators for Analogy Detection ; Represen\\n',\n",
       " ' Object-oriented Neural Programming (OONP) for Document Understanding ; We propose Object-oriented Neural Programming (OONP), a f\\n',\n",
       " ' A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering ; This paper proposes a novel neural machine reading mode\\n',\n",
       " ' Improving speech recognition by revising gated recurrent units ; Speech recognition is largely taking advantage of deep learning\\n',\n",
       " ' Integrating planning for task-completion dialogue policy learning ; Training a task-completion dialogue agent with real users vi\\n',\n",
       " ' Building DNN Acoustic Models for Large Vocabulary Speech Recognition ; Deep neural networks (DNNs) are now a central component o\\n',\n",
       " ' Deep Recurrent Neural Networks for Acoustic Modelling ; We present a novel deep Recurrent Neural Network (RNN) model for acousti\\n',\n",
       " ' Regularizing RNNs by Stabilizing Activations ; We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing th\\n',\n",
       " ' Outrageously Large Neural Networks: The Sparsely-Gated   Mixture-of-Experts Layer ; The capacity of a neural network to absorb i\\n',\n",
       " ' Discourse-Based Objectives for Fast Unsupervised Sentence Representation   Learning ; This work presents a novel objective funct\\n',\n",
       " ' Learning Convolutional Text Representations for Visual Question   Answering ; Visual question answering is a recently proposed a\\n',\n",
       " ' Learning Phrase Representations using RNN Encoder-Decoder for   Statistical Machine Translation ; In this paper, we propose a no\\n',\n",
       " ' Recurrent Neural Network Training with Dark Knowledge Transfer ; Recurrent neural networks (RNNs), particularly long short-term \\n',\n",
       " ' Long Short-Term Memory Based Recurrent Neural Network Architectures for   Large Vocabulary Speech Recognition ; Long Short-Term \\n',\n",
       " ' Monitoring Term Drift Based on Semantic Consistency in an Evolving   Vector Field ; Based on the Aristotelian concept of potenti\\n',\n",
       " ' Towards better decoding and language model integration in sequence to   sequence models ; The recently proposed Sequence-to-Sequ\\n',\n",
       " ' Neural Machine Translation by Jointly Learning to Align and Translate ; Neural machine translation is a recently proposed approa\\n',\n",
       " ' Overcoming the Curse of Sentence Length for Neural Machine Translation   using Automatic Segmentation ; The authors of (Cho et a\\n',\n",
       " ' Transferring Knowledge from a RNN to a DNN ; Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art result\\n',\n",
       " ' Correlational Neural Networks ; Common Representation Learning (CRL), wherein different descriptions (or views) of the data are \\n',\n",
       " ' Attention-Based Models for Speech Recognition ; Recurrent sequence generators conditioned on input data through an attention mec\\n',\n",
       " ' Fast and Accurate Recurrent Neural Network Acoustic Models for Speech   Recognition ; We have recently shown that deep Long Shor\\n',\n",
       " ' Listen, Attend and Spell ; We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utteranc\\n',\n",
       " ' BlackOut: Speeding up Recurrent Neural Network Language Models With Very   Large Vocabularies ; We propose BlackOut, an approxim\\n',\n",
       " ' Character-based Neural Machine Translation ; Neural Machine Translation (MT) has reached state-of-the-art results. However, one \\n',\n",
       " ' A Latent Variable Recurrent Neural Network for Discourse Relation   Language Models ; This paper presents a novel latent variabl\\n',\n",
       " ' Multi-task Recurrent Model for Speech and Speaker Recognition ; Although highly correlated, speech and speaker recognition have \\n',\n",
       " ' Hierarchical Memory Networks ; Memory networks are neural networks with an explicit memory component that can be both read and w\\n',\n",
       " ' Sequence-to-Sequence Learning as Beam-Search Optimization ; Sequence-to-Sequence (seq2seq) modeling has rapidly become an import\\n',\n",
       " ' Grounded Recurrent Neural Networks ; In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural n\\n',\n",
       " ' Latent Intention Dialogue Models ; Developing a dialogue agent that is capable of making autonomous decisions and communicating \\n',\n",
       " ' Transfer Learning for Speech Recognition on a Budget ; End-to-end training of automated speech recognition (ASR) systems require\\n',\n",
       " ' Optimizing expected word error rate via sampling for speech recognition ; State-level minimum Bayes risk (sMBR) training has bec\\n',\n",
       " ' Neural Networks Compression for Language Modeling ; In this paper, we consider several compression techniques for the language m\\n',\n",
       " \" Avoiding Your Teacher's Mistakes: Training Neural Networks with   Controlled Weak Supervision ; Training deep neural networks re\\n\",\n",
       " ' Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy   Optimisation ; In statistical dialogue management, th\\n',\n",
       " ' On Extended Long Short-term Memory and Dependent Bidirectional Recurrent   Neural Network ; In this work, we investigate the mem\\n',\n",
       " ' Learning to Answer Questions From Image Using Convolutional Neural   Network ; In this paper, we propose to employ the convoluti\\n',\n",
       " ' Stacked Attention Networks for Image Question Answering ; This paper presents stacked attention networks (SANs) that learn to an\\n',\n",
       " ' Neural Module Networks ; Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\\n',\n",
       " ' Symbol Grounding Association in Multimodal Sequences with Missing   Elements ; In this paper, we extend a symbolic association f\\n',\n",
       " ' Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe   Noise ; The growing importance of massive datasets wit\\n',\n",
       " ' Describing Multimedia Content using Attention-based Encoder--Decoder   Networks ; Whereas deep neural networks were first mostly\\n',\n",
       " ' Multilingual Image Description with Neural Sequence Models ; In this paper we present an approach to multi-language image descri\\n',\n",
       " ' Deep Embedding for Spatial Role Labeling ; This paper introduces the visually informed embedding of word (VIEW), a continuous ve\\n',\n",
       " ' Image-to-Markup Generation with Coarse-to-Fine Attention ; We present a neural encoder-decoder model to convert images into pres\\n',\n",
       " ' Teaching Machines to Code: Neural Markup Generation with Visual   Attention ; We present a deep recurrent neural network model w\\n',\n",
       " ' Evolution in Groups: A deeper look at synaptic cluster driven evolution   of deep neural networks ; A promising paradigm for ach\\n',\n",
       " ' Mesh Learning for Classifying Cognitive Processes ; A relatively recent advance in cognitive neuroscience has been multi-voxel p\\n',\n",
       " ' Synthesizing Deep Neural Network Architectures using Biological Synaptic   Strength Distributions ; In this work, we perform an \\n',\n",
       " ' A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters   Optimization ; Addressing the issue of SVMs parameters op\\n',\n",
       " ' Density estimation using Real NVP ; Unsupervised learning of probabilistic models is a central yet challenging problem in machin\\n',\n",
       " ' Evolution Strategies as a Scalable Alternative to Reinforcement Learning ; We explore the use of Evolution Strategies (ES), a cl\\n',\n",
       " ' QMDP-Net: Deep Learning for Planning under Partial Observability ; This paper introduces the QMDP-net, a neural network architec\\n',\n",
       " ' TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep   Reinforcement Learning ; Combining deep model-free reinforce\\n',\n",
       " ' Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent   Networks ; A major drawback of backpropagation throug\\n',\n",
       " ' Stochastic Deep Learning in Memristive Networks ; We study the performance of stochastically trained deep neural networks (DNNs)\\n',\n",
       " ' PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction ; Multi-step-ahead time series prediction is one of the \\n',\n",
       " ' Norm-Based Capacity Control in Neural Networks ; We investigate the capacity, convexity and characterization of a general family\\n',\n",
       " ' Improving the Performance of Neural Networks in Regression Tasks Using   Drawering ; The method presented extends a given regres\\n',\n",
       " ' Learning unbiased features ; A key element in transfer learning is representation learning; if representations can be developed \\n',\n",
       " ' Compatible Value Gradients for Reinforcement Learning of Continuous Deep   Policies ; This paper proposes GProp, a deep reinforc\\n',\n",
       " ' Learning dynamic Boltzmann machines with spike-timing dependent   plasticity ; We propose a particularly structured Boltzmann ma\\n',\n",
       " ' Gated Graph Sequence Neural Networks ; Graph-structured data appears frequently in domains including chemistry, natural language\\n',\n",
       " ' Deep Reinforcement Learning in Large Discrete Action Spaces ; Being able to reason in an environment with a large number of disc\\n',\n",
       " ' Value Iteration Networks ; We introduce the value iteration network (VIN): a fully differentiable neural network with a `plannin\\n',\n",
       " ' Recurrent Orthogonal Networks and Long-Memory Tasks ; Although RNNs have been shown to be powerful tools for processing sequenti\\n',\n",
       " ' Learning values across many orders of magnitude ; Most learning algorithms are not invariant to the scale of the function that i\\n',\n",
       " ' Genetic Architect: Discovering Genomic Structure with Learned Neural   Architectures ; Each human genome is a 3 billion base pai\\n',\n",
       " ' Deep Successor Reinforcement Learning ; Learning robust value functions given raw observations and rewards is now possible with \\n',\n",
       " ' RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning ; Deep reinforcement learning (deep RL) has been successful \\n',\n",
       " ' Capacity and Trainability in Recurrent Neural Networks ; Two potential bottlenecks on the expressiveness of recurrent neural net\\n',\n",
       " ' Causal Regularization ; In application domains such as healthcare, we want accurate predictive models that are also causally int\\n',\n",
       " ' On the Behavior of Convolutional Nets for Feature Extraction ; Deep neural networks are representation learning techniques. Duri\\n',\n",
       " ' Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in   Generative Models ; Adversarial learning of probabilistic m\\n',\n",
       " ' Filtering Variational Objectives ; When used as a surrogate objective for maximum likelihood estimation in latent variable model\\n',\n",
       " ' Kernel Implicit Variational Inference ; Recent progress in variational inference has paid much attention to the flexibility of v\\n',\n",
       " ' Non-Markovian Control with Gated End-to-End Memory Policy Networks ; Partially observable environments present an important open\\n',\n",
       " ' Automated Problem Identification: Regression vs Classification via   Evolutionary Deep Networks ; Regression or classification? \\n',\n",
       " ' A Simple Neural Attentive Meta-Learner ; Deep neural networks excel in regimes with large amounts of data, but tend to struggle \\n',\n",
       " ' Kafnets: kernel-based non-parametric activation functions for neural   networks ; Neural networks are generally built by interle\\n',\n",
       " ' Learning model-based planning from scratch ; Conventional wisdom holds that model-based planning is a powerful approach to seque\\n',\n",
       " ' Recurrent Ladder Networks ; We propose a recurrent extension of the Ladder networks whose structure is motivated by the inferenc\\n',\n",
       " ' Generalization in Deep Learning ; With a direct analysis of neural networks, this paper presents a mathematically tight generali\\n',\n",
       " ' Parametrizing filters of a CNN with a GAN ; It is commonly agreed that the use of relevant invariances as a good statistical bia\\n',\n",
       " ' Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence   Learning ; Long Short-Term Memory (LSTM) is a popular appr\\n',\n",
       " ' Learning and Real-time Classification of Hand-written Digits With   Spiking Neural Networks ; We describe a novel spiking neural\\n',\n",
       " ' Overcoming catastrophic forgetting with hard attention to the task ; Catastrophic forgetting occurs when a neural network loses \\n',\n",
       " ' Detecting and Correcting for Label Shift with Black Box Predictors ; Faced with distribution shift between training and test set\\n',\n",
       " ' Generalization in Machine Learning via Analytical Learning Theory ; This paper introduces a novel measure-theoretic learning the\\n',\n",
       " ' Sensitivity and Generalization in Neural Networks: an Empirical Study ; In practice it is often found that large over-parameteri\\n',\n",
       " ' On the importance of single directions for generalization ; Despite their ability to memorize large datasets, deep neural networ\\n',\n",
       " ' Maximin affinity learning of image segmentation ; Images can be segmented by first using a classifier to predict an affinity gra\\n',\n",
       " ' A General Framework for Development of the Cortex-like Visual Object   Recognition System: Waves of Spikes, Predictive Coding an\\n',\n",
       " ' Handwritten Digit Recognition with a Committee of Deep Neural Nets on   GPUs ; The competitive MNIST handwritten digit recogniti\\n',\n",
       " ' Eclectic Extraction of Propositional Rules from Neural Networks ; Artificial Neural Network is among the most popular algorithm \\n',\n",
       " ' Message Passing Multi-Agent GANs ; Communicating and sharing intelligence among agents is an important facet of achieving Artifi\\n',\n",
       " ' Mode Regularized Generative Adversarial Networks ; Although Generative Adversarial Networks achieve state-of-the-art results on \\n',\n",
       " ' Layer-Specific Adaptive Learning Rates for Deep Networks ; The increasing complexity of deep learning architectures is resulting\\n',\n",
       " ' Return of Frustratingly Easy Domain Adaptation ; Unlike human learning, machine learning often fails to handle changes between t\\n',\n",
       " ' Origami: A 803 GOp/s/W Convolutional Network Accelerator ; An ever increasing number of computer vision and image/video processi\\n',\n",
       " ' Option Discovery in Hierarchical Reinforcement Learning using   Spatio-Temporal Clustering ; This paper introduces an automated \\n',\n",
       " ' Residual Networks Behave Like Ensembles of Relatively Shallow Networks ; In this work we propose a novel interpretation of resid\\n',\n",
       " ' Synthesizing the preferred inputs for neurons in neural networks via   deep generator networks ; Deep neural networks (DNNs) hav\\n',\n",
       " ' Structured Convolution Matrices for Energy-efficient Deep learning ; We derive a relationship between network representation in \\n',\n",
       " ' Deep CORAL: Correlation Alignment for Deep Domain Adaptation ; Deep neural networks are able to learn powerful representations f\\n',\n",
       " ' Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition ; 3D action recognition - analysis of human actions based \\n',\n",
       " ' Generalized Dropout ; Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer t\\n',\n",
       " ' Parsimonious Inference on Convolutional Neural Networks: Learning and   applying on-line kernel activation rules ; A new, radica\\n',\n",
       " ' Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks ; We propose an algorithm for meta-learning that is model-agno\\n',\n",
       " ' WRPN: Training and Inference using Wide Reduced-Precision Networks ; For computer vision applications, prior works have shown th\\n',\n",
       " ' Deep Learning is Robust to Massive Label Noise ; Deep neural networks trained on large supervised datasets have led to impressiv\\n',\n",
       " ' Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object   Rotation ; Content-invariance in mapping codes learned\\n',\n",
       " ' Deep Learning for Sensor-based Activity Recognition: A Survey ; Sensor-based activity recognition seeks the profound high-level \\n',\n",
       " ' On the Importance of Consistency in Training Deep Neural Networks ; We explain that the difficulties of training deep neural net\\n',\n",
       " ' UI-Net: Interactive Artificial Neural Networks for Iterative Image   Segmentation Based on a User Model ; For complex segmentati\\n',\n",
       " ' Lightweight Neural Networks ; Most of the weights in a Lightweight Neural Network have a value of zero, while the remaining ones\\n',\n",
       " ' Tensor Field Networks: Rotation- and Translation-Equivariant Neural   Networks for 3D Point Clouds ; We introduce tensor field n\\n',\n",
       " ' Knowledge Matters: Importance of Prior Information for Optimization ; We explore the effect of introducing prior information int\\n',\n",
       " ' Zero-bias autoencoders and the benefits of co-adapting features ; Regularized training of an autoencoder typically results in hi\\n',\n",
       " ' Theory and Tools for the Conversion of Analog to Spiking Convolutional   Neural Networks ; Deep convolutional neural networks (C\\n',\n",
       " ' Stacked Generative Adversarial Networks ; In this paper, we propose a novel generative model named Stacked Generative Adversaria\\n',\n",
       " ' Self-informed neural network structure learning ; We study the problem of large scale, multi-label visual recognition with a lar\\n',\n",
       " ' Learning Activation Functions to Improve Deep Neural Networks ; Artificial neural networks typically have a fixed, non-linear ac\\n',\n",
       " ' Denoising autoencoder with modulated lateral connections learns   invariant representations of natural images ; Suitable lateral\\n',\n",
       " ' A Probabilistic Theory of Deep Learning ; A grand challenge in machine learning is the development of computational algorithms t\\n',\n",
       " ' Integrated Inference and Learning of Neural Factors in Structural   Support Vector Machines ; Tackling pattern recognition probl\\n',\n",
       " ' What Happened to My Dog in That Network: Unraveling Top-down Generators   in Convolutional Neural Networks ; Top-down informatio\\n',\n",
       " ' Virtual Worlds as Proxy for Multi-Object Tracking Analysis ; Modern computer vision algorithms typically require expensive data \\n',\n",
       " ' Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet ; Video sequences contain rich dynamic patterns, such as dy\\n',\n",
       " ' Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural   Networks ; Taking inspiration from biological evolution, we e\\n',\n",
       " ' Alternating Back-Propagation for Generator Network ; This paper proposes an alternating back-propagation algorithm for learning \\n',\n",
       " ' Hyperparameter Transfer Learning through Surrogate Alignment for   Efficient Deep Neural Network Training ; Recently, several op\\n',\n",
       " ' Towards Bayesian Deep Learning: A Framework and Some Existing Methods ; While perception tasks such as visual object recognition\\n',\n",
       " ' Deciding How to Decide: Dynamic Routing in Artificial Neural Networks ; We propose and systematically evaluate three strategies \\n',\n",
       " ' Pixel Deconvolutional Networks ; Deconvolutional layers have been widely used in a variety of deep models for up-sampling, inclu\\n',\n",
       " ' Gaussian Prototypical Networks for Few-Shot Learning on Omniglot ; We propose a novel architecture for $k$-shot classification o\\n',\n",
       " ' Super-Convergence: Very Fast Training of Residual Networks Using Large   Learning Rates ; In this paper, we show a phenomenon, w\\n',\n",
       " ' Generative learning for deep networks ; Learning, taking into account full distribution of the data, referred to as generative, \\n',\n",
       " ' Hierarchical Representations for Efficient Architecture Search ; We explore efficient neural architecture search methods and sho\\n',\n",
       " ' Data Augmentation Generative Adversarial Networks ; Effective training of neural networks requires much data. In the low-data re\\n',\n",
       " ' DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the   Jigsaw Puzzle Problem ; This paper introduces the first dee\\n',\n",
       " ' DeepPainter: Painter Classification Using Deep Convolutional   Autoencoders ; In this paper we describe the problem of painter c\\n',\n",
       " ' DeepBrain: Functional Representation of Neural In-Situ Hybridization   Images for Gene Ontology Classification Using Deep Convol\\n',\n",
       " ' Generative Adversarial Perturbations ; In this paper, we propose novel generative models for creating adversarial examples, slig\\n',\n",
       " ' A Rotation and a Translation Suffice: Fooling CNNs with Simple   Transformations ; We show that simple transformations, namely t\\n',\n",
       " ' Peephole: Predicting Network Performance Before Training ; The quest for performant networks has been a significant force that d\\n',\n",
       " ' An Architecture Combining Convolutional Neural Network (CNN) and Support   Vector Machine (SVM) for Image Classification ; Convo\\n',\n",
       " ' Benchmarking Decoupled Neural Interfaces with Synthetic Gradients ; Artifical Neural Networks are a particular class of learning\\n',\n",
       " ' Segmentation hiérarchique faiblement supervisée ; Image segmentation is the process of partitioning an image into a set of meani\\n',\n",
       " ' Training wide residual networks for deployment using a single bit for   each weight ; For fast and energy-efficient deployment o\\n',\n",
       " ' Deep Learning using Rectified Linear Units (ReLU) ; We introduce the use of rectified linear units (ReLU) as the classification \\n',\n",
       " ' Rectified Factor Networks ; We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-d\\n',\n",
       " ' From Maxout to Channel-Out: Encoding Information on Sparse Pathways ; Motivated by an important insight from neural science, we \\n',\n",
       " ' Competitive Learning with Feedforward Supervisory Signal for Pre-trained   Multilayered Networks ; We propose a novel learning m\\n',\n",
       " ' Deeply-Supervised Nets ; Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while ma\\n',\n",
       " ' Path-SGD: Path-Normalized Optimization in Deep Neural Networks ; We revisit the choice of SGD for training deep neural networks \\n',\n",
       " ' Adapting Resilient Propagation for Deep Learning ; The Resilient Propagation (Rprop) algorithm has been very popular for backpro\\n',\n",
       " ' Convolutional Neural Network for Stereotypical Motor Movement Detection   in Autism ; Autism Spectrum Disorders (ASDs) are often\\n',\n",
       " ' Resnet in Resnet: Generalizing Residual Architectures ; Residual networks (ResNets) have recently achieved state-of-the-art on c\\n',\n",
       " ' Evolutionary Synthesis of Deep Neural Networks via Synaptic   Cluster-driven Genetic Encoding ; There has been significant recen\\n',\n",
       " ' Neural Photo Editing with Introspective Adversarial Networks ; The increasingly photorealistic sample quality of generative imag\\n',\n",
       " ' Adaptive Neural Networks for Efficient Inference ; We present an approach to adaptively utilize deep neural networks in order to\\n',\n",
       " ' Spatial Variational Auto-Encoding via Matrix-Variate Normal   Distributions ; The key idea of variational auto-encoders (VAEs) r\\n',\n",
       " ' Dense Transformer Networks ; The key idea of current deep learning methods for dense prediction is to apply a model on a regular\\n',\n",
       " ' Progressive Learning for Systematic Design of Large Neural Networks ; We develop an algorithm for systematic design of a large a\\n',\n",
       " ' A Classification-Based Perspective on GAN Distributions ; A fundamental, and still largely unanswered, question in the context o\\n',\n",
       " ' Learning Visual Reasoning Without Strong Priors ; Achieving artificial visual reasoning - the ability to answer image-related qu\\n',\n",
       " ' Men Also Like Shopping: Reducing Gender Bias Amplification using   Corpus-level Constraints ; Language is increasingly being use\\n',\n",
       " ' Acquiring Common Sense Spatial Knowledge through Implicit Spatial   Templates ; Spatial understanding is a fundamental problem w\\n',\n",
       " ' FiLM: Visual Reasoning with a General Conditioning Layer ; We introduce a general-purpose conditioning method for neural network\\n',\n",
       " ' Unsupervised Induction of Semantic Roles within a Reconstruction-Error   Minimization Framework ; We introduce a new approach to\\n',\n",
       " ' Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word   Embeddings ; The blind application of machine learning \\n',\n",
       " ' TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency ; In this paper, we propose TopicRNN, a recurrent neura\\n',\n",
       " ' Gaussian Attention Model and Its Application to Knowledge Base Embedding   and Question Answering ; We propose the Gaussian atte\\n',\n",
       " ' Variable Computation in Recurrent Neural Networks ; Recurrent neural networks (RNNs) have been used extensively and with increas\\n',\n",
       " ' Learning to Learn from Weak Supervision by Full Supervision ; In this paper, we propose a method for training neural networks wh\\n',\n",
       " ' SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for   Predicting Chemical Properties ; Chemical databases store\\n',\n",
       " ' Sample Efficient Deep Reinforcement Learning for Dialogue Systems with   Large Action Spaces ; In spoken dialogue systems, we ai\\n',\n",
       " ' High-Dimensional Vector Semantics ; In this paper we explore the \"vector semantics\" problem from the perspective of \"almost orth\\n',\n",
       " ' Learning Semantic Script Knowledge with Event Embeddings ; Induction of common sense knowledge about prototypical sequences of e\\n',\n",
       " ' Mathematical Language Processing: Automatic Grading and Feedback for   Open Response Mathematical Questions ; While computer and\\n',\n",
       " ' Nonparametric Bayesian Double Articulation Analyzer for Direct Language   Acquisition from Continuous Speech Signals ; Human inf\\n',\n",
       " ' Harnessing Deep Neural Networks with Logic Rules ; Combining deep neural networks with structured logic rules is desirable to ha\\n',\n",
       " ' Toward Controlled Generation of Text ; Generic generation and manipulation of text is challenging and has limited success compar\\n',\n",
       " ' Adversarial Connective-exploiting Networks for Implicit Discourse   Relation Classification ; Implicit discourse relation classi\\n',\n",
       " ' Abstract Syntax Networks for Code Generation and Semantic Parsing ; Tasks like code generation and semantic parsing require mapp\\n',\n",
       " ' Multimodal Word Distributions ; Word embeddings provide point representations of words containing useful semantic information. W\\n',\n",
       " ' Guiding Reinforcement Learning Exploration Using Natural Language ; In this work we present a technique to use natural language \\n',\n",
       " ' Robust Task Clustering for Deep Many-Task Learning ; We investigate task clustering for deep-learning based multi-task and few-s\\n',\n",
       " ' Natural Language Multitasking: Analyzing and Improving Syntactic   Saliency of Hidden Representations ; We train multi-task auto\\n',\n",
       " ' Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement   Learning ; With the increasing popularity of video shar\\n',\n",
       " ' A Supervised Approach to Extractive Summarisation of Scientific Papers ; Automatic summarisation is a popular approach to reduce\\n',\n",
       " ' Language Models for Image Captioning: The Quirks and What Works ; Two recent approaches have achieved state-of-the-art results i\\n',\n",
       " ' Exploring Models and Data for Image Question Answering ; This work aims to address the problem of image-based question-answering\\n',\n",
       " ' Making the V in VQA Matter: Elevating the Role of Image Understanding in   Visual Question Answering ; Problems at the intersect\\n',\n",
       " ' A Multi-World Approach to Question Answering about Real-World Scenes   based on Uncertain Input ; We propose a method for automa\\n',\n",
       " ' Hard to Cheat: A Turing Test based on Answering Questions about Images ; Progress in language and image understanding by machine\\n',\n",
       " ' Analyzing the Behavior of Visual Question Answering Models ; Recently, a number of deep-learning based models have been proposed\\n',\n",
       " ' Sort Story: Sorting Jumbled Images and Captions into Stories ; Temporal common sense has applications in AI tasks such as QA, mu\\n',\n",
       " ' Mean Box Pooling: A Rich Image Representation and Output Embedding for   the Visual Madlibs Task ; We present Mean Box Pooling, \\n',\n",
       " ' Learning to generalize to new compositions in image understanding ; Recurrent neural networks have recently been used for learni\\n',\n",
       " ' Measuring Machine Intelligence Through Visual Question Answering ; As machines have become more intelligent, there has been a re\\n',\n",
       " ' Towards Transparent AI Systems: Interpreting Visual Question Answering   Models ; Deep neural networks have shown striking progr\\n',\n",
       " ' Visual Dialog ; We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in na\\n',\n",
       " ' Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic   Speech Recognition ; Multi-task learning (MTL) involves\\n',\n",
       " ' Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning ; We introduce the first goal-driven training for v\\n',\n",
       " ' Being Negative but Constructively: Lessons Learnt from Creating Better   Visual Question Answering Datasets ; Visual question an\\n',\n",
       " ' C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0   Dataset ; Visual Question Answering (VQA) has receive\\n',\n",
       " ' Deep learning evaluation using deep linguistic processing ; We discuss problems with the standard approaches to evaluation for t\\n',\n",
       " ' meProp: Sparsified Back Propagation for Accelerated Deep Learning with   Reduced Overfitting ; We propose a simple yet effective\\n',\n",
       " ' Towards Crafting Text Adversarial Samples ; Adversarial samples are strategically modified samples, which are crafted with the p\\n',\n",
       " ' Reinforced Video Captioning with Entailment Rewards ; Sequence-to-sequence models have shown promising improvements on the tempo\\n',\n",
       " ' Hierarchically-Attentive RNN for Album Summarization and Storytelling ; We address the problem of end-to-end visual storytelling\\n',\n",
       " ' Generating Natural Adversarial Examples ; Due to their complex nature, it is hard to characterize the ways in which machine lear\\n',\n",
       " ' Training Simplification and Model Simplification for Deep Learning: A   Minimal Effort Back Propagation Method ; We propose a si\\n',\n",
       " ' Embodied Question Answering ; We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned \\n',\n",
       " \" Don't Just Assume; Look and Answer: Overcoming Priors for Visual   Question Answering ; A number of studies have found that toda\\n\",\n",
       " ' CoDraw: Visual Dialog for Collaborative Drawing ; In this work, we propose a goal-driven collaborative task that contains vision\\n',\n",
       " \" Answerer in Questioner's Mind for Goal-Oriented Visual Dialogue ; Goal-oriented dialogue has been paid attention for its numerou\\n\",\n",
       " ' Resource Constrained Structured Prediction ; We study the problem of structured prediction under test-time budget constraints. W\\n',\n",
       " ' Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to   Action Sequences ; We propose a neural sequence-to-se\\n',\n",
       " ' Coupling Distributed and Symbolic Execution for Natural Language Queries ; Building neural networks to query a knowledge base (a\\n',\n",
       " ' An agent-driven semantical identifier using radial basis neural networks   and reinforcement learning ; Due to the huge availabi\\n',\n",
       " ' Where is my forearm? Clustering of body parts from simultaneous tactile   and linguistic input using sequential mapping ; Humans\\n',\n",
       " ' Improvements to deep convolutional neural networks for LVCSR ; Deep Convolutional Neural Networks (CNNs) are more powerful than \\n',\n",
       " ' Collaborative Deep Learning for Recommender Systems ; Collaborative filtering (CF) is a successful approach commonly used by man\\n',\n",
       " ' Explaining Predictions of Non-Linear Classifiers in NLP ; Layer-wise relevance propagation (LRP) is a recently proposed techniqu\\n',\n",
       " ' Tensor network language model ; We propose a new statistical model suitable for machine learning of systems with long distance c\\n',\n",
       " ' Language as a matrix product state ; We propose a statistical model for natural language that begins by considering language as \\n',\n",
       " ' Accelerating Hessian-free optimization for deep neural networks by   implicit preconditioning and sampling ; Hessian-free traini\\n',\n",
       " ' Is a Picture Worth Ten Thousand Words in a Review Dataset? ; While textual reviews have become prominent in many recommendation-\\n',\n",
       " ' Validation of nonlinear PCA ; Linear principal component analysis (PCA) can be extended to a nonlinear PCA by using artificial n\\n',\n",
       " ' Graph Approximation and Clustering on a Budget ; We consider the problem of learning from a similarity matrix (such as spectral \\n',\n",
       " ' ShareBoost: Efficient Multiclass Learning with Feature Sharing ; Multiclass prediction is the problem of classifying an object i\\n',\n",
       " ' Functional Principal Component Analysis and Randomized Sparse Clustering   Algorithm for Medical Image Analysis ; Due to advance\\n',\n",
       " ' Jointly Learning Multiple Measures of Similarities from Triplet   Comparisons ; Similarity between objects is multi-faceted and \\n',\n",
       " ' Variational Inference for Uncertainty on the Inputs of Gaussian Process   Models ; The Gaussian process latent variable model (G\\n',\n",
       " ' Conditional Generative Adversarial Nets ; Generative Adversarial Nets [8] were recently introduced as a novel way to train gener\\n',\n",
       " ' Visual Causal Feature Learning ; We provide a rigorous definition of the visual cause of a behavior that is broadly applicable t\\n',\n",
       " ' In Search of the Real Inductive Bias: On the Role of Implicit   Regularization in Deep Learning ; We present experiments demonst\\n',\n",
       " ' Domain Generalization for Object Recognition with Multi-task   Autoencoders ; The problem of domain generalization is to take kn\\n',\n",
       " ' Data-Efficient Learning of Feedback Policies from Image Pixels using   Deep Dynamical Models ; Data-efficient reinforcement lear\\n',\n",
       " ' Scatter Component Analysis: A Unified Framework for Domain Adaptation   and Domain Generalization ; This paper addresses classif\\n',\n",
       " ' Robust Subspace Clustering via Tighter Rank Approximation ; Matrix rank minimization problem is in general NP-hard. The nuclear \\n',\n",
       " ' Recognizing Semantic Features in Faces using Deep Learning ; The human face constantly conveys information, both consciously and\\n',\n",
       " ' Deep Reconstruction-Classification Networks for Unsupervised Domain   Adaptation ; In this paper, we propose a novel unsupervise\\n',\n",
       " ' A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation ; Finding the most effective way to aggregate multi-subject \\n',\n",
       " ' Feedback-Controlled Sequential Lasso Screening ; One way to solve lasso problems when the dictionary does not fit into available\\n',\n",
       " ' The Symmetry of a Simple Optimization Problem in Lasso Screening ; Recently dictionary screening has been proposed as an effecti\\n',\n",
       " ' Hard Negative Mining for Metric Learning Based Zero-Shot Classification ; Zero-Shot learning has been shown to be an efficient s\\n',\n",
       " ' Pose-Selective Max Pooling for Measuring Similarity ; In this paper, we deal with two challenges for measuring the similarity of\\n',\n",
       " ' Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble   of Autoencoders ; A fall is an abnormal activity that\\n',\n",
       " ' Generalization Error of Invariant Classifiers ; This paper studies the generalization error of invariant classifiers. In particu\\n',\n",
       " ' Universal adversarial perturbations ; Given a state-of-the-art deep neural network classifier, we show the existence of a univer\\n',\n",
       " ' Linear Disentangled Representation Learning for Facial Actions ; Limited annotated data available for the recognition of facial \\n',\n",
       " ' On Detecting Adversarial Perturbations ; Machine learning and deep learning in particular has advanced tremendously on perceptua\\n',\n",
       " ' Activation Maximization Generative Adversarial Nets ; Class labels have been empirically shown useful in improving the sample qu\\n',\n",
       " ' Interpretable Explanations of Black Boxes by Meaningful Perturbation ; As machine learning algorithms are increasingly applied t\\n',\n",
       " ' A General Theory for Training Learning Machine ; Though the deep learning is pushing the machine learning to a new stage, basic \\n',\n",
       " ' A Generalization of Convolutional Neural Networks to Graph-Structured   Data ; This paper introduces a generalization of Convolu\\n',\n",
       " ' Formal Guarantees on the Robustness of a Classifier against Adversarial   Manipulation ; Recent work has shown that state-of-the\\n',\n",
       " ' Classification regions of deep neural networks ; The goal of this paper is to analyze the geometric properties of deep neural ne\\n',\n",
       " ' Analysis of universal adversarial perturbations ; Deep networks have recently been shown to be vulnerable to universal perturbat\\n',\n",
       " ' Bayesian GAN ; Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which\\n',\n",
       " ' Unsupervised Learning of Disentangled Representations from Video ; We present a new model DrNET that learns disentangled image r\\n',\n",
       " ' Dualing GANs ; Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is howe\\n',\n",
       " ' Wavelet Residual Network for Low-Dose CT via Deep Convolutional   Framelets ; Model based iterative reconstruction (MBIR) algori\\n',\n",
       " ' 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks ; The success of various applications including robotics, di\\n',\n",
       " ' Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x))   Alternative ; In this article, we mathematically study sev\\n',\n",
       " ' A Brief Survey of Deep Reinforcement Learning ; Deep reinforcement learning is poised to revolutionise the field of AI and repre\\n',\n",
       " ' CirCNN: Accelerating and Compressing Deep Neural Networks Using   Block-CirculantWeight Matrices ; Large-scale deep neural netwo\\n',\n",
       " ' XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual   Classification ; We propose two multimodal deep learning archite\\n',\n",
       " ' Context Embedding Networks ; Low dimensional embeddings that capture the main variations of interest in collections of data are \\n',\n",
       " ' How Much Chemistry Does a Deep Neural Network Need to Know to Make   Accurate Predictions? ; The meteoric rise of deep learning \\n',\n",
       " ' Variational Inference of Disentangled Latent Concepts from Unlabeled   Observations ; Disentangled representations, where the hi\\n',\n",
       " ' Three Factors Influencing Minima in SGD ; We study the properties of the endpoint of stochastic gradient descent (SGD). By appro\\n',\n",
       " ' Learning to Play Othello with Deep Neural Networks ; Achieving superhuman playing level by AlphaGo corroborated the capabilities\\n',\n",
       " ' Deep Learning Can Reverse Photon Migration for Diffuse Optical   Tomography ; Can artificial intelligence (AI) learn complicated\\n',\n",
       " ' Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for   Transferable Chemical Property Prediction ; With access to\\n',\n",
       " ' Deep Learning in RF Sub-sampled B-mode Ultrasound Imaging ; In portable, three dimensional, and ultra-fast ultrasound (US) imagi\\n',\n",
       " ' Deep Learning Interior Tomography for Region-of-Interest Reconstruction ; Interior tomography for the region-of-interest (ROI) i\\n',\n",
       " ' Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner ; For homeland and transportation security applications, \\n',\n",
       " ' Effective Building Block Design for Deep Convolutional Neural Networks   using Search ; Deep learning has shown promising result\\n',\n",
       " ' TVAE: Triplet-Based Variational Autoencoder using Metric Learning ; Deep metric learning has been demonstrated to be highly effe\\n',\n",
       " ' Learning to Play with Intrinsically-Motivated Self-Aware Agents ; Infants are experts at playing, with an amazing ability to gen\\n',\n",
       " ' Emergence of Structured Behaviors from Curiosity-Based Intrinsic   Motivation ; Infants are experts at playing, with an amazing \\n',\n",
       " ' Stochastic Video Generation with a Learned Prior ; Generating video frames that accurately predict future world states is challe\\n',\n",
       " ' Multi-Evidence Filtering and Fusion for Multi-Label Classification,   Object Detection and Semantic Segmentation Based on Weakly\\n',\n",
       " ' Neural Networks Should Be Wide Enough to Learn Disconnected Decision   Regions ; In the recent literature the important role of \\n',\n",
       " \" Visual Explanations From Deep 3D Convolutional Neural Networks for   Alzheimer's Disease Classification ; We develop three effic\\n\",\n",
       " ' Averaging Weights Leads to Wider Optima and Better Generalization ; Deep neural networks are typically trained by optimizing a l\\n',\n",
       " ' SENNS: Sparse Extraction Neural NetworkS for Feature Extraction ; By drawing on ideas from optimisation theory, artificial neura\\n',\n",
       " ' Generative Models and Model Criticism via Optimized Maximum Mean   Discrepancy ; We propose a method to optimize the representat\\n',\n",
       " ' Deep Learning Approximation for Stochastic Control Problems ; Many real world stochastic control problems suffer from the \"curse\\n',\n",
       " ' Generating Focussed Molecule Libraries for Drug Discovery with Recurrent   Neural Networks ; In de novo drug design, computation\\n',\n",
       " ' Parameter Space Noise for Exploration ; Deep reinforcement learning (RL) methods generally engage in exploratory behavior throug\\n',\n",
       " ' On The Robustness of a Neural Network ; With the development of neural networks based machine learning and their usage in missio\\n',\n",
       " ' ZhuSuan: A Library for Bayesian Deep Learning ; In this paper we introduce ZhuSuan, a python probabilistic programming library f\\n',\n",
       " ' Using Parameterized Black-Box Priors to Scale Up Model-Based Policy   Search for Robotics ; The most data-efficient algorithms f\\n',\n",
       " ' Bayesian Optimization with Automatic Prior Selection for Data-Efficient   Direct Policy Search ; One of the most interesting fea\\n',\n",
       " ' Bounding and Counting Linear Regions of Deep Neural Networks ; In this paper, we study the representational power of deep neural\\n',\n",
       " ' Deep Rewiring: Training very sparse deep networks ; Neuromorphic hardware tends to pose limits on the connectivity of deep netwo\\n',\n",
       " ' Comparing heterogeneous entities using artificial neural networks of   trainable weighted structural components and machine-lear\\n',\n",
       " ' Active Learning of Inverse Models with Intrinsically Motivated Goal   Exploration in Robots ; We introduce the Self-Adaptive Goa\\n',\n",
       " ' End-to-End Tracking and Semantic Segmentation Using Recurrent Neural   Networks ; In this work we present a novel end-to-end fra\\n',\n",
       " ' Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks ; This paper presents to the best of our knowledge the first\\n',\n",
       " ' Deep Predictive Coding Networks for Video Prediction and Unsupervised   Learning ; While great strides have been made in using d\\n',\n",
       " ' Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient   Convolutional Neural Networks ; This paper proposes a comp\\n',\n",
       " ' On Convergence and Stability of GANs ; We propose studying GAN training dynamics as regret minimization, which is in contrast to\\n',\n",
       " ' Imitation from Observation: Learning to Imitate Behaviors from Raw Video   via Context Translation ; Imitation learning is an ef\\n',\n",
       " ' Convergence rates for pretraining and dropout: Guiding learning   parameters using network structure ; Unsupervised pretraining \\n',\n",
       " ' Learning Discriminative Features via Label Consistent Neural Network ; Deep Convolutional Neural Networks (CNN) enforces supervi\\n',\n",
       " ' Out-of-Sample Extension for Dimensionality Reduction of Noisy Time   Series ; This paper proposes an out-of-sample extension fra\\n',\n",
       " ' Adversarial Examples for Semantic Image Segmentation ; Machine learning methods in general and Deep Neural Networks in particula\\n',\n",
       " ' Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box   Machine Learning Models ; Many machine learning algorit\\n',\n",
       " ' Towards Building an Intelligent Anti-Malware System: A Deep Learning   Approach using Support Vector Machine (SVM) for Malware C\\n',\n",
       " ' Feature extraction using Latent Dirichlet Allocation and Neural   Networks: A case study on movie synopses ; Feature extraction \\n',\n",
       " ' A Survey of Available Corpora for Building Data-Driven Dialogue Systems ; During the past decade, several areas of speech and la\\n',\n",
       " ' Generative Topic Embedding: a Continuous Representation of Documents   (Extended Version with Proofs) ; Word embedding maps word\\n',\n",
       " ' Fine-Grained Entity Typing with High-Multiplicity Assignments ; As entity type systems become richer and more fine-grained, we e\\n',\n",
       " ' Towards a Visual Turing Challenge ; As language and visual understanding by machines progresses rapidly, we are observing an inc\\n',\n",
       " ' Interactive Robot Learning of Gestures, Language and Affordances ; A growing field in robotics and Artificial Intelligence (AI) \\n',\n",
       " ' Visual Features for Context-Aware Speech Recognition ; Automatic transcriptions of consumer-generated multi-media content such a\\n',\n",
       " ' Examining Cooperation in Visual Dialog Models ; In this work we propose a blackbox intervention method for visual dialog models,\\n',\n",
       " ' Video Highlight Prediction Using Audience Chat Reactions ; Sports channel video portals offer an exciting domain for research on\\n',\n",
       " ' Invariant Representations for Noisy Speech Recognition ; Modern automatic speech recognition (ASR) systems need to be robust und\\n',\n",
       " ' Self-Supervised Vision-Based Detection of the Active Speaker as a   Prerequisite for Socially-Aware Language Acquisition ; This \\n',\n",
       " ' Product Characterisation towards Personalisation: Learning Attributes   from Unstructured Data to Recommend Fashion Products ; I\\n',\n",
       " ' The Self-Organization of Speech Sounds ; The speech code is a vehicle of language: it defines a set of forms used by a community\\n',\n",
       " \" What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes ; The F-measure or F-score is one of the most commonly \\n\",\n",
       " ' A Machine Learning Perspective on Predictive Coding with PAQ ; PAQ8 is an open source lossless data compression algorithm that c\\n',\n",
       " ' A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale   SVM Training ; Recently, there has been a renewed inte\\n',\n",
       " ' Semi-supervised Vocabulary-informed Learning ; Despite significant progress in object categorization, in recent years, a number \\n',\n",
       " ' Submodular meets Structured: Finding Diverse Subsets in   Exponentially-Large Structured Item Sets ; To cope with the high level\\n',\n",
       " ' ZM-Net: Real-time Zero-shot Image Manipulation Network ; Many problems in image processing and computer vision (e.g. colorizatio\\n',\n",
       " ' Multi-Agent Diverse Generative Adversarial Networks ; We propose an intuitive generalization to the Generative Adversarial Netwo\\n',\n",
       " ' Geometric GAN ; Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has i\\n',\n",
       " ' A Data and Model-Parallel, Distributed and Scalable Framework for   Training of Deep Networks in Apache Spark ; Training deep ne\\n',\n",
       " ' Understanding and Comparing Deep Neural Networks for Age and Gender   Classification ; Recently, deep neural networks have demon\\n',\n",
       " ' When is a Convolutional Filter Easy To Learn? ; We analyze the convergence of (stochastic) gradient descent algorithm for learni\\n',\n",
       " ' Learning Sparse Visual Representations with Leaky Capped Norm   Regularizers ; Sparsity inducing regularization is an important \\n',\n",
       " ' ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection,   Adversarial Examples and Model Criticism ; ConvNets and I\\n',\n",
       " \" Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of   Spurious Local Minima ; We consider the problem of learning a\\n\",\n",
       " ' Curiosity-driven Exploration by Self-supervised Prediction ; In many real-world scenarios, rewards extrinsic to the agent are ex\\n',\n",
       " ' Houdini: Fooling Deep Structured Prediction Models ; Generating adversarial examples is a critical step for evaluating and impro\\n',\n",
       " ' Recent Advances in Zero-shot Recognition ; With the recent renaissance of deep convolution neural networks, encouraging breakthr\\n',\n",
       " ' The loss surface and expressivity of deep convolutional neural networks ; We analyze the expressiveness and loss surface of prac\\n',\n",
       " ' Physics-guided Neural Networks (PGNN): An Application in Lake   Temperature Modeling ; This paper introduces a novel framework f\\n',\n",
       " ' Unified Spectral Clustering with Optimal Graph ; Spectral clustering has found extensive use in many areas. Most traditional spe\\n',\n",
       " ' On the Inductive Bias of Dropout ; Dropout is a simple but effective technique for learning in neural networks and other setting\\n',\n",
       " ' Surprising properties of dropout in deep networks ; We analyze dropout in deep networks with rectified linear units and the quad\\n',\n",
       " ' Training Probabilistic Spiking Neural Networks with First-to-spike   Decoding ; Third-generation neural networks, or Spiking Neu\\n',\n",
       " ' A Novel Clustering Algorithm Based on Quantum Games ; Enormous successes have been made by quantum algorithms during the last de\\n',\n",
       " ' Exact solutions to the nonlinear dynamics of learning in deep linear   neural networks ; Despite the widespread practical succes\\n',\n",
       " ' Entropy of Overcomplete Kernel Dictionaries ; In signal analysis and synthesis, linear approximation theory considers a linear d\\n',\n",
       " ' Rotation-invariant convolutional neural networks for galaxy morphology   prediction ; Measuring the morphological parameters of \\n',\n",
       " ' Kernel Nonnegative Matrix Factorization Without the Curse of the   Pre-image - Application to Unmixing Hyperspectral Images ; Th\\n',\n",
       " ' Approximation errors of online sparsification criteria ; Many machine learning frameworks, such as resource-allocating networks,\\n',\n",
       " ' Discrete Deep Feature Extraction: A Theory and New Architectures ; First steps towards a mathematical theory of deep convolution\\n',\n",
       " ' Neural Responding Machine for Short-Text Conversation ; We propose Neural Responding Machine (NRM), a neural network-based respo\\n',\n",
       " ' Deep Active Learning for Dialogue Generation ; We propose an online, end-to-end, neural generative conversational model for open\\n',\n",
       " ' Teaching Machines to Read and Comprehend ; Teaching machines to read natural language documents remains an elusive challenge. Ma\\n',\n",
       " ' Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models   of Meaning ; Deep compositional models of meaning actin\\n',\n",
       " ' A Deep Architecture for Semantic Matching with Multiple Positional   Sentence Representations ; Matching natural language senten\\n',\n",
       " ' LSTM Neural Reordering Feature for Statistical Machine Translation ; Artificial neural networks are powerful models, which have \\n',\n",
       " ' Learning Natural Language Inference with LSTM ; Natural language inference (NLI) is a fundamentally important task in natural la\\n',\n",
       " ' Quantifying the vanishing gradient and long distance dependency problem   in recursive neural networks and recursive LSTMs ; Rec\\n',\n",
       " ' Implicit Discourse Relation Classification via Multi-Task Neural   Networks ; Without discourse connectives, classifying implici\\n',\n",
       " ' Enhancing Sentence Relation Modeling with Auxiliary Character-level   Embedding ; Neural network based approaches for sentence r\\n',\n",
       " ' Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks   with Feedback Negative Sampling ; Previous studies in\\n',\n",
       " ' Question Answering over Knowledge Base with Neural Attention Combining   Global Knowledge Information ; With the rapid growth of\\n',\n",
       " ' Generating Natural Language Inference Chains ; The ability to reason with natural language is a fundamental prerequisite for man\\n',\n",
       " ' MuFuRU: The Multi-Function Recurrent Unit ; Recurrent neural networks such as the GRU and LSTM found wide adoption in natural la\\n',\n",
       " ' LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in   Recurrent Neural Networks ; Recurrent neural networks, and in \\n',\n",
       " ' Compression of Neural Machine Translation Models via Pruning ; Neural Machine Translation (NMT), like many other deep learning d\\n',\n",
       " ' Constructing a Natural Language Inference Dataset using Generative   Neural Networks ; Natural Language Inference is an importan\\n',\n",
       " ' Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain   Factoid Question Answering ; While question answering (QA\\n',\n",
       " ' Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM   Encoder-Decoder ; We present Tweet2Vec, a novel method for\\n',\n",
       " ' Online Segment to Segment Neural Transduction ; We introduce an online neural sequence to sequence model that learns to alternat\\n',\n",
       " ' Semantic Parsing with Semi-Supervised Sequential Autoencoders ; We present a novel semi-supervised approach for sequence transdu\\n',\n",
       " ' Exploiting Sentence and Context Representations in Deep Neural Models   for Spoken Language Understanding ; This paper presents \\n',\n",
       " ' The Neural Noisy Channel ; We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent \\n',\n",
       " ' Generative Deep Neural Networks for Dialogue: A Short Review ; Researchers have recently started investigating deep neural netwo\\n',\n",
       " ' Learning Python Code Suggestion with a Sparse Pointer Network ; To enhance developer productivity, all modern integrated develop\\n',\n",
       " ' OpenNMT: Open-Source Toolkit for Neural Machine Translation ; We describe an open-source toolkit for neural machine translation \\n',\n",
       " ' Making Neural QA as Simple as Possible but not Simpler ; Recent development of large-scale question answering (QA) datasets trig\\n',\n",
       " ' Survey of the State of the Art in Natural Language Generation: Core   tasks, applications and evaluation ; This paper surveys th\\n',\n",
       " ' A Constrained Sequence-to-Sequence Neural Model for Sentence   Simplification ; Sentence simplification reduces semantic complex\\n',\n",
       " ' Improved Neural Relation Detection for Knowledge Base Question Answering ; Relation detection is a core component for many NLP a\\n',\n",
       " ' ASR error management for improving spoken language understanding ; This paper addresses the problem of automatic speech recognit\\n',\n",
       " ' Dynamic Integration of Background Knowledge in Neural NLU Systems ; Common-sense or background knowledge is required to understa\\n',\n",
       " ' Rethinking Skip-thought: A Neighborhood based Approach ; We study the skip-thought model with neighborhood information as weak s\\n',\n",
       " ' Neural Domain Adaptation for Biomedical Question Answering ; Factoid question answering (QA) has recently benefited from the dev\\n',\n",
       " ' Neural Models for Key Phrase Detection and Question Generation ; We propose a two-stage neural model to tackle question generati\\n',\n",
       " ' Neural Question Answering at BioASQ 5B ; This paper describes our submission to the 2017 BioASQ challenge. We participated in Ta\\n',\n",
       " ' A Deep Network with Visual Text Composition Behavior ; While natural languages are compositional, how state-of-the-art neural mo\\n',\n",
       " ' Semi-supervised emotion lexicon expansion with label propagation and   specialized word embeddings ; There exist two main approa\\n',\n",
       " ' Modelling Protagonist Goals and Desires in First-Person Narrative ; Many genres of natural language text are narratively structu\\n',\n",
       " ' Understanding Grounded Language Learning Agents ; Neural network-based systems can now learn to locate the referents of words an\\n',\n",
       " ' Just ASK: Building an Architecture for Extensible Self-Service Spoken   Language Understanding ; This paper presents the design \\n',\n",
       " ' The NarrativeQA Reading Comprehension Challenge ; Reading comprehension (RC)---in contrast to information retrieval---requires i\\n',\n",
       " ' Cognitive Database: A Step towards Endowing Relational Databases with   Artificial Intelligence Capabilities ; We propose Cognit\\n',\n",
       " ' Feudal Reinforcement Learning for Dialogue Management in Large Domains ; Reinforcement learning (RL) is a promising approach to \\n',\n",
       " ' An Analysis of Neural Language Modeling at Multiple Scales ; Many of the leading approaches in language modeling introduce novel\\n',\n",
       " ' Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy   and Reverberant Environments ; We propose a spatial dif\\n',\n",
       " ' Character-Aware Neural Language Models ; We describe a simple neural language model that relies only on character-level inputs. \\n',\n",
       " ' Neural-based machine translation for medical text domain. Based on   European Medicines Agency leaflet texts ; The quality of ma\\n',\n",
       " ' Conditional Generation and Snapshot Learning in Neural Dialogue Systems ; Recently a variety of LSTM-based conditional language \\n',\n",
       " ' Dialog state tracking, a machine reading approach using Memory Network ; In an end-to-end dialog system, the aim of dialog state\\n',\n",
       " ' A Physical Metaphor to Study Semantic Drift ; In accessibility tests for digital preservation, over time we experience drifts of\\n',\n",
       " ' Optimizing Neural Network Hyperparameters with Gaussian Processes for   Dialog Act Classification ; Systems based on artificial \\n',\n",
       " ' A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder ; Speech Translation has always been about giving source \\n',\n",
       " ' Learning to Reason With Adaptive Computation ; Multi-hop inference is necessary for machine learning systems to successfully sol\\n',\n",
       " ' Feature-Augmented Neural Networks for Patient Note De-identification ; Patient notes contain a wealth of information of potentia\\n',\n",
       " ' Direct Acoustics-to-Word Models for English Conversational Speech   Recognition ; Recent work on end-to-end automatic speech rec\\n',\n",
       " ' Factorization tricks for LSTM networks ; We present two simple ways of reducing the number of parameters and accelerating the tr\\n',\n",
       " ' NeuroNER: an easy-to-use program for named-entity recognition based on   neural networks ; Named-entity recognition (NER) aims a\\n',\n",
       " ' Syllable-aware Neural Language Models: A Failure to Beat Character-aware   Ones ; Syllabification does not seem to improve word-\\n',\n",
       " ' A Benchmarking Environment for Reinforcement Learning Based Task   Oriented Dialogue Management ; Dialogue assistants are rapidl\\n',\n",
       " ' Reusing Weights in Subword-aware Neural Language Models ; We propose several ways of reusing subword embeddings and other weight\\n',\n",
       " ' Multi-task Learning of Pairwise Sequence Classification Tasks Over   Disparate Label Spaces ; We combine multi-task learning and\\n',\n",
       " ' Dynamic Memory Networks for Visual and Textual Question Answering ; Neural network architectures with memory and attention mecha\\n',\n",
       " ' Picture It In Your Mind: Generating High Level Visual Representations   From Textual Descriptions ; In this paper we tackle the \\n',\n",
       " ' Where to put the Image in an Image Caption Generator ; When a recurrent neural network language model is used for caption genera\\n',\n",
       " ' A Focused Dynamic Attention Model for Visual Question Answering ; Visual Question and Answering (VQA) problems are attracting in\\n',\n",
       " ' Simple Image Description Generator via a Linear Phrase-Based Approach ; Generating a novel textual description of an image is an\\n',\n",
       " ' Multimodal Convolutional Neural Networks for Matching Image and Sentence ; In this paper, we propose multimodal convolutional ne\\n',\n",
       " ' Learning to Compose Neural Networks for Question Answering ; We describe a question answering model that applies to both images \\n',\n",
       " ' Signer-independent Fingerspelling Recognition with Deep Neural Network   Adaptation ; We study the problem of recognition of fin\\n',\n",
       " ' Full-Network Embedding in a Multimodal Embedding Pipeline ; The current state-of-the-art for image annotation and image retrieva\\n',\n",
       " ' What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption   Generator? ; In neural image captioning systems, a re\\n',\n",
       " ' A Fixed-Size Encoding Method for Variable-Length Sequences with its   Application to Neural Network Language Models ; In this pa\\n',\n",
       " ' Transition-Based Dependency Parsing with Stack Long Short-Term Memory ; We propose a technique for learning representations of p\\n',\n",
       " ' A Semisupervised Approach for Language Identification based on Ladder   Networks ; In this study we address the problem of train\\n',\n",
       " ' First-Pass Large Vocabulary Continuous Speech Recognition using   Bi-Directional Recurrent DNNs ; We present a method to perform\\n',\n",
       " ' Applying deep learning techniques on medical corpora from the World Wide   Web: a prototypical system and evaluation ; BACKGROUN\\n',\n",
       " ' Syntax-based Deep Matching of Short Texts ; Many tasks in natural language processing, ranging from machine translation to quest\\n',\n",
       " ' Ensemble of Generative and Discriminative Techniques for Sentiment   Analysis of Movie Reviews ; Sentiment analysis is a common \\n',\n",
       " ' Diverse Embedding Neural Network Language Models ; We propose Diverse Embedding Neural Network (DENN), a novel architecture for \\n',\n",
       " ' Learning linearly separable features for speech recognition using   convolutional neural networks ; Automatic speech recognition\\n',\n",
       " ' Learning to Transduce with Unbounded Memory ; Recently, strong results have been demonstrated by Deep Recurrent Neural Networks \\n',\n",
       " ' Feedforward Sequential Memory Neural Networks without Recurrent Feedback ; We introduce a new structure for memory neural networ\\n',\n",
       " ' Towards Structured Deep Neural Network for Automatic Speech Recognition ; In this paper we propose the Structured Deep Neural Ne\\n',\n",
       " ' Character-Level Incremental Speech Recognition with Recurrent Neural   Networks ; In real-time speech recognition applications, \\n',\n",
       " ' Globally Normalized Transition-Based Neural Networks ; We introduce a globally normalized transition-based neural network model \\n',\n",
       " ' Clinical Information Extraction via Convolutional Neural Network ; We report an implementation of a clinical information extract\\n',\n",
       " ' Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations ; We propose zoneout, a novel method for regularizing RNNs.\\n',\n",
       " ' Stance Detection with Bidirectional Conditional Encoding ; Stance detection is the task of classifying the attitude expressed in\\n',\n",
       " ' SMS Spam Filtering using Probabilistic Topic Modelling and Stacked   Denoising Autoencoder ; In This paper we present a novel ap\\n',\n",
       " ' Bidirectional Recurrent Neural Networks for Medical Event Detection in   Electronic Health Records ; Sequence labeling for extra\\n',\n",
       " ' Sequence Training and Adaptation of Highway Deep Neural Networks ; Highway deep neural network (HDNN) is a type of depth-gated f\\n',\n",
       " ' Recurrent Highway Networks ; Many sequential processing tasks require complex nonlinear transition functions from one step to th\\n',\n",
       " ' Towards cross-lingual distributed representations without parallel text   trained with adversarial autoencoders ; Current approa\\n',\n",
       " ' Memory Visualization for Gated Recurrent Neural Networks in Speech   Recognition ; Recurrent neural networks (RNNs) have shown c\\n',\n",
       " ' Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large   Vocabulary Speech Recognition ; We present results that show i\\n',\n",
       " ' Unsupervised Pretraining for Sequence to Sequence Learning ; This work presents a general unsupervised learning method to improv\\n',\n",
       " ' Structured Attention Networks ; Attention networks have proven to be an effective approach for embedding categorical inference w\\n',\n",
       " ' End-to-End Multi-View Networks for Text Classification ; We propose a multi-view network for text classification. Our method aut\\n',\n",
       " ' Differentiable Scheduled Sampling for Credit Assignment ; We demonstrate that a continuous relaxation of the argmax operation ca\\n',\n",
       " ' Phone-aware Neural Language Identification ; Pure acoustic neural models, particularly the LSTM-RNN model, have shown great pote\\n',\n",
       " ' Detecting Off-topic Responses to Visual Prompts ; Automated methods for essay scoring have made great progress in recent years, \\n',\n",
       " ' Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation   Functions in Quasi-Recurrent Neural Networks ; In this\\n',\n",
       " ' Fidelity-Weighted Learning ; Training deep neural networks requires many training samples, but in practice training labels are e\\n',\n",
       " ' Feature Learning in Deep Neural Networks - Studies on Speech Recognition   Tasks ; Recent studies have shown that deep neural ne\\n',\n",
       " ' Estimating Phoneme Class Conditional Probabilities from Raw Speech   Signal using Convolutional Neural Networks ; In hybrid hidd\\n',\n",
       " ' Recursive Neural Networks Can Learn Logical Semantics ; Tree-structured recursive neural networks (TreeRNNs) for sentence meanin\\n',\n",
       " ' A Re-ranking Model for Dependency Parser with Recursive Convolutional   Neural Network ; In this work, we address the problem to\\n',\n",
       " ' Deep Speaker Vectors for Semi Text-independent Speaker Verification ; Recent research shows that deep neural networks (DNNs) can\\n',\n",
       " ' Advances in Very Deep Convolutional Neural Networks for LVCSR ; Very deep CNNs with small 3x3 kernels have recently been shown t\\n',\n",
       " ' Learning Compact Recurrent Neural Networks ; Recurrent neural networks (RNNs), including long short-term memory (LSTM) RNNs, hav\\n',\n",
       " ' Dependency Parsing with LSTMs: An Empirical Evaluation ; We propose a transition-based dependency parser using Recurrent Neural \\n',\n",
       " ' Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis   and Application to Information Retrieval ; This paper \\n',\n",
       " ' Encoding Source Language with Convolutional Neural Network for Machine   Translation ; The recently proposed neural network join\\n',\n",
       " ' Maximum a Posteriori Adaptation of Network Parameters in Deep Models ; We present a Bayesian approach to adapting parameters of \\n',\n",
       " ' Context-Dependent Translation Selection Using Convolutional Neural   Network ; We propose a novel method for translation selecti\\n',\n",
       " ' Convolutional Neural Network Architectures for Matching Natural Language   Sentences ; Semantic matching is of central importanc\\n',\n",
       " ' Long Short-Term Memory Over Tree Structures ; The chain-structured long short-term memory (LSTM) has showed to be effective in a\\n',\n",
       " ' Improving the Performance of Neural Machine Translation Involving   Morphologically Rich Languages ; The advent of the attention\\n',\n",
       " ' A recurrent neural network without chaos ; We introduce an exceptionally simple gated recurrent neural network (RNN) that achiev\\n',\n",
       " ' End-to-end Phoneme Sequence Recognition using Convolutional Neural   Networks ; Most phoneme recognition state-of-the-art system\\n',\n",
       " ' A Deep Learning Approach to Data-driven Parameterizations for   Statistical Parametric Speech Synthesis ; Nearly all Statistical\\n',\n",
       " ' Addressing the Rare Word Problem in Neural Machine Translation ; Neural Machine Translation (NMT) is a new approach to machine t\\n',\n",
       " ' Investigating the Role of Prior Disambiguation in Deep-learning   Compositional Models of Meaning ; This paper aims to explore t\\n',\n",
       " ' Deep Speech: Scaling up end-to-end speech recognition ; We present a state-of-the-art speech recognition system developed using \\n',\n",
       " ' Incremental Adaptation Strategies for Neural Network Language Models ; It is today acknowledged that neural network language mod\\n',\n",
       " ' Joint RNN-Based Greedy Parsing and Word Composition ; This paper introduces a greedy parser based on neural networks, which leve\\n',\n",
       " ' Efficient Exact Gradient Update for training Deep Networks with Very   Large Sparse Targets ; An important class of problems inv\\n',\n",
       " ' Discriminative Neural Sentence Modeling by Tree-Based Convolution ; This paper proposes a tree-based convolutional neural networ\\n',\n",
       " ' Self-Adaptive Hierarchical Sentence Model ; The ability to accurately model a sentence at varying stages (e.g., word-phrase-sent\\n',\n",
       " ' Classifying Relations by Ranking with Convolutional Neural Networks ; Relation classification is an important semantic processin\\n',\n",
       " ' Lexical Translation Model Using a Deep Neural Network Architecture ; In this paper we combine the advantages of a model using gl\\n',\n",
       " ' Visualizing and Understanding Recurrent Networks ; Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-\\n',\n",
       " ' A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for   Unsupervised Discovery of Linguistic Units and Generatio\\n',\n",
       " ' Author Identification using Multi-headed Recurrent Neural Networks ; Recurrent neural networks (RNNs) are very good at modelling\\n',\n",
       " ' A Deep Memory-based Architecture for Sequence-to-Sequence Learning ; We propose DEEPMEMORY, a novel deep architecture for sequen\\n',\n",
       " ' Ask Me Anything: Dynamic Memory Networks for Natural Language Processing ; Most tasks in natural language processing can be cast\\n',\n",
       " ' Improved Deep Speaker Feature Learning for Text-Dependent Speaker   Recognition ; A deep learning approach has been proposed rec\\n',\n",
       " ' Grid Long Short-Term Memory ; This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidime\\n',\n",
       " ' A Dependency-Based Neural Network for Relation Classification ; Previous research on relation classification has verified the ef\\n',\n",
       " ' PTE: Predictive Text Embedding through Large-scale Heterogeneous Text   Networks ; Unsupervised text embedding methods, such as \\n',\n",
       " ' Relation Classification via Recurrent Neural Network ; Deep learning has gained much success in sentence-level relation classifi\\n',\n",
       " ' Learning from LDA using Deep Neural Networks ; Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian model fo\\n',\n",
       " ' Online Representation Learning in Recurrent Neural Language Models ; We investigate an extension of continuous online learning i\\n',\n",
       " \" A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional   Neural Networks for Sentence Classification ; Convolutio\\n\",\n",
       " ' Prediction-Adaptation-Correction Recurrent Neural Networks for   Low-Resource Language Speech Recognition ; In this paper, we in\\n',\n",
       " ' Generating Text with Deep Reinforcement Learning ; We introduce a novel schema for sequence to sequence learning with a Deep Q-N\\n',\n",
       " ' Detecting Interrogative Utterances with Recurrent Neural Networks ; In this paper, we explore different neural network architect\\n',\n",
       " ' A Neural Transducer ; Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitabl\\n',\n",
       " ' Skip-Thought Memory Networks ; Question Answering (QA) is fundamental to natural language processing in that most nlp problems c\\n',\n",
       " ' Named Entity Recognition with Bidirectional LSTM-CNNs ; Named entity recognition is a challenging task that has traditionally re\\n',\n",
       " ' Generating News Headlines with Recurrent Neural Networks ; We describe an application of an encoder-decoder recurrent neural net\\n',\n",
       " ' Words are not Equal: Graded Weighting Model for building Composite   Document Vectors ; Despite the success of distributional se\\n',\n",
       " ' Small-footprint Deep Neural Networks with Highway Connections for Speech   Recognition ; For speech recognition, deep neural net\\n',\n",
       " ' Backward and Forward Language Modeling for Constrained Sentence   Generation ; Recent language models, especially those based on\\n',\n",
       " ' Online Keyword Spotting with a Character-Level Recurrent Neural Network ; In this paper, we propose a context-aware keyword spot\\n',\n",
       " ' Domain Specific Author Attribution Based on Feedforward Neural Network   Language Models ; Authorship attribution refers to the \\n',\n",
       " ' Segmental Recurrent Neural Networks for End-to-end Speech Recognition ; We study the segmental recurrent neural network for end-\\n',\n",
       " ' How Transferable are Neural Networks in NLP Applications? ; Transfer learning is aimed to make use of valuable knowledge in a so\\n',\n",
       " ' Recurrent Neural Network Encoder with Attention for Community Question   Answering ; We apply a general recurrent neural network\\n',\n",
       " ' Recursive Neural Language Architecture for Tag Prediction ; We consider the problem of learning distributed representations for \\n',\n",
       " ' On the Compression of Recurrent Neural Networks with an Application to   LVCSR acoustic modeling for Embedded Speech Recognition\\n',\n",
       " ' Pointing the Unknown Words ; The problem of rare and unknown words is an important issue that can potentially influence the perf\\n',\n",
       " ' Learning Multiscale Features Directly From Waveforms ; Deep learning has dramatically improved the performance of speech recogni\\n',\n",
       " ' Joint Learning of Sentence Embeddings for Relevance and Entailment ; We consider the problem of Recognizing Textual Entailment w\\n',\n",
       " ' Deep API Learning ; Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. \\n',\n",
       " ' Does Multimodality Help Human and Machine for Translation and Image   Captioning? ; This paper presents the systems developed by\\n',\n",
       " ' Very Deep Convolutional Networks for Text Classification ; The dominant approach for many NLP tasks are recurrent neural network\\n',\n",
       " ' Improving Recurrent Neural Networks For Sequence Labelling ; In this paper we study different types of Recurrent Neural Networks\\n',\n",
       " ' Sentence Similarity Measures for Fine-Grained Estimation of Topical   Relevance in Learner Essays ; We investigate the task of a\\n',\n",
       " ' Deep CNNs along the Time Axis with Intermap Pooling for Robustness to   Spectral Variations ; Convolutional neural networks (CNN\\n',\n",
       " ' Automatic Text Scoring Using Neural Networks ; Automated Text Scoring (ATS) provides a cost-effective and consistent alternative\\n',\n",
       " ' A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic   Modeling in Speech Recognition ; We present a comprehensive\\n',\n",
       " ' Sequence-Level Knowledge Distillation ; Neural machine translation (NMT) offers a novel alternative formulation of translation t\\n',\n",
       " ' Learning Semantically Coherent and Reusable Kernels in Convolution   Neural Nets for Sentence Classification ; The state-of-the-\\n',\n",
       " ' RETURNN: The RWTH Extensible Training framework for Universal Recurrent   Neural Networks ; In this work we release our extensib\\n',\n",
       " ' Character-Level Language Modeling with Hierarchical Recurrent Neural   Networks ; Recurrent neural network (RNN) based character\\n',\n",
       " ' Multi-task Recurrent Model for True Multilingual Speech Recognition ; Research on multilingual speech recognition remains attrac\\n',\n",
       " ' Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep   Recurrent models ; Sentiment Analysis (SA) is an acti\\n',\n",
       " ' Attending to Characters in Neural Sequence Labeling Models ; Sequence labeling architectures use word embeddings for capturing s\\n',\n",
       " ' Visualizing and Understanding Curriculum Learning for Long Short-Term   Memory Networks ; Curriculum Learning emphasizes the ord\\n',\n",
       " ' Dense Prediction on Sequences with Time-Dilated Convolutions for Speech   Recognition ; In computer vision pixelwise dense predi\\n',\n",
       " ' End-to-End ASR-free Keyword Search from Speech ; End-to-end (E2E) systems have achieved competitive results compared to conventi\\n',\n",
       " ' Training Language Models Using Target-Propagation ; While Truncated Back-Propagation through Time (BPTT) is the most popular app\\n',\n",
       " ' Deep Voice: Real-time Neural Text-to-Speech ; We present Deep Voice, a production-quality text-to-speech system constructed enti\\n',\n",
       " ' Improved Variational Autoencoders for Text Modeling using Dilated   Convolutions ; Recent work on generative modeling of text ha\\n',\n",
       " ' Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence   Labelling ; Most existing sequence labelling models r\\n',\n",
       " ' Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model) ; We examine Memory Networks for the task of question answerin\\n',\n",
       " ' Simplified End-to-End MMI Training and Voting for ASR ; A simplified speech recognition system that uses the maximum mutual info\\n',\n",
       " ' Learning to Generate Reviews and Discovering Sentiment ; We explore the properties of byte-level recurrent language models. When\\n',\n",
       " ' Semi-supervised Multitask Learning for Sequence Labeling ; We propose a sequence labeling framework with a secondary training ob\\n',\n",
       " ' Going Wider: Recurrent Neural Network With Parallel Cells ; Recurrent Neural Network (RNN) has been widely applied for sequence \\n',\n",
       " ' Phonetic Temporal Neural Model for Language Identification ; Deep neural models, particularly the LSTM-RNN model, have shown gre\\n',\n",
       " ' Relevance-based Word Embedding ; Learning a high-dimensional dense representation for vocabulary terms, also known as a word emb\\n',\n",
       " ' Deriving Neural Architectures from Sequence and Graph Kernels ; The design of neural architectures for structured objects is typ\\n',\n",
       " ' On Multilingual Training of Neural Dependency Parsers ; We show that a recently proposed neural dependency parser can be improve\\n',\n",
       " ' Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks ; Ladder networks are a notable new concept in the field of s\\n',\n",
       " ' Adversarially Regularized Autoencoders ; While autoencoders are a key technique in representation learning for continuous struct\\n',\n",
       " ' Auxiliary Objectives for Neural Error Detection Models ; We investigate the utility of different auxiliary objectives and traini\\n',\n",
       " ' An Error-Oriented Approach to Word Embedding Pre-Training ; We propose a novel word embedding pre-training approach that exploit\\n',\n",
       " ' A Continuous Relaxation of Beam Search for End-to-end Training of Neural   Sequence Models ; Beam search is a desirable choice o\\n',\n",
       " ' Regularizing and Optimizing LSTM Language Models ; Recurrent neural networks (RNNs), such as long short-term memory networks (LS\\n',\n",
       " ' Supervised Speech Separation Based on Deep Learning: An Overview ; Speech separation is the task of separating target speech fro\\n',\n",
       " ' Grasping the Finer Point: A Supervised Similarity Network for Metaphor   Detection ; The ubiquity of metaphor in our everyday co\\n',\n",
       " ' Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words ; Distributed word embeddings have shown superior perfo\\n',\n",
       " ' KeyVec: Key-semantics Preserving Document Representations ; Previous studies have demonstrated the empirical success of word emb\\n',\n",
       " ' Exploring Asymmetric Encoder-Decoder Structure for Context-based   Sentence Representation Learning ; Context information plays \\n',\n",
       " ' CNN Is All You Need ; The Convolution Neural Network (CNN) has demonstrated the unique advantage in audio, image and text learni\\n',\n",
       " ' Combining Representation Learning with Logic for Language Processing ; The current state-of-the-art in many natural language pro\\n',\n",
       " ' A Note on Topology Preservation in Classification, and the Construction   of a Universal Neuron Grid ; It will be shown that acc\\n',\n",
       " ' Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On   Boltzmann Machines ; One conjecture in both deep learni\\n',\n",
       " ' Mapping Temporal Variables into the NeuCube for Improved Pattern   Recognition, Predictive Modelling and Understanding of Stream\\n',\n",
       " ' An Evolutionary Algorithm to Learn SPARQL Queries for   Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia \\n',\n",
       " ' A Geometric Framework for Convolutional Neural Networks ; In this paper, a geometric framework for neural networks is proposed. \\n',\n",
       " ' A Novel Representation of Neural Networks ; Deep Neural Networks (DNNs) have become very popular for prediction in many areas. T\\n',\n",
       " ' Converting Cascade-Correlation Neural Nets into Probabilistic Generative   Models ; Humans are not only adept in recognizing wha\\n',\n",
       " ' On the Performance of Network Parallel Training in Artificial Neural   Networks ; Artificial Neural Networks (ANNs) have receive\\n',\n",
       " ' Programmable Agents ; We build deep RL agents that execute declarative programs expressed in formal language. The agents learn t\\n',\n",
       " ' Explainable Artificial Intelligence: Understanding, Visualizing and   Interpreting Deep Learning Models ; With the availability \\n',\n",
       " ' Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games? ; Deep reinforcement learning has achieved many recent succ\\n',\n",
       " ' Emergence of grid-like representations by training recurrent neural   networks to perform spatial localization ; Decades of rese\\n',\n",
       " ' Dimensionality Reduction and Reconstruction using Mirroring Neural   Networks and Object Recognition based on Reduced Dimension \\n',\n",
       " ' Parcellation of fMRI Datasets with ICA and PLS-A Data Driven Approach ; Inter-subject parcellation of functional Magnetic Resona\\n',\n",
       " ' Iris Codes Classification Using Discriminant and Witness Directions ; The main topic discussed in this paper is how to use intel\\n',\n",
       " ' Algorithms for Image Analysis and Combination of Pattern Classifiers   with Application to Medical Diagnosis ; Medical Informati\\n',\n",
       " ' Deep Neural Networks are Easily Fooled: High Confidence Predictions for   Unrecognizable Images ; Deep neural networks (DNNs) ha\\n',\n",
       " ' Homogeneous Spiking Neuromorphic System for Real-World Pattern   Recognition ; A neuromorphic chip that combines CMOS analog spi\\n',\n",
       " ' Crowd Behavior Analysis: A Review where Physics meets Biology ; Although the traits emerged in a mass gathering are often non-de\\n',\n",
       " ' Can Pretrained Neural Networks Detect Anatomy? ; Convolutional neural networks demonstrated outstanding empirical results in com\\n',\n",
       " ' Metaheuristic Algorithms for Convolution Neural Network ; A typical modern optimization technique is usually either heuristic or\\n',\n",
       " ' Hadamard Product for Low-rank Bilinear Pooling ; Bilinear models provide rich representations compared with linear models. They \\n',\n",
       " ' Incremental Network Quantization: Towards Lossless CNNs with   Low-Precision Weights ; This paper presents incremental network q\\n',\n",
       " ' LesionSeg: Semantic segmentation of skin lesions using Deep   Convolutional Neural Network ; We present a method for skin lesion\\n',\n",
       " ' Convolutional Spike Timing Dependent Plasticity based Feature Learning   in Spiking Neural Networks ; Brain-inspired learning mo\\n',\n",
       " ' Adversarial Transformation Networks: Learning to Generate Adversarial   Examples ; Multiple different approaches of generating a\\n',\n",
       " ' Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced   Attentive Response Approach for Explaining and Visual\\n',\n",
       " ' Fast YOLO: A Fast You Only Look Once System for Real-time Embedded   Object Detection in Video ; Object detection is considered \\n',\n",
       " ' NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm ; Neural networks (NNs) have begun to have a pervasive \\n',\n",
       " ' Analysis of supervised and semi-supervised GrowCut applied to   segmentation of masses in mammography images ; Breast cancer is \\n',\n",
       " ' Empirical Explorations in Training Networks with Discrete Activations ; We present extensive experiments training and testing hi\\n',\n",
       " ' Regularized Evolution for Image Classifier Architecture Search ; The effort devoted to hand-crafting image classifiers has motiv\\n',\n",
       " ' Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network   for Real-time Embedded Object Detection ; Object dete\\n',\n",
       " ' Inferencing Based on Unsupervised Learning of Disentangled   Representations ; Combining Generative Adversarial Networks (GANs) \\n',\n",
       " ' The Parameter-Less Self-Organizing Map algorithm ; The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network algori\\n',\n",
       " ' Simplified firefly algorithm for 2D image key-points search ; In order to identify an object, human eyes firstly search the fiel\\n',\n",
       " ' Deep-Plant: Plant Identification with convolutional neural networks ; This paper studies convolutional neural networks (CNN) to \\n',\n",
       " ' Adapting Deep Network Features to Capture Psychological Representations ; Deep neural networks have become increasingly successf\\n',\n",
       " ' Large-Scale Evolution of Image Classifiers ; Neural networks have proven effective at solving difficult problems but designing t\\n',\n",
       " ' A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification   and Domain Adaptation ; Recently, DNN model compression \\n',\n",
       " ' Identifying Spatial Relations in Images using Convolutional Neural   Networks ; Traditional approaches to building a large scale\\n',\n",
       " ' Hierarchical Attentive Recurrent Tracking ; Class-agnostic object tracking is particularly difficult in cluttered environments a\\n',\n",
       " ' PSIque: Next Sequence Prediction of Satellite Images using a   Convolutional Sequence-to-Sequence Network ; Predicting unseen we\\n',\n",
       " \" Evaluation of Alzheimer's Disease by Analysis of MR Images using   Multilayer Perceptrons and Kohonen SOM Classifiers as an Alte\\n\",\n",
       " ' Neural tuning size is a key factor underlying holistic face processing ; Faces are a class of visual stimuli with unique signifi\\n',\n",
       " ' Distribution of the search of evolutionary product unit neural networks   for classification ; This paper deals with the distrib\\n',\n",
       " ' Correlation Alignment for Unsupervised Domain Adaptation ; In this chapter, we present CORrelation ALignment (CORAL), a simple y\\n',\n",
       " \" CITlab ARGUS for historical handwritten documents ; We describe CITlab's recognition system for the HTRtS competition attached t\\n\",\n",
       " ' Generalized Haar Filter based Deep Networks for Real-Time Object   Detection in Traffic Scene ; Vision-based object detection is\\n',\n",
       " ' Autoencoder Regularized Network For Driving Style Representation   Learning ; In this paper, we study learning generalized drivi\\n',\n",
       " ' Fashioning with Networks: Neural Style Transfer to Design Clothes ; Convolutional Neural Networks have been highly successful in\\n',\n",
       " ' GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from   Remote Sensing Imagery ; Advances in remote sensing tech\\n',\n",
       " ' Improving Efficiency in Convolutional Neural Network with Multilinear   Filters ; The excellent performance of deep neural netwo\\n',\n",
       " ' Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided   Diagnosis of Diabetic Retinopathy ; Objective: Radiomics-drive\\n',\n",
       " ' HP-GAN: Probabilistic 3D human motion prediction via GAN ; Predicting and understanding human motion dynamics has many applicati\\n',\n",
       " ' Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro   Gesture ; In the research of the impact of gestures usin\\n',\n",
       " ' Nature vs. Nurture: The Role of Environmental Resources in Evolutionary   Deep Intelligence ; Evolutionary deep intelligence syn\\n',\n",
       " ' A stochastic model of human visual attention with a dynamic Bayesian   network ; Recent studies in the field of human vision sci\\n',\n",
       " ' Smart Content Recognition from Images Using a Mixture of Convolutional   Neural Networks ; With rapid development of the Interne\\n',\n",
       " ' Cortical spatio-temporal dimensionality reduction for visual grouping ; The visual systems of many mammals, including humans, is\\n',\n",
       " ' Visual Sentiment Prediction with Deep Convolutional Neural Networks ; Images have become one of the most popular types of media \\n',\n",
       " ' Correntropy Maximization via ADMM - Application to Robust Hyperspectral   Unmixing ; In hyperspectral images, some spectral band\\n',\n",
       " ' Identifying individual facial expressions by deconstructing a neural   network ; This paper focuses on the problem of explaining\\n',\n",
       " ' Object Boundary Detection and Classification with Image-level Labels ; Semantic boundary and edge detection aims at simultaneous\\n',\n",
       " ' Evolving Spatially Aggregated Features from Satellite Imagery for   Regional Modeling ; Satellite imagery and remote sensing pro\\n',\n",
       " ' Pillar Networks++: Distributed non-parametric deep and wide networks ; In recent work, it was shown that combining multi-kernel \\n',\n",
       " ' Market-Based Reinforcement Learning in Partially Observable Worlds ; Unlike traditional reinforcement learning (RL), market-base\\n',\n",
       " ' Controlled hierarchical filtering: Model of neocortical sensory   processing ; A model of sensory information processing is pres\\n',\n",
       " ' When Do Differences Matter? On-Line Feature Extraction Through Cognitive   Economy ; For an intelligent agent to be truly autono\\n',\n",
       " ' Applying Policy Iteration for Training Recurrent Neural Networks ; Recurrent neural networks are often used for learning time-se\\n',\n",
       " ' A Neural-Network Technique to Learn Concepts from Electroencephalograms ; A new technique is presented developed to learn multi-\\n',\n",
       " ' Empirical learning aided by weak domain knowledge in the form of feature   importance ; Standard hybrid learners that use domain\\n',\n",
       " ' Evolutionary Algorithms for Reinforcement Learning ; There are two distinct approaches to solving reinforcement learning problem\\n',\n",
       " ' On Training Deep Boltzmann Machines ; The deep Boltzmann machine (DBM) has been an important development in the quest for powerf\\n',\n",
       " ' Memristive fuzzy edge detector ; Fuzzy inference systems always suffer from the lack of efficient structures or platforms for th\\n',\n",
       " ' Echo State Queueing Network: a new reservoir computing learning tool ; In the last decade, a new computational paradigm was intr\\n',\n",
       " ' The Predictron: End-To-End Learning and Planning ; One of the key challenges of artificial intelligence is to learn models that \\n',\n",
       " ' Quadratically constrained quadratic programming for classification using   particle swarms and applications ; Particle swarm opt\\n',\n",
       " ' Learning to Execute ; Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are\\n',\n",
       " ' Bitwise Neural Networks ; Based on the assumption that there exists a neural network that efficiently represents a set of Boolea\\n',\n",
       " ' Graying the black box: Understanding DQNs ; In recent years there is a growing interest in using deep representations for reinfo\\n',\n",
       " ' Evaluation of a Tree-based Pipeline Optimization Tool for Automating   Data Science ; As the field of data science continues to \\n',\n",
       " ' Probabilistic Reasoning via Deep Learning: Neural Association Models ; In this paper, we propose a new deep learning approach, c\\n',\n",
       " ' Deep Reinforcement Learning With Macro-Actions ; Deep reinforcement learning has been shown to be a powerful framework for learn\\n',\n",
       " ' RETAIN: An Interpretable Predictive Model for Healthcare using Reverse   Time Attention Mechanism ; Accuracy and interpretabilit\\n',\n",
       " ' A High Speed Multi-label Classifier based on Extreme Learning Machines ; In this paper a high speed neural network classifier ba\\n',\n",
       " ' An Online Universal Classifier for Binary, Multi-class and Multi-label   Classification ; Classification involves the learning o\\n',\n",
       " ' Adaptive Online Sequential ELM for Concept Drift Tackling ; A machine learning method needs to adapt to over time changes in the\\n',\n",
       " ' Adaptive Convolutional ELM For Concept Drift Handling in Online Stream   Data ; In big data era, the data continuously generated\\n',\n",
       " ' Particle Swarm Optimization for Generating Interpretable Fuzzy   Reinforcement Learning Policies ; Fuzzy controllers are efficie\\n',\n",
       " ' A Growing Long-term Episodic & Semantic Memory ; The long-term memory of most connectionist systems lies entirely in the weights\\n',\n",
       " ' Cognitive Discriminative Mappings for Rapid Learning ; Humans can learn concepts or recognize items from just a handful of examp\\n',\n",
       " ' Towards a Mathematical Understanding of the Difficulty in Learning with   Feedforward Neural Networks ; Training deep neural net\\n',\n",
       " ' An effective algorithm for hyperparameter optimization of neural   networks ; A major challenge in designing neural network (NN)\\n',\n",
       " ' Evolutionary Training of Sparse Artificial Neural Networks: A Network   Science Perspective ; Through the success of deep learni\\n',\n",
       " ' Attend and Predict: Understanding Gene Regulation by Selective Attention   on Chromatin ; The past decade has seen a revolution \\n',\n",
       " ' Parallelizing Linear Recurrent Neural Nets Over Sequence Length ; Recurrent neural networks (RNNs) are widely used to model sequ\\n',\n",
       " ' Feature learning in feature-sample networks using multi-objective   optimization ; Data and knowledge representation are fundame\\n',\n",
       " ' Meta-Learning and Universality: Deep Representations and Gradient   Descent can Approximate any Learning Algorithm ; Learning to\\n',\n",
       " ' Hindsight policy gradients ; Goal-conditional policies allow reinforcement learning agents to pursue specific goals during diffe\\n',\n",
       " ' SquishedNets: Squishing SqueezeNet further for edge device scenarios via   deep evolutionary synthesis ; While deep neural netwo\\n',\n",
       " ' Autonomous development and learning in artificial intelligence and   robotics: Scaling up deep learning to human--like learning \\n',\n",
       " ' Learning from Scarce Experience ; Searching the space of policies directly for the optimal policy has been one popular method fo\\n',\n",
       " ' Fitness inheritance in the Bayesian optimization algorithm ; This paper describes how fitness inheritance can be used to estimat\\n',\n",
       " ' The Combined Technique for Detection of Artifacts in Clinical   Electroencephalograms of Sleeping Newborns ; In this paper we de\\n',\n",
       " ' Evolving Classifiers: Methods for Incremental Learning ; The ability of a classifier to take on new information and classes by e\\n',\n",
       " ' Automatic Pattern Classification by Unsupervised Learning Using   Dimensionality Reduction of Data with Mirroring Neural Network\\n',\n",
       " ' Improving the Performance of PieceWise Linear Separation Incremental   Algorithms for Practical Hardware Implementations ; In th\\n',\n",
       " ' A Novel Rough Set Reduct Algorithm for Medical Domain Based on Bee   Colony Optimization ; Feature selection refers to the probl\\n',\n",
       " ' Automated Query Learning with Wikipedia and Genetic Programming ; Most of the existing information retrieval systems are based o\\n',\n",
       " ' Scaling Up Estimation of Distribution Algorithms For Continuous   Optimization ; Since Estimation of Distribution Algorithms (ED\\n',\n",
       " ' Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA ; An automated technique has recently been proposed to tra\\n',\n",
       " ' Discrete Dynamical Genetic Programming in XCS ; A number of representation schemes have been presented for use within Learning C\\n',\n",
       " ' Fuzzy Dynamical Genetic Programming in XCSF ; A number of representation schemes have been presented for use within Learning Cla\\n',\n",
       " ' Learning-Based Procedural Content Generation ; Procedural content generation (PCG) has recently become one of the hottest topics\\n',\n",
       " ' Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in   the Othello League ; N-tuple networks have been successf\\n',\n",
       " ' Towards a Self-Organized Agent-Based Simulation Model for Exploration of   Human Synaptic Connections ; In this paper, the early\\n',\n",
       " ' Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural   Network ; The paper presents the electronic design and m\\n',\n",
       " ' Learning Bayesian Network Equivalence Classes with Ant Colony   Optimization ; Bayesian networks are a useful tool in the repres\\n',\n",
       " ' Probabilistic Neural Programs ; We present probabilistic neural programs, a framework for program induction that permits flexibl\\n',\n",
       " ' Cognitive Deep Machine Can Train Itself ; Machine learning is making substantial progress in diverse applications. The success i\\n',\n",
       " ' Summary - TerpreT: A Probabilistic Programming Language for Program   Induction ; We study machine learning formulations of indu\\n',\n",
       " ' Learning in the Machine: Random Backpropagation and the Deep Learning   Channel ; Random backpropagation (RBP) is a variant of t\\n',\n",
       " ' Highway and Residual Networks learn Unrolled Iterative Estimation ; The past year saw the introduction of new architectures such\\n',\n",
       " ' Deep neural heart rate variability analysis ; Despite of the pain and limited accuracy of blood tests for early recognition of c\\n',\n",
       " ' A neural network approach to ordinal regression ; Ordinal regression is an important type of learning, which has properties of b\\n',\n",
       " ' Computational Model of Music Sight Reading: A Reinforcement Learning   Approach ; Although the Music Sight Reading process has b\\n',\n",
       " ' Using Artificial Bee Colony Algorithm for MLP Training on Earthquake   Time Series Data Prediction ; Nowadays, computer scientis\\n',\n",
       " ' Multiple chaotic central pattern generators with learning for legged   locomotion and malfunction compensation ; An originally c\\n',\n",
       " ' Teaching Deep Convolutional Neural Networks to Play Go ; Mastering the game of Go has remained a long standing challenge to the \\n',\n",
       " ' Polyphonic Music Generation by Modeling Temporal Dependencies Using a   RNN-DBN ; In this paper, we propose a generic technique \\n',\n",
       " ' Massively Parallel Methods for Deep Reinforcement Learning ; We present the first massively distributed architecture for deep re\\n',\n",
       " ' A genetic algorithm for autonomous navigation in partially observable   domain ; The problem of autonomous navigation is one of \\n',\n",
       " ' Distributed Deep Q-Learning ; We propose a distributed deep learning model to successfully learn control policies directly from \\n',\n",
       " ' Lifted Relational Neural Networks ; We propose a method combining relational-logic representations with neural network learning.\\n',\n",
       " ' Giraffe: Using Deep Reinforcement Learning to Play Chess ; This report presents Giraffe, a chess engine that uses self-play to d\\n',\n",
       " ' Attention with Intention for a Neural Network Conversation Model ; In a conversation or a dialogue process, attention and intent\\n',\n",
       " ' Deep Reinforcement Learning in Parameterized Action Space ; Recent work has shown that deep neural networks are capable of appro\\n',\n",
       " ' MazeBase: A Sandbox for Learning from Games ; This paper introduces MazeBase: an environment for simple 2D games, designed as a \\n',\n",
       " ' On Learning to Think: Algorithmic Information Theory for Novel   Combinations of Reinforcement Learning Controllers and Recurren\\n',\n",
       " ' An Empirical Comparison of Neural Architectures for Reinforcement   Learning in Partially Observable Environments ; This paper e\\n',\n",
       " ' Predicting Clinical Events by Combining Static and Dynamic Information   Using Recurrent Neural Networks ; In clinical data sets\\n',\n",
       " ' Weight Normalization: A Simple Reparameterization to Accelerate Training   of Deep Neural Networks ; We present weight normaliza\\n',\n",
       " ' Bounded Rational Decision-Making in Feedforward Neural Networks ; Bounded rational decision-makers transform sensory input into \\n',\n",
       " ' Lie Access Neural Turing Machine ; Following the recent trend in explicit neural memory structures, we present a new design of a\\n',\n",
       " ' Towards Machine Intelligence ; There exists a theory of a single general-purpose learning algorithm which could explain the prin\\n',\n",
       " ' Dynamic Frame skip Deep Q Network ; Deep Reinforcement Learning methods have achieved state of the art performance in learning c\\n',\n",
       " ' Programming with a Differentiable Forth Interpreter ; Given that in practice training data is scarce for all but a small set of \\n',\n",
       " ' Generative Choreography using Deep Learning ; Recent advances in deep learning have enabled the extraction of high-level feature\\n',\n",
       " ' Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and   Knowledge ; We propose Logic Tensor Networks: a unifo\\n',\n",
       " ' Identifying and Harnessing the Building Blocks of Machine Learning   Pipelines for Sensible Initialization of a Data Science Aut\\n',\n",
       " ' Neuroevolution-Based Inverse Reinforcement Learning ; The problem of Learning from Demonstration is targeted at learning to perf\\n',\n",
       " ' TerpreT: A Probabilistic Programming Language for Program Induction ; We study machine learning formulations of inductive progra\\n',\n",
       " ' Multi-Label Classification Method Based on Extreme Learning Machines ; In this paper, an Extreme Learning Machine (ELM) based te\\n',\n",
       " ' A Novel Online Real-time Classifier for Multi-label Data Streams ; In this paper, a novel extreme learning machine based online \\n',\n",
       " ' A Novel Progressive Learning Technique for Multi-class Classification ; In this paper, a progressive learning technique for mult\\n',\n",
       " ' A novel online multi-label classifier for high-speed streaming data   applications ; In this paper, a high-speed online neural n\\n',\n",
       " ' Ternary Neural Networks for Resource-Efficient AI Applications ; The computation and storage requirements for Deep Neural Networ\\n',\n",
       " ' Fitted Learning: Models with Awareness of their Limits ; Though deep learning has pushed the boundaries of classification forwar\\n',\n",
       " ' Learning to learn with backpropagation of Hebbian plasticity ; Hebbian plasticity is a powerful principle that allows biological\\n',\n",
       " ' Learning by Stimulation Avoidance: A Principle to Control Spiking Neural   Networks Dynamics ; Learning based on networks of rea\\n',\n",
       " ' Surprisal-Driven Zoneout ; We propose a novel method of regularization for recurrent neural networks called suprisal-driven zone\\n',\n",
       " ' Neural Architecture Search with Reinforcement Learning ; Neural networks are powerful and flexible models that work well for man\\n',\n",
       " ' Emergence of foveal image sampling from learning to attend in visual   scenes ; We describe a neural attention model with a lear\\n',\n",
       " ' Long Timescale Credit Assignment in NeuralNetworks with External Memory ; Credit assignment in traditional recurrent neural netw\\n',\n",
       " ' Energy Saving Additive Neural Network ; In recent years, machine learning techniques based on neural networks for mobile computi\\n',\n",
       " ' Learning to Repeat: Fine Grained Action Repetition for Deep   Reinforcement Learning ; Reinforcement Learning algorithms can lea\\n',\n",
       " ' Survey of reasoning using Neural networks ; Reason and inference require process as well as memory skills by humans. Neural netw\\n',\n",
       " ' One-Shot Imitation Learning ; Imitation learning has been commonly applied to solve different tasks in isolation. This usually r\\n',\n",
       " ' Deep Learning for Explicitly Modeling Optimization Landscapes ; In all but the most trivial optimization problems, the structure\\n',\n",
       " ' Stochastic Neural Networks for Hierarchical Reinforcement Learning ; Deep reinforcement learning has achieved many impressive re\\n',\n",
       " ' Batch Reinforcement Learning on the Industrial Benchmark: First   Experiences ; The Particle Swarm Optimization Policy (PSO-P) h\\n',\n",
       " ' End-to-End Differentiable Proving ; We introduce neural networks for end-to-end differentiable proving of queries to knowledge b\\n',\n",
       " ' Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments ; We explore deep reinforcement learning methods for mul\\n',\n",
       " ' Getting deep recommenders fit: Bloom embeddings for sparse binary   input/output networks ; Recommendation algorithms that incor\\n',\n",
       " ' Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural   Network and Long-Term Evaluation ; Monte Carlo tree se\\n',\n",
       " ' Hindsight Experience Replay ; Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We pr\\n',\n",
       " ' Trial without Error: Towards Safe Reinforcement Learning via Human   Intervention ; AI systems are increasingly applied to compl\\n',\n",
       " ' Reverse Curriculum Generation for Reinforcement Learning ; Many relevant tasks require an agent to reach a certain state, or to \\n',\n",
       " ' Ideological Sublations: Resolution of Dialectic in Population-based   Optimization ; A population-based optimization algorithm w\\n',\n",
       " ' ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural   Projections ; Deep neural networks have become ubiquito\\n',\n",
       " ' A Flow Model of Neural Networks ; Based on a natural connection between ResNet and transport equation or its characteristic equa\\n',\n",
       " ' Multimodal Content Analysis for Effective Advertisements on YouTube ; The rapid advances in e-commerce and Web 2.0 technologies \\n',\n",
       " ' Overcoming Exploration in Reinforcement Learning with Demonstrations ; Exploration in environments with sparse rewards has been \\n',\n",
       " ' Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency   for Sequence Modeling ; Recurrent neural networks hav\\n',\n",
       " ' Scalable Recollections for Continual Lifelong Learning ; Given the recent success of Deep Learning applied to a variety of singl\\n',\n",
       " ' Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data ; The paper introduces the Hidden Tree Markov Network (H\\n',\n",
       " ' Hierarchical Actor-Critic ; The ability to learn at different resolutions in time may help overcome one of the main challenges i\\n',\n",
       " ' Proximodistal Exploration in Motor Learning as an Emergent Property of   Optimization ; To harness the complexity of their high-\\n',\n",
       " ' Null Dynamical State Models of Human Cognitive Dysfunction ; The hard problem in artificial intelligence asks how the shuffling \\n',\n",
       " \" Accelerating Deep Learning with Memcomputing ; Restricted Boltzmann machines (RBMs) and their extensions, called 'deep-belief ne\\n\",\n",
       " ' mvn2vec: Preservation and Collaboration in Multi-View Network Embedding ; Multi-view networks are ubiquitous in real-world appli\\n',\n",
       " ' Granger-causal Attentive Mixtures of Experts ; Several methods have recently been proposed to detect salient input features for \\n',\n",
       " ' Memorize or generalize? Searching for a compositional RNN in a haystack ; Neural networks are very powerful learning systems, bu\\n',\n",
       " ' Continual Reinforcement Learning with Complex Synapses ; Unlike humans, who are capable of continual learning over their lifetim\\n',\n",
       " ' Meta-Reinforcement Learning of Structured Exploration Strategies ; Exploration is a fundamental challenge in reinforcement learn\\n',\n",
       " ' Approximation Algorithms for Cascading Prediction Models ; We present an approximation algorithm that takes a pool of pre-traine\\n',\n",
       " ' Coloring black boxes: visualization of neural network decisions ; Neural networks are commonly regarded as black boxes performin\\n',\n",
       " ' Relational Neural Expectation Maximization: Unsupervised Discovery of   Objects and their Interactions ; Common-sense physical r\\n',\n",
       " ' A Bayesian Model for Activities Recommendation and Event Structure   Optimization Using Visitors Tracking ; In events that are c\\n',\n",
       " ' The Lottery Ticket Hypothesis: Training Pruned Neural Networks ; Recent work on neural network pruning indicates that, at traini\\n',\n",
       " ' Learning recurrent dynamics in spiking networks ; Spiking activity of neurons engaged in learning and performing a task show com\\n',\n",
       " ' Principal Graphs and Manifolds ; In many physical, statistical, biological and other investigations it is desirable to approxima\\n',\n",
       " ' Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint ; Deep Belief Networks (DBN) have been successfully appl\\n',\n",
       " ' Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary   Independent Stochastic Neurons ; In this paper, a simpl\\n',\n",
       " ' Locally Imposing Function for Generalized Constraint Neural Networks - A   Study on Equality Constraints ; This work is a furthe\\n',\n",
       " ' Evolution of Covariance Functions for Gaussian Process Regression using   Genetic Programming ; In this contribution we describe\\n',\n",
       " ' Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image   Statistics ; We present a theoretical analysis of Gaus\\n',\n",
       " ' Training Restricted Boltzmann Machine by Perturbation ; A new approach to maximum likelihood learning of discrete graphical mode\\n',\n",
       " ' Multilayer bootstrap networks ; Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from botto\\n',\n",
       " ' Invariant backpropagation: how to train a transformation-invariant   neural network ; In many classification problems a classifi\\n',\n",
       " ' Shared latent subspace modelling within Gaussian-Binary Restricted   Boltzmann Machines for NIST i-Vector Challenge 2014 ; This \\n',\n",
       " ' A Neural Transfer Function for a Smooth and Differentiable Transition   Between Additive and Multiplicative Interactions ; Exist\\n',\n",
       " ' A Probabilistic Framework for Deep Learning ; We develop a probabilistic framework for deep learning based on the Deep Rendering\\n',\n",
       " ' Neurogenesis Deep Learning ; Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable succe\\n',\n",
       " ' Deep learning for neuroimaging: a validation study ; Deep learning methods have recently made notable advances in the tasks of c\\n',\n",
       " ' Improving Deep Neural Networks with Probabilistic Maxout Units ; We present a probabilistic variant of the recently introduced m\\n',\n",
       " ' How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need? ; In numerous applicative contexts, data are too rich and\\n',\n",
       " ' Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures ; Model-based methods and deep neural networks have both bee\\n',\n",
       " ' Learning deep dynamical models from image pixels ; Modeling dynamical systems is important in many disciplines, e.g., control, r\\n',\n",
       " ' From neural PCA to deep unsupervised learning ; A network supporting deep unsupervised learning is presented. The network is an \\n',\n",
       " ' Qualitatively characterizing neural network optimization problems ; Training neural networks involves solving large-scale non-co\\n',\n",
       " ' Why does Deep Learning work? - A perspective from Group Theory ; Why does Deep Learning work? What representations does it captu\\n',\n",
       " ' ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient ; Stochastic gradient algorithms have been the main focus of la\\n',\n",
       " ' A Unified Perspective on Multi-Domain and Multi-Task Learning ; In this paper, we provide a new neural-network based perspective\\n',\n",
       " ' A Neural Network Anomaly Detector Using the Random Cluster Model ; The random cluster model is used to define an upper bound on \\n',\n",
       " ' A Group Theoretic Perspective on Unsupervised Deep Learning ; Why does Deep Learning work? What representations does it capture?\\n',\n",
       " ' A Generative Model for Deep Convolutional Learning ; A generative model is developed for deep (multi-layered) convolutional dict\\n',\n",
       " ' Knowledge Transfer Pre-training ; Pre-training is crucial for learning deep neural networks. Most of existing pre-training metho\\n',\n",
       " ' Stacked What-Where Auto-encoders ; We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integr\\n',\n",
       " ' Training recurrent networks online without backtracking ; We introduce the \"NoBackTrack\" algorithm to train the parameters of dy\\n',\n",
       " ' Deep clustering: Discriminative embeddings for segmentation and   separation ; We address the problem of acoustic source separat\\n',\n",
       " ' Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural   Networks ; Several popular graph embedding techniques f\\n',\n",
       " ' Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A   Systematic Study ; This paper presents Rudra, a parameter s\\n',\n",
       " ' Convolutional Networks on Graphs for Learning Molecular Fingerprints ; We introduce a convolutional neural network that operates\\n',\n",
       " ' Population-Contrastive-Divergence: Does Consistency help with RBM   training? ; Estimating the log-likelihood gradient with resp\\n',\n",
       " ' AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery ; Deep convolutional\\n',\n",
       " ' Distillation as a Defense to Adversarial Perturbations against Deep   Neural Networks ; Deep learning algorithms have been shown\\n',\n",
       " ' The Variational Gaussian Process ; Variational inference is a powerful tool for approximate inference, and it has been recently \\n',\n",
       " ' Partial Reinitialisation for Optimisers ; Heuristic optimisers which search for an optimal configuration of variables relative t\\n',\n",
       " ' Efficient Representation of Low-Dimensional Manifolds using Deep   Networks ; We consider the ability of deep neural networks to\\n',\n",
       " ' Enhanced perceptrons using contrastive biclusters ; Perceptrons are neuronal devices capable of fully discriminating linearly se\\n',\n",
       " ' Alternating optimization method based on nonnegative matrix   factorizations for deep neural networks ; The backpropagation algo\\n',\n",
       " ' Robust Large Margin Deep Neural Networks ; The generalization error of deep neural networks via their classification margin is s\\n',\n",
       " ' No bad local minima: Data independent training error guarantees for   multilayer neural networks ; We use smoothed analysis tech\\n',\n",
       " ' Learning Structured Sparsity in Deep Neural Networks ; High demand for computation resources severely hinders deployment of larg\\n',\n",
       " ' Depth-Width Tradeoffs in Approximating Natural Functions with Neural   Networks ; We provide several new depth-based separation \\n',\n",
       " ' Tensor Switching Networks ; We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes th\\n',\n",
       " ' Survey of Expressivity in Deep Neural Networks ; We survey results on neural network expressivity described in \"On the Expressiv\\n',\n",
       " ' Precise Recovery of Latent Vectors from Generative Adversarial Networks ; Generative adversarial networks (GANs) transform laten\\n',\n",
       " ' Predicting Surgery Duration with Neural Heteroscedastic Regression ; Scheduling surgeries is a challenging task due to the funda\\n',\n",
       " ' Depth Creates No Bad Local Minima ; In deep learning, \\\\textit{depth}, as well as \\\\textit{nonlinearity}, create non-convex loss s\\n',\n",
       " ' Deep Semi-Random Features for Nonlinear Function Approximation ; We propose semi-random features for nonlinear function approxim\\n',\n",
       " ' Curriculum Dropout ; Dropout is a very effective way of regularizing neural networks. Stochastically \"dropping out\" units with a\\n',\n",
       " ' The power of deeper networks for expressing natural functions ; It is well-known that neural networks are universal approximator\\n',\n",
       " ' Gradient Descent for Spiking Neural Networks ; Much of studies on neural computation are based on network models of static neuro\\n',\n",
       " ' Unsure When to Stop? Ask Your Semantic Neighbors ; In iterative supervised learning algorithms it is common to reach a point in \\n',\n",
       " ' Anomaly Detection on Graph Time Series ; In this paper, we use variational recurrent neural network to investigate the anomaly d\\n',\n",
       " ' A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and   Support Vector Machine (SVM) for Intrusion Detection in\\n',\n",
       " ' DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in   Neural Networks ; Deep neural networks have become wid\\n',\n",
       " ' A Method of Generating Random Weights and Biases in Feedforward Neural   Networks with Random Hidden Nodes ; Neural networks wit\\n',\n",
       " ' Rotational Unit of Memory ; The concepts of unitary evolution matrices and associative memory have boosted the field of Recurren\\n',\n",
       " ' Progressive Growing of GANs for Improved Quality, Stability, and   Variation ; We describe a new training methodology for genera\\n',\n",
       " ' Generative Adversarial Source Separation ; Generative source separation methods such as non-negative matrix factorization (NMF) \\n',\n",
       " ' A Supervised STDP-based Training Algorithm for Living Neural Networks ; Neural networks have shown great potential in many appli\\n',\n",
       " ' Improving Factor-Based Quantitative Investing by Forecasting Company   Fundamentals ; On a periodic basis, publicly traded compa\\n',\n",
       " ' Genetic Algorithms for Mentor-Assisted Evaluation Function Optimization ; In this paper we demonstrate how genetic algorithms ca\\n',\n",
       " ' Block Neural Network Avoids Catastrophic Forgetting When Learning   Multiple Task ; In the present work we propose a Deep Feed F\\n',\n",
       " ' A Scalable Deep Neural Network Architecture for Multi-Building and   Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinti\\n',\n",
       " ' Dynamic Boltzmann Machines for Second Order Moments and Generalized   Gaussian Distributions ; Dynamic Boltzmann Machine (DyBM) \\n',\n",
       " ' Multi-timescale memory dynamics in a reinforcement learning network with   attention-gated memory ; Learning and memory are inte\\n',\n",
       " ' Weighted Contrastive Divergence ; Learning algorithms for energy based Boltzmann architectures that rely on gradient descent are\\n',\n",
       " ' Dynamic Optimization of Neural Network Structures Using Probabilistic   Modeling ; Deep neural networks (DNNs) are powerful mach\\n',\n",
       " ' Pruning Techniques for Mixed Ensembles of Genetic Programming Models ; The objective of this paper is to define an effective str\\n',\n",
       " ' Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines ; This paper introduces the Metric-Free Natural Gradient (\\n',\n",
       " ' Stochastic Pooling for Regularization of Deep Convolutional Neural   Networks ; We introduce a simple and effective method for r\\n',\n",
       " ' Training Neural Networks with Stochastic Hessian-Free Optimization ; Hessian-free (HF) optimization has been successfully used f\\n',\n",
       " ' Reversible Jump MCMC Simulated Annealing for Neural Networks ; We propose a novel reversible jump Markov chain Monte Carlo (MCMC\\n',\n",
       " ' Predicting Parameters in Deep Learning ; We demonstrate that there is significant redundancy in the parameterization of several \\n',\n",
       " ' Disentangling Factors of Variation via Generative Entangling ; Here we propose a novel model family with the objective of learni\\n',\n",
       " ' Neural Networks for Complex Data ; Artificial neural networks are simple and efficient machine learning tools. Defined originall\\n',\n",
       " ' Multi-task Neural Networks for QSAR Predictions ; Although artificial neural networks have occasionally been used for Quantitati\\n',\n",
       " ' A Hybrid Latent Variable Neural Network Model for Item Recommendation ; Collaborative filtering is used to recommend items to a \\n',\n",
       " ' Techniques for Learning Binary Stochastic Feedforward Neural Networks ; Stochastic binary hidden units in a multi-layer perceptr\\n',\n",
       " ' Learning ELM network weights using linear discriminant analysis ; We present an alternative to the pseudo-inverse method for det\\n',\n",
       " ' Exponentially Increasing the Capacity-to-Computation Ratio for   Conditional Computation in Deep Learning ; Many state-of-the-ar\\n',\n",
       " ' Soft-Deep Boltzmann Machines ; We present a layered Boltzmann machine (BM) that can better exploit the advantages of a distribut\\n',\n",
       " ' Domain-Adversarial Training of Neural Networks ; We introduce a new representation learning approach for domain adaptation, in w\\n',\n",
       " ' Deep Online Convex Optimization with Gated Games ; Methods from convex optimization are widely used as building blocks for deep \\n',\n",
       " ' Churn analysis using deep convolutional neural networks and autoencoders ; Customer temporal behavioral data was represented as \\n',\n",
       " ' Developing an ICU scoring system with interaction terms using a genetic   algorithm ; ICU mortality scoring systems attempt to p\\n',\n",
       " ' Scale Normalization ; One of the difficulties of training deep neural networks is caused by improper scaling between layers. Sca\\n',\n",
       " ' Layer-wise learning of deep generative models ; When using deep, multi-layered architectures to build generative models of data,\\n',\n",
       " ' Distributed optimization of deeply nested systems ; In science and engineering, intelligent processing of complex signals such a\\n',\n",
       " ' Understanding Boltzmann Machine and Deep Learning via A Confident   Information First Principle ; Typical dimensionality reducti\\n',\n",
       " ' Canonical dual solutions to nonconvex radial basis neural network   optimization problem ; Radial Basis Functions Neural Network\\n',\n",
       " ' On the Number of Linear Regions of Deep Neural Networks ; We study the complexity of functions computable by deep feedforward ne\\n',\n",
       " ' Geometry and Expressive Power of Conditional Restricted Boltzmann   Machines ; Conditional restricted Boltzmann machines are und\\n',\n",
       " ' Is Joint Training Better for Deep Auto-Encoders? ; Traditionally, when generative models of data are developed via deep architec\\n',\n",
       " ' Massively Multitask Networks for Drug Discovery ; Massively multitask neural architectures provide a learning framework for drug\\n',\n",
       " ' Gated Feedback Recurrent Neural Networks ; In this work, we propose a novel recurrent neural network (RNN) architecture. The pro\\n',\n",
       " ' Deep Learning with Limited Numerical Precision ; Training of large-scale deep neural networks is often constrained by the availa\\n',\n",
       " ' MADE: Masked Autoencoder for Distribution Estimation ; There has been a lot of recent interest in designing neural network model\\n',\n",
       " ' Simple, Efficient, and Neural Algorithms for Sparse Coding ; Sparse coding is a basic task in many fields including signal proce\\n',\n",
       " ' Toxicity Prediction using Deep Learning ; Everyday we are exposed to various chemicals via food additives, cleaning and cosmetic\\n',\n",
       " ' To Drop or Not to Drop: Robustness, Consistency and Differential Privacy   Properties of Dropout ; Training deep belief networks\\n',\n",
       " ' Distilling the Knowledge in a Neural Network ; A very simple way to improve the performance of almost any machine learning algor\\n',\n",
       " ' A mathematical motivation for complex-valued convolutional networks ; A complex-valued convolutional network (convnet) implement\\n',\n",
       " ' Optimizing Neural Networks with Kronecker-factored Approximate Curvature ; We propose an efficient method for approximating natu\\n',\n",
       " ' Unsupervised model compression for multilayer bootstrap networks ; Recently, multilayer bootstrap network (MBN) has demonstrated\\n',\n",
       " ' Positive blood culture detection in time series data using a BiLSTM   network ; The presence of bacteria or fungi in the bloodst\\n',\n",
       " ' Known Unknowns: Uncertainty Quality in Bayesian Neural Networks ; We evaluate the uncertainty quality in neural networks using a\\n',\n",
       " ' Semi-Supervised Learning with the Deep Rendering Mixture Model ; Semi-supervised learning algorithms reduce the high cost of acq\\n',\n",
       " ' Self-calibrating Neural Networks for Dimensionality Reduction ; Recently, a novel family of biologically plausible online algori\\n',\n",
       " ' Tunable Efficient Unitary Neural Networks (EUNN) and their application   to RNNs ; Using unitary (instead of general) matrices i\\n',\n",
       " ' Sequence Transduction with Recurrent Neural Networks ; Many machine learning tasks can be expressed as the transformation---or \\\\\\n',\n",
       " ' On Fast Dropout and its Applicability to Recurrent Networks ; Recurrent Neural Networks (RNNs) are rich models for the processin\\n',\n",
       " ' Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks ; In this paper we propose and investigate a novel nonli\\n',\n",
       " ' Missing Value Imputation With Unsupervised Backpropagation ; Many data mining and data analysis techniques operate on dense matr\\n',\n",
       " ' Stochastic Gradient Estimate Variance in Contrastive Divergence and   Persistent Contrastive Divergence ; Contrastive Divergence\\n',\n",
       " ' How to Construct Deep Recurrent Neural Networks ; In this paper, we explore different ways to extend a recurrent neural network \\n',\n",
       " ' Neuronal Synchrony in Complex-Valued Deep Networks ; Deep learning has recently led to great successes in tasks such as image re\\n',\n",
       " ' An empirical analysis of dropout in piecewise linear networks ; The recently introduced dropout training criterion for neural ne\\n',\n",
       " ' An Empirical Investigation of Catastrophic Forgetting in Gradient-Based   Neural Networks ; Catastrophic forgetting is a problem\\n',\n",
       " ' Unsupervised Domain Adaptation by Backpropagation ; Top-performing deep architectures are trained on massive amounts of labeled \\n',\n",
       " ' Deep Directed Generative Autoencoders ; For discrete data, the likelihood $P(x)$ can be rewritten exactly and parametrized into \\n',\n",
       " ' An exact mapping between the Variational Renormalization Group and Deep   Learning ; Deep learning is a broad set of techniques \\n',\n",
       " ' Non-parametric Bayesian Learning with Deep Learning Structure and Its   Applications in Wireless Networks ; In this paper, we pr\\n',\n",
       " ' Parallel training of DNNs with Natural Gradient and Parameter Averaging ; We describe the neural-network training framework used\\n',\n",
       " ' End-to-end Continuous Speech Recognition using Attention-based Recurrent   NN: First Results ; We replace the Hidden Markov Mode\\n',\n",
       " ' Provable Methods for Training Neural Networks with Sparse Connectivity ; We provide novel guaranteed approaches for training fee\\n',\n",
       " ' Domain-Adversarial Neural Networks ; We introduce a new representation learning algorithm suited to the context of domain adapta\\n',\n",
       " ' Learning with Pseudo-Ensembles ; We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models \\n',\n",
       " ' Random Walk Initialization for Training Very Deep Feedforward Networks ; Training very deep networks is an important open proble\\n',\n",
       " ' Variational Recurrent Auto-Encoders ; In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variati\\n',\n",
       " ' Neural Network Regularization via Robust Weight Factorization ; Regularization is essential when training large neural networks.\\n',\n",
       " ' A Bayesian encourages dropout ; Dropout is one of the key techniques to prevent the learning from overfitting. It is explained t\\n',\n",
       " ' Deep Fried Convnets ; The fully connected layers of a deep convolutional neural network typically contain over 90% of the networ\\n',\n",
       " ' Lateral Connections in Denoising Autoencoders Support Supervised   Learning ; We show how a deep denoising autoencoder with late\\n',\n",
       " ' Deep Neural Networks with Random Gaussian Weights: A Universal   Classification Strategy? ; Three important properties of a clas\\n',\n",
       " ' Imaging Time-Series to Improve Classification and Imputation ; Inspired by recent successes of deep learning in computer vision,\\n',\n",
       " ' Blocks and Fuel: Frameworks for deep learning ; We introduce two Python frameworks to train neural networks on large datasets: B\\n',\n",
       " ' Adaptive Normalized Risk-Averting Training For Deep Neural Networks ; This paper proposes a set of new error criteria and learni\\n',\n",
       " ' Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer   Free Energy ; Restricted Boltzmann machines are undire\\n',\n",
       " ' Pointer Networks ; We introduce a new neural architecture to learn the conditional probability of an output sequence with elemen\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3df85ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_size = 16\n",
    "emb = Embedding(len(token_to_id), emb_size)\n",
    "emb.set_optimizer(ADAM(learning_rate=0.01))\n",
    "\n",
    "lstm_unit = LSTM(\n",
    "    n_input=emb_size,\n",
    "    n_hidden=64,\n",
    "    n_output=len(token_to_id),\n",
    "    bptt_trunc=MAX_LENGTH\n",
    ")\n",
    "lstm_unit.set_optimizer(ADAM(learning_rate=0.01))\n",
    "\n",
    "batch_size = 64\n",
    "history = []\n",
    "for i in range(1000):\n",
    "    batch_ix = to_matrix(np.random.choice(lines, size=batch_size, replace=False), token_to_id, max_len=MAX_LENGTH)\n",
    "    \n",
    "    encoded_data = emb(batch_ix)\n",
    "    \n",
    "    pred = lstm_unit(encoded_data)\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(batch_ix.shape[1]-1):\n",
    "        loss += cross_entropy_loss(batch_ix[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "        \n",
    "    errors = np.zeros(shape=(batch_size, MAX_LENGTH-1, len(token_to_id)))\n",
    "    for t in range(errors.shape[1]-1):\n",
    "        errors[:, t, :] = cross_entropy_loss_derivative(batch_ix[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "    \n",
    "    err = lstm_unit.backward(errors)\n",
    "    emb.backward(err)\n",
    "    \n",
    "    # visualizing training process\n",
    "    history.append(loss / batch_size)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aab9b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0727101786846394"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c9e164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    phrase = copy.copy(seed_phrase)\n",
    "    \n",
    "    for t in range(len(seed_phrase)-1, max_length-len(seed_phrase)):\n",
    "        x_sequence = to_matrix([phrase], token_to_id, max_len=max_length)\n",
    "        encoded_data = emb(x_sequence)\n",
    "\n",
    "        pred = char_rnn(encoded_data)\n",
    "        probs = softmax(pred[:, t] / temperature).ravel()\n",
    "        next_ix = np.random.choice(len(tokens), p=probs)\n",
    "        phrase += tokens[next_ix]\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a17cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learning for Deep many with a searning the word to deep recognition for and scale the-an article in the an action and Sensitive C\n",
      " On for Semantic   Representation ; 3D networks and intructation of a parameter Operal Sparial the application ; This paper are fo\n",
      " Images ; We propose a novel optimal Semantic Transoge statistical Markong Deep Learning ; We conversion and Processes ; The lavel\n",
      " The an and Exploration ; A Unle a consimel sementation ; In this paper many Converming the interves for a new in models ; In this\n",
      " A Colling Detection ; This paper is the problem ; We present ; The an and the proposed Systems ; We present Fultionaly and Time a\n",
      " Learning to deep sequention for Analysation ; This paper is the a new an architecture problem of Time Diption and an action with \n",
      " System ; The discrition of the information ; A class is a paper model in the and Blation ; The present Transed classification ; W\n",
      " Recognition ; In conscoge and Recognition ; The computer search ; We propose a sparses ; In this paper for Learning ; In this pap\n",
      " Samentation of State of inference reserent of Sent of Data ; We propose a new the the problem of semand and Sparms ; This paper d\n",
      " Manguage of combination of the Production ; Automatich for combines: a serent the inferent lingual Networks ; This paper deep Net\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(lstm_unit, seed_phrase=' ', temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b6bf58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 33,  81, 101,  60,   7,  37,  24,  37,   7,   3,   6,  81,  45,\n",
       "         6,  26,  84,  60,   6,  37,   7,  46,  65,  24,   3, 117,  24,\n",
       "        46,  60,   6,  37,   7,  20,  60,   7,   6,  37,  11,   6, 117,\n",
       "        46,  81, 119,  20, 117,  37, 117,  24,  46,  81,   6,  20,   6,\n",
       "        15,  37, 117,  84,  46,  81,  24,  69,   6,   6,  37,  81,  65,\n",
       "         6,  37,  81,  46,  60,  29, 117,   6,  33,  60,  37, 117,  24,\n",
       "         6,  46,  60, 117, 113,  46,   7,  51, 101,   6,  26,  81,   6,\n",
       "       117,  60,   3,  24,  37,  20,   6,  37,  37, 117,  24,   3,  81,\n",
       "        26,   6,  53,  84,  60,   6,  60,   6,  37,  81,   6,  37,   6,\n",
       "        81,  60,  81,  45,  60,   6,  37,  81,  37,  12, 101, 117,   6],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_unit(encoded_data)[1].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efd3604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([122,  81,  65,  60,   7, 101, 117,  37,  81,  65,  24,  81,  45,\n",
       "         6, 117,  84,  60,   6,  73,   7,  60,  65,  24,   3, 117,  24,\n",
       "        36,  60,   6,  73,  46, 113,  60,   7,   6,  46,  11,   6, 128,\n",
       "        46,  29, 119,  12, 117,  37, 117,  24,  46,  81,  37,  20,   6,\n",
       "       124,  60,   3,  84,  37,  81,  24,   3, 101,   6,  37,  81,  65,\n",
       "         6, 127,   3,  84,  46,   6,   6,   6,  49, 117,  37, 117,  60,\n",
       "         6,  15,  60, 117, 113,  46,   7,  51, 101,   6,  24,  81,   6,\n",
       "        49,  46,   3,  24,  37,  20,   6, 124,  60,  65,  24,  37,   6,\n",
       "        26,   6,   0,  84,  60,   7,  60,   6,  24, 101,   6,  37,   6,\n",
       "        20,  37,   7,  45,  60,   6,  37,  29,  46,  12,  81,  97])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ix[1, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e074c3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div #notebook {\n",
       "    background-color: #FFF9EE;\n",
       "    margin: auto;\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "    padding: 15px;\n",
       "    background-color: #FFFAFA;\n",
       "    min-height: 0;\n",
       "    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width to about 80 chars */\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "\n",
       "div.cell.border-box-sizing.code_cell.running { \n",
       "    border: 3px solid #111;\n",
       "}\n",
       "\n",
       "div.cell.code_cell {\n",
       "    background-color: #FFFAFA ;\n",
       "    border-radius: 5px;\n",
       "    padding: 1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Times New Roman';\n",
       "    color: #B8860B\n",
       "}\n",
       "\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 40pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 700;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 25pt;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: italic;\n",
       "    color: #8B4513;\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 20pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 16pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 10pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render p {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 15pt;\n",
       "    color: black;\n",
       "    text-align: justify;\n",
       "    text-justify: inter-word;\n",
       "    line-height: 1.5;\n",
       "}\n",
       "\n",
       "mark {\n",
       "  background: #D5EAFF;\n",
       "  color: black;\n",
       "}\n",
       "\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:2000px;  /* your desired max-height here */\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./style.css') as f:\n",
    "    style = f.read()\n",
    "HTML(style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_39",
   "language": "python",
   "name": "data_science_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
