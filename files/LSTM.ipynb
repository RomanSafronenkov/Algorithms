{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbda81a",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16265124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union\n",
    "from IPython.display import HTML, clear_output\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4455dfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Ababko', ' Abaev', ' Abagyan', ' Abaidulin', ' Abaidullin']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/russian_names.txt') as input_file:\n",
    "    names = input_file.read()[:-1].split('\\n')\n",
    "    names = [' ' + line for line in names]\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54e6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens =  53\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(''.join(names)))\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print('num_tokens = ', num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b1e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'g': 0,\n",
       " 'd': 1,\n",
       " 't': 2,\n",
       " 'k': 3,\n",
       " 'f': 4,\n",
       " 'T': 5,\n",
       " '-': 6,\n",
       " 'P': 7,\n",
       " 'r': 8,\n",
       " 'J': 9,\n",
       " 'a': 10,\n",
       " 'A': 11,\n",
       " 'h': 12,\n",
       " 's': 13,\n",
       " 'S': 14,\n",
       " 'N': 15,\n",
       " 'm': 16,\n",
       " 'v': 17,\n",
       " 'C': 18,\n",
       " 'D': 19,\n",
       " 'o': 20,\n",
       " 'b': 21,\n",
       " 'V': 22,\n",
       " 'u': 23,\n",
       " 'j': 24,\n",
       " 'c': 25,\n",
       " 'L': 26,\n",
       " 'i': 27,\n",
       " 'E': 28,\n",
       " 'G': 29,\n",
       " 'z': 30,\n",
       " '\\xa0': 31,\n",
       " 'w': 32,\n",
       " 'В': 33,\n",
       " 'M': 34,\n",
       " 'I': 35,\n",
       " 'n': 36,\n",
       " \"'\": 37,\n",
       " 'O': 38,\n",
       " 'R': 39,\n",
       " 'l': 40,\n",
       " 'Z': 41,\n",
       " 'K': 42,\n",
       " ',': 43,\n",
       " 'H': 44,\n",
       " 'y': 45,\n",
       " 'U': 46,\n",
       " 'Y': 47,\n",
       " 'B': 48,\n",
       " 'F': 49,\n",
       " 'e': 50,\n",
       " ' ': 51,\n",
       " 'p': 52}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185191cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems alright!\n"
     ]
    }
   ],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e75a93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first=True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, data))\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[c] for c in data[i]]\n",
    "        data_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        data_ix = np.transpose(data_ix)\n",
    "\n",
    "    return data_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537348a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ababko\n",
      " Chihachev\n",
      " Isaikov\n",
      " Nakhamkin\n",
      " Ustenko\n",
      "[[51 11 21 10 21  3 20 51 51 51]\n",
      " [51 18 12 27 12 10 25 12 50 17]\n",
      " [51 35 13 10 27  3 20 17 51 51]\n",
      " [51 15 10  3 12 10 16  3 27 36]\n",
      " [51 46 13  2 50 36  3 20 51 51]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(names[::2000]))\n",
    "print(to_matrix(names[::2000], token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfba1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, names))\n",
    "sample = to_matrix(np.random.choice(names, size=5), token_to_id, max_len=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6fa5bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 21, 53)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data = np.zeros(shape=(5, MAX_LENGTH, len(token_to_id)))\n",
    "\n",
    "for text_i, text in enumerate(encoded_data):\n",
    "    for letter_i, letter in enumerate(text):\n",
    "        encoded_data[text_i, letter_i, sample[text_i, letter_i]] = 1\n",
    "\n",
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d61fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        return self.forward(x, grad)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(BaseLayer):\n",
    "    \"\"\"\n",
    "    Linear class permorms ordinary FC layer in neural networks\n",
    "    Parameters:\n",
    "    n_input - size of input neurons\n",
    "    n_output - size of output neurons\n",
    "    Methods:\n",
    "    set_optimizer(optimizer) - is used for setting an optimizer for gradient descent\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input: int, n_output: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.w = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(n_input, n_output))\n",
    "        self.b = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(1, n_output))\n",
    "\n",
    "        self.w_optimizer = None\n",
    "        self.b_optimizer = None\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.w_optimizer = copy.copy(optimizer)\n",
    "        self.b_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.w_optimizer.set_weight(self.w)\n",
    "        self.b_optimizer.set_weight(self.b)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return x.dot(self.w) + self.b\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        assert self.w_optimizer is not None and self.b_optimizer is not None, 'You should set an optimizer'\n",
    "        w_grad = self.input.T.dot(output_error)\n",
    "        b_grad = np.ones((1, len(output_error))).dot(output_error)\n",
    "        input_error = output_error.dot(self.w.T)\n",
    "\n",
    "        self.w = self.w_optimizer.step(w_grad)\n",
    "        self.b = self.b_optimizer.step(b_grad)\n",
    "        return input_error\n",
    "\n",
    "\n",
    "class Activation(BaseLayer):\n",
    "    \"\"\"\n",
    "    Activation class is used for activation function of the FC layer\n",
    "    Params:\n",
    "    activation_function - activation function (e.g. sigmoid, RElU, tanh)\n",
    "    activation_derivative - derivative of the activation function\n",
    "    Methods:\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_function: callable, activation_derivative: callable) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.activation = activation_function\n",
    "        self.derivative = activation_derivative\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return self.activation(x)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return output_error * self.derivative(self.input)\n",
    "\n",
    "class BaseOptimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "class ADAM(BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm.\n",
    "\n",
    "    learning_rate (float, optional) – learning rate (default: 1e-3)\n",
    "    beta1, beta2 (Tuple[float, float], optional) –\n",
    "    coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "    eps (float, optional) – term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8,\n",
    "                 learning_rate: float = 3e-4, weight_decay: float = 0) -> None:\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.EMA1 = None\n",
    "        self.EMA2 = None\n",
    "\n",
    "        self.weight = None\n",
    "\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        self.weight = weight.copy()\n",
    "        self.EMA1 = np.zeros(shape=self.weight.shape)\n",
    "        self.EMA2 = np.zeros(shape=self.weight.shape)\n",
    "\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        assert self.weight is not None, 'You should set the weight'\n",
    "        grad = grad.copy() + self.weight_decay * self.weight\n",
    "        self.EMA1 = (1 - self.beta1) * grad + self.beta1 * self.EMA1\n",
    "        self.EMA2 = (1 - self.beta2) * grad ** 2 + self.beta2 * self.EMA2\n",
    "        self.weight -= self.learning_rate * self.EMA1 / (np.sqrt(self.EMA2) + self.eps)\n",
    "\n",
    "        return self.weight.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6375dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Tanh function\n",
    "    \"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Tanh function derivative\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "def sigmoid(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z: Union[np.array, float, int, list]) -> Union[np.array, float]:\n",
    "    \"\"\"\n",
    "    Sigmoid function derivative\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(z: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \"\"\"\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1, 1)\n",
    "\n",
    "def cross_entropy_loss(y_true: np.array, a_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    arg = -a_pred[lenght_y, y_true.ravel()]\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1)\n",
    "    loss = np.sum(arg + np.log(sum_exp))\n",
    "    return loss / len(y_true)\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true: np.array, a_pred: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss derivative for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: np.array with shape [n_samples, n_classes] with CrossEntropyLoss derivatives for each weight\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1).reshape(-1, 1)\n",
    "    loss = np.exp(a_pred.copy()) / sum_exp\n",
    "    loss[lenght_y, y_true.ravel()] -= 1\n",
    "\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04065a",
   "metadata": {},
   "source": [
    "![](./images/lstm.png)\n",
    "![](./images/graph_meanings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "627b9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input,\n",
    "        n_hidden,\n",
    "        bptt_trunc=4\n",
    "    ):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.bptt_trunc = bptt_trunc\n",
    "        \n",
    "        self.forget_gate = Linear(n_input + n_hidden, n_hidden)\n",
    "        self.forget_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        self.input_layer_gate = Linear(n_input + n_hidden, n_hidden)\n",
    "        self.input_layer_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        self.candidate_gate = Linear(n_input + n_hidden, n_hidden)\n",
    "        self.candidate_gate_activation = Activation(tanh, tanh_derivative)\n",
    "        self.output_gate = Linear(n_input + n_hidden, n_hidden)\n",
    "        self.output_gate_activation = Activation(sigmoid, sigmoid_derivative)\n",
    "        self.output_to_logits = Linear(n_hidden, n_input)\n",
    "        \n",
    "        self.x_and_h = None\n",
    "        self.hidden_states = None\n",
    "        self.cell_states = None\n",
    "        \n",
    "        self.forget_inputs = None\n",
    "        self.forget_states = None\n",
    "        \n",
    "        self.input_inputs = None\n",
    "        self.input_states = None\n",
    "        \n",
    "        self.candidate_inputs = None\n",
    "        self.candidate_states = None\n",
    "        \n",
    "        self.output_input = None\n",
    "        self.output_states = None\n",
    "        \n",
    "        self.outputs = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.forget_gate.set_optimizer(optimizer)\n",
    "        self.input_layer_gate.set_optimizer(optimizer)\n",
    "        self.candidate_gate.set_optimizer(optimizer)\n",
    "        self.output_gate.set_optimizer(optimizer)\n",
    "        self.output_to_logits.set_optimizer(optimizer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        batch_size, timesteps, input_dim = x.shape\n",
    "        \n",
    "        self.x_and_h = np.zeros((batch_size, timesteps, self.n_input+self.n_hidden))\n",
    "        self.hidden_states = np.zeros((batch_size, timesteps+1, self.n_hidden))\n",
    "        self.cell_states = np.zeros((batch_size, timesteps+1, self.n_hidden))\n",
    "        \n",
    "        self.forget_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.forget_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.input_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.input_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.candidate_inputs = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.candidate_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.output_input = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        self.output_states = np.zeros((batch_size, timesteps, self.n_hidden))\n",
    "        \n",
    "        self.outputs = np.zeros((batch_size, timesteps, self.n_input))\n",
    "\n",
    "        self.hidden_states[:, -1] = np.zeros((batch_size, self.n_hidden))\n",
    "        self.cell_states[:, -1] = np.zeros((batch_size, self.n_hidden))\n",
    "        for t in range(timesteps):\n",
    "            # соединяем вход и прошлый h\n",
    "            self.x_and_h[:, t] = np.concatenate((x[:, t], self.hidden_states[:, t-1]), axis=1)\n",
    "            \n",
    "            # forget gate проход\n",
    "            self.forget_inputs[:, t] = self.forget_gate(self.x_and_h[:, t])\n",
    "            self.forget_states[:, t] = self.forget_gate_activation(self.forget_inputs[:, t])\n",
    "            \n",
    "            # выбор кандидатов для C\n",
    "            self.input_inputs[:, t] = self.input_layer_gate(self.x_and_h[:, t])\n",
    "            self.input_states[:, t] = self.input_layer_gate_activation(self.input_inputs[:, t])\n",
    "            \n",
    "            self.candidate_inputs[:, t] = self.candidate_gate(self.x_and_h[:, t])\n",
    "            self.candidate_states[:, t] = self.candidate_gate_activation(self.candidate_inputs[:, t])\n",
    "            \n",
    "            self.cell_states[:, t] = self.forget_states[:, t] * self.cell_states[:, t-1]\\\n",
    "            + self.input_states[:, t] * self.candidate_states[:, t]\n",
    "            \n",
    "            self.output_input[:, t] = self.output_gate(self.x_and_h[:, t])\n",
    "            self.output_states[:, t] = self.output_gate_activation(self.output_input[:, t])\n",
    "            \n",
    "            self.hidden_states[:, t] = self.output_states[:, t] * tanh(self.cell_states[:, t])\n",
    "            \n",
    "            # дополнительный слой, не указан на рисунке, для перевода состояния в логиты выхода\n",
    "            self.outputs[:, t] = self.output_to_logits(self.hidden_states[:, t])\n",
    "\n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, output_error):        \n",
    "        _, timesteps, _ = output_error.shape\n",
    "\n",
    "        input_error = np.zeros_like(output_error)\n",
    "        for t in np.arange(timesteps)[::-1]:\n",
    "            # в разные моменты времени у слоев был разный вход, необходимо искусственно его поменять\n",
    "            self.forget_gate.input = self.x_and_h[:, t]\n",
    "            self.forget_gate_activation.input = self.forget_inputs[:, t]\n",
    "            self.input_layer_gate.input = self.x_and_h[:, t]\n",
    "            self.input_layer_gate_activation.input = self.input_inputs[:, t]\n",
    "            self.candidate_gate.input = self.x_and_h[:, t]\n",
    "            self.candidate_gate_activation.input = self.candidate_inputs[:, t]\n",
    "            self.output_gate.input = self.x_and_h[:, t]\n",
    "            self.output_gate_activation.input = self.output_input[:, t]\n",
    "            self.output_to_logits.input = self.hidden_states[:, t]\n",
    "            \n",
    "            # проход по нижнему уровню\n",
    "            hidden_error = self.output_to_logits.backward(output_error[:, t])\n",
    "            # та, что идет вверх\n",
    "            cell_error = tanh_derivative(self.cell_states[:, t]) * self.output_states[:, t] * hidden_error\n",
    "            # ошибка идет и вниз и вверх\n",
    "            hidden_error = self.output_gate_activation.backward(hidden_error) * tanh(self.cell_states[:, t])\n",
    "            # та, что идет вниз\n",
    "            hidden_error = self.output_gate.backward(hidden_error)\n",
    "\n",
    "            \n",
    "            # идем по верху\n",
    "            hidden_candidate_error = self.candidate_gate_activation.backward(cell_error) * self.input_states[:, t]\n",
    "            hidden_candidate_error = self.candidate_gate.backward(hidden_candidate_error)\n",
    "            \n",
    "            hidden_inputs_error = self.input_layer_gate_activation.backward(cell_error) * self.candidate_states[:, t]\n",
    "            hidden_inputs_error = self.input_layer_gate.backward(hidden_inputs_error)\n",
    "            \n",
    "            hidden_forget_error = self.forget_gate_activation.backward(cell_error) * self.cell_states[:, t-1]\n",
    "            hidden_forget_error = self.forget_gate.backward(hidden_forget_error)\n",
    "            \n",
    "            # добавляются ошибки с мест копии\n",
    "            hidden_error += hidden_candidate_error\n",
    "            hidden_error += hidden_inputs_error\n",
    "            hidden_error += hidden_forget_error\n",
    "            \n",
    "            # ошибка, которая по времени уходит по низу назад\n",
    "            hidden_error = hidden_error[:, self.n_input:]\n",
    "            # ошибка, которая по времени уходит по верху назад\n",
    "            cell_error = cell_error * self.forget_states[:, t]\n",
    "            # ошибка входа\n",
    "            input_error[:, t] = hidden_error[:, :self.n_input]\n",
    "            \n",
    "            for t_ in np.arange(max(0, t - self.bptt_trunc), t)[::-1]:\n",
    "                # проход по времени\n",
    "                self.forget_gate.input = self.x_and_h[:, t_]\n",
    "                self.forget_gate_activation.input = self.forget_inputs[:, t_]\n",
    "                self.input_layer_gate.input = self.x_and_h[:, t_]\n",
    "                self.input_layer_gate_activation.input = self.input_inputs[:, t_]\n",
    "                self.candidate_gate.input = self.x_and_h[:, t_]\n",
    "                self.candidate_gate_activation.input = self.candidate_inputs[:, t_]\n",
    "                self.output_gate.input = self.x_and_h[:, t_]\n",
    "                self.output_gate_activation.input = self.output_input[:, t_]\n",
    "                \n",
    "                # та, что идет по верху\n",
    "                cell_error += tanh_derivative(self.cell_states[:, t_]) * self.output_states[:, t_]\\\n",
    "                * hidden_error\n",
    "                \n",
    "                hidden_error = self.output_gate_activation.backward(hidden_error) * tanh(self.cell_states[:, t_])\n",
    "                # та, что идет вниз\n",
    "                hidden_error = self.output_gate.backward(hidden_error)\n",
    "                \n",
    "                hidden_candidate_error = self.candidate_gate_activation.backward(cell_error) * self.input_states[:, t_]\n",
    "                hidden_candidate_error = self.candidate_gate.backward(hidden_candidate_error)\n",
    "\n",
    "                hidden_inputs_error = self.input_layer_gate_activation.backward(cell_error) * self.candidate_states[:, t_]\n",
    "                hidden_inputs_error = self.input_layer_gate.backward(hidden_inputs_error)\n",
    "\n",
    "                hidden_forget_error = self.forget_gate_activation.backward(cell_error) * self.cell_states[:, t_-1]\n",
    "                hidden_forget_error = self.forget_gate.backward(hidden_forget_error)\n",
    "                \n",
    "                # добавляются ошибки с мест копии\n",
    "                hidden_error += hidden_candidate_error\n",
    "                hidden_error += hidden_inputs_error\n",
    "                hidden_error += hidden_forget_error\n",
    "\n",
    "                # ошибка которая по времени уходит по низу назад\n",
    "                hidden_error = hidden_error[:, self.n_input:]\n",
    "                cell_error = cell_error * self.forget_states[:, t_]\n",
    "\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b6d16",
   "metadata": {},
   "source": [
    "![](./images/lstm_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39fce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_unit = LSTM(n_input=len(token_to_id),\n",
    "    n_hidden=64,\n",
    "    bptt_trunc=15)\n",
    "\n",
    "lstm_unit.set_optimizer(ADAM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e855c682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 21, 53)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d57e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 21, 53)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_unit.forward(encoded_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dafb612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01841832, -0.00299377, -0.01709173, ...,  0.0187551 ,\n",
       "         -0.01542174,  0.00738827],\n",
       "        [ 0.00610701,  0.0004487 ,  0.00323593, ...,  0.00307519,\n",
       "         -0.0059377 ,  0.00682948],\n",
       "        [ 0.00200931,  0.00614388, -0.0010309 , ..., -0.00786876,\n",
       "          0.00463106, -0.00482423],\n",
       "        ...,\n",
       "        [-0.00638979,  0.00315084, -0.00743177, ...,  0.00574639,\n",
       "          0.00707862,  0.00414502],\n",
       "        [-0.00145176, -0.00203039, -0.01251759, ...,  0.0107941 ,\n",
       "          0.00205527,  0.00915858],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.01045198,  0.00379643,  0.00421158, ..., -0.00409587,\n",
       "          0.0142586 ,  0.00746141],\n",
       "        [-0.0042876 ,  0.01150004,  0.00437068, ..., -0.0130914 ,\n",
       "          0.01422028, -0.00884061],\n",
       "        [ 0.01740552, -0.02266821, -0.0144504 , ...,  0.00951029,\n",
       "         -0.02038288,  0.00134361],\n",
       "        ...,\n",
       "        [-0.00645534,  0.00315477, -0.00743469, ...,  0.00588877,\n",
       "          0.00703757,  0.00420242],\n",
       "        [-0.00146692, -0.00204267, -0.01251271, ...,  0.01088547,\n",
       "          0.00203323,  0.00918661],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.00123696, -0.00592319,  0.0161516 , ..., -0.00104245,\n",
       "          0.00075182, -0.00450754],\n",
       "        [ 0.00027502,  0.00790023,  0.00079883, ..., -0.00864585,\n",
       "          0.00577879, -0.00486886],\n",
       "        [ 0.00659759,  0.00341055,  0.00209666, ..., -0.00284087,\n",
       "         -0.00574902,  0.00351541],\n",
       "        ...,\n",
       "        [-0.00644339,  0.00315915, -0.00742487, ...,  0.00588046,\n",
       "          0.00701898,  0.0041972 ],\n",
       "        [-0.00145757, -0.00203774, -0.01250548, ...,  0.01087918,\n",
       "          0.00202067,  0.00918149],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.01072769, -0.00450577, -0.00399002, ...,  0.00176496,\n",
       "         -0.00626149, -0.00637584],\n",
       "        [ 0.00311774, -0.00518544,  0.00162681, ...,  0.00338689,\n",
       "          0.00166149, -0.0016845 ],\n",
       "        [-0.01627862,  0.01496398,  0.01758497, ..., -0.01944823,\n",
       "          0.00668897, -0.00623159],\n",
       "        ...,\n",
       "        [-0.00643003,  0.00315667, -0.00741492, ...,  0.00586768,\n",
       "          0.0070196 ,  0.00418545],\n",
       "        [-0.00144896, -0.0020385 , -0.01249723, ...,  0.0108686 ,\n",
       "          0.00202227,  0.00917203],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.00123696, -0.00592319,  0.0161516 , ..., -0.00104245,\n",
       "          0.00075182, -0.00450754],\n",
       "        [ 0.00543835,  0.00056147,  0.0059408 , ...,  0.00256652,\n",
       "          0.00034823,  0.00041062],\n",
       "        [ 0.01050936, -0.01121163, -0.01589967, ...,  0.01586311,\n",
       "         -0.01273522,  0.00700955],\n",
       "        ...,\n",
       "        [-0.0064554 ,  0.00315759, -0.00743399, ...,  0.00586587,\n",
       "          0.00703609,  0.00419154],\n",
       "        [-0.00146977, -0.00203823, -0.01251391, ...,  0.01087098,\n",
       "          0.00203037,  0.00917978],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lstm_unit(encoded_data)\n",
    "\n",
    "loss = 0\n",
    "for t in range(sample.shape[1]-1):\n",
    "    loss += cross_entropy_loss(sample[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "\n",
    "errors = np.zeros(shape=(5, MAX_LENGTH-1, len(token_to_id)))\n",
    "for t in range(errors.shape[1]-1):\n",
    "    errors[:, t, :] = cross_entropy_loss_derivative(sample[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "\n",
    "lstm_unit.backward(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df85ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_unit = LSTM(\n",
    "    n_input=len(token_to_id),\n",
    "    n_hidden=64,\n",
    "    bptt_trunc=15\n",
    ")\n",
    "lstm_unit.set_optimizer(ADAM(learning_rate=0.0001))\n",
    "\n",
    "batch_size = 64\n",
    "history = []\n",
    "for i in range(1000):\n",
    "    batch_ix = to_matrix(np.random.choice(names, size=batch_size, replace=False), token_to_id, max_len=MAX_LENGTH)\n",
    "    \n",
    "    encoded_data = np.zeros(shape=(batch_size, MAX_LENGTH, len(token_to_id)))\n",
    "\n",
    "    for text_i, text in enumerate(encoded_data):\n",
    "        for letter_i, letter in enumerate(text):\n",
    "            encoded_data[text_i, letter_i, batch_ix[text_i, letter_i]] = 1\n",
    "    \n",
    "    pred = lstm_unit(encoded_data)\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(batch_ix.shape[1]-1):\n",
    "        loss += cross_entropy_loss(batch_ix[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "        \n",
    "    errors = np.zeros(shape=(batch_size, MAX_LENGTH-1, len(token_to_id)))\n",
    "    for t in range(errors.shape[1]-1):\n",
    "        errors[:, t, :] = cross_entropy_loss_derivative(batch_ix[:, t+1].reshape(-1, 1), pred[:, t, :])\n",
    "    \n",
    "    lstm_unit.backward(errors)\n",
    "    \n",
    "    # visualizing training process\n",
    "    history.append(loss / batch_size)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aab9b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2752717450505818"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c9e164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    phrase = copy.copy(seed_phrase)\n",
    "    \n",
    "    for t in range(len(seed_phrase)-1, max_length-len(seed_phrase)):\n",
    "        x_sequence = to_matrix([phrase], token_to_id, max_len=max_length)\n",
    "        encoded_data = np.zeros(shape=(1, max_length, len(token_to_id)))\n",
    "\n",
    "        for text_i, text in enumerate(encoded_data):\n",
    "            for letter_i, letter in enumerate(text):\n",
    "                encoded_data[text_i, letter_i, x_sequence[text_i, letter_i]] = 1\n",
    "\n",
    "        pred = char_rnn(encoded_data)\n",
    "        probs = softmax(pred[:, t] / temperature).ravel()\n",
    "        next_ix = np.random.choice(len(tokens), p=probs)\n",
    "        phrase += tokens[next_ix]\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a17cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Maryov              \n",
      " Shakhahin           \n",
      " Avarov              \n",
      " Hulanov             \n",
      " Agarin              \n",
      " Poarov              \n",
      " Abalusky            \n",
      " Abarokov            \n",
      " Dalenkov            \n",
      " Makhanev            \n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(lstm_unit, seed_phrase=' ', temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b6bf58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 12, 12,  8, 13, 36, 51, 13,  3, 17, 51, 51, 51, 51, 51, 51, 51,\n",
       "       51, 51, 51, 51], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_unit(encoded_data)[1].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efd3604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 25, 50,  2, 27, 36,  2, 13, 50, 17, 51, 51, 51, 51, 51, 51, 51,\n",
       "       51, 51, 51])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ix[1, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e074c3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div #notebook {\n",
       "    background-color: #FFF9EE;\n",
       "    margin: auto;\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "    padding: 15px;\n",
       "    background-color: #FFFAFA;\n",
       "    min-height: 0;\n",
       "    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width to about 80 chars */\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "\n",
       "div.cell.border-box-sizing.code_cell.running { \n",
       "    border: 3px solid #111;\n",
       "}\n",
       "\n",
       "div.cell.code_cell {\n",
       "    background-color: #FFFAFA ;\n",
       "    border-radius: 5px;\n",
       "    padding: 1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Times New Roman';\n",
       "    color: #B8860B\n",
       "}\n",
       "\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 40pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 700;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 25pt;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: italic;\n",
       "    color: #8B4513;\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 20pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 16pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 10pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render p {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 15pt;\n",
       "    color: black;\n",
       "    text-align: justify;\n",
       "    text-justify: inter-word;\n",
       "    line-height: 1.5;\n",
       "}\n",
       "\n",
       "mark {\n",
       "  background: #D5EAFF;\n",
       "  color: black;\n",
       "}\n",
       "\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:2000px;  /* your desired max-height here */\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./style.css') as f:\n",
    "    style = f.read()\n",
    "HTML(style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_39",
   "language": "python",
   "name": "data_science_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
