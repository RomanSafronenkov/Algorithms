{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75be7672",
   "metadata": {},
   "source": [
    "# CNN Kaggle MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a28423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import copy\n",
    "import time\n",
    "from typing import Union, Tuple\n",
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921d6d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './data/digit_data/'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    shutil.rmtree(data_path)\n",
    "    \n",
    "with ZipFile('./data/digit-recognizer.zip') as f:\n",
    "    f.extractall(data_path)\n",
    "    \n",
    "data = pd.read_csv('./data/digit_data/train.csv')\n",
    "test_data = pd.read_csv('./data/digit_data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/digit_data/sample_submission.csv')\n",
    "\n",
    "shutil.rmtree(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0302bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('label', axis=1)\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bc230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "x_train = x_train.values.reshape(-1, 1, 28, 28)\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "\n",
    "x_test = x_test.values.reshape(-1, 1, 28, 28)\n",
    "y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "x_train = x_train / 255 # нейросетям легче, если нормировать значения\n",
    "x_test =  x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a4984",
   "metadata": {},
   "source": [
    "# Полносвязные слои\n",
    "\n",
    "#### $z=y({c({q({l})})})$\n",
    "#### $q(l)={w_q}\\cdot{l}$\n",
    "#### ${\\frac {dz}{dw_q}}={\\frac {dz}{dy}}\\cdot{\\frac {dy}{dc}}\\cdot{\\frac {dc}{dq}}\\cdot{\\frac {dq}{dw_q}}={\\frac {dz}{dy}}\\cdot{\\frac {dy}{dc}}\\cdot{\\frac {dc}{dq}}\\cdot{l}$\n",
    "\n",
    "Это значит, что для нахождения градиента весов i-го слоя нас интересуют произхводные по входу каждого слоя от N до i-1 включительно, они будут накапливаться и попадать в i-й слой как output_error.\n",
    "\n",
    "Соответственно, для нахождения градиента весов i+1-го слоя, нам необходимо будет знать и производную по входу i-го слоя, так как для i+1-го слоя она будет являться частью накопленной output_error\n",
    "\n",
    "Таким образом, например, $c$ - тоже функция вида: ${c}={w_c}\\cdot{q}$, все, что нас интересует - это то, что она зависит от своего входа $q$, который дальше может зависеть от чего угодно, но на данном этапе, нам нужно посчитать производную ошибки от $w_c$\n",
    "\n",
    "Получим:\n",
    "#### ${\\frac {dz}{dw_с}}={\\frac {dz}{dy}}\\cdot{\\frac {dy}{dc}}\\cdot{\\frac {dc}{dw_c}}={\\frac {dz}{dy}}\\cdot{\\frac {dy}{dc}}\\cdot{q}$\n",
    "\n",
    "Как видно, для поиска градиента по весам слоя, необходимо найти в конечном итоге его производную по этим весам, но она домножается на накопленную ошибку по входам всех предыдущих слоев, а значит, чтобы передавать эту ошибку дальше, необходимо считать и производную по входу (таким образом производная ${\\frac {dc}{dq}}$ пригодилась при поиске градиента слоя $q$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd69091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        return self.forward(x, grad)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(BaseLayer):\n",
    "    \"\"\"\n",
    "    Linear class permorms ordinary FC layer in neural networks\n",
    "    Parameters:\n",
    "    n_input - size of input neurons\n",
    "    n_output - size of output neurons\n",
    "    Methods:\n",
    "    set_optimizer(optimizer) - is used for setting an optimizer for gradient descent\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input: int, n_output: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.w = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(n_input, n_output))\n",
    "        self.b = np.random.normal(scale=np.sqrt(2 / (n_input + n_output)), size=(1, n_output))\n",
    "\n",
    "        self.w_optimizer = None\n",
    "        self.b_optimizer = None\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.w_optimizer = copy.copy(optimizer)\n",
    "        self.b_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.w_optimizer.set_weight(self.w)\n",
    "        self.b_optimizer.set_weight(self.b)\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return x.dot(self.w) + self.b\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        assert self.w_optimizer is not None and self.b_optimizer is not None, 'You should set an optimizer'\n",
    "        w_grad = self.input.T.dot(output_error)\n",
    "        b_grad = np.ones((1, len(output_error))).dot(output_error)\n",
    "        input_error = output_error.dot(self.w.T)\n",
    "\n",
    "        self.w = self.w_optimizer.step(w_grad)\n",
    "        self.b = self.b_optimizer.step(b_grad)\n",
    "        return input_error\n",
    "\n",
    "\n",
    "class Activation(BaseLayer):\n",
    "    \"\"\"\n",
    "    Activation class is used for activation function of the FC layer\n",
    "    Params:\n",
    "    activation_function - activation function (e.g. sigmoid, RElU, tanh)\n",
    "    activation_derivative - derivative of the activation function\n",
    "    Methods:\n",
    "    forward(x) - performs forward pass of the layer\n",
    "    backward(output_error, learning_rate) - performs backward pass of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_function: callable, activation_derivative: callable) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "        self.activation = activation_function\n",
    "        self.derivative = activation_derivative\n",
    "\n",
    "    def forward(self, x: np.array, grad: bool = True) -> np.array:\n",
    "        self.input = x\n",
    "        return self.activation(x)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return output_error * self.derivative(self.input)\n",
    "    \n",
    "    \n",
    "class Ravel(BaseLayer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input_tensor: np.array, grad: bool = False) -> np.array:\n",
    "        length = len(input_tensor)\n",
    "        self.input = input_tensor\n",
    "        return input_tensor.reshape(length, -1)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return output_error.reshape(self.input.shape)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    NeuralNetwork class is used to build the Neural Network and train it\n",
    "    Methods:\n",
    "    use(loss, loss_derivative) - set the loss function and it's derivative\n",
    "    add_layer(layer) - constructor of the NN, add one of the layers described above\n",
    "    predict(x, grad) - forward pass through the network\n",
    "    fit(x, y, n_epochs, x_val, y_val, batch_size, echo) - fit the network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, random_state=None) -> None:\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_derivative = None\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "        self.optimizer = optimizer\n",
    "        self.best_model = None\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def use(self, loss: callable, loss_derivative: callable) -> None:\n",
    "        self.loss = loss\n",
    "        self.loss_derivative = loss_derivative\n",
    "\n",
    "    def add_layer(self, layer) -> None:\n",
    "        if 'set_optimizer' in layer.__dir__():\n",
    "            layer.set_optimizer(self.optimizer)\n",
    "\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def predict(self, x: np.array, grad: bool = False) -> np.array:\n",
    "        prediction = x\n",
    "        for layer in self.layers:\n",
    "            prediction = layer.forward(prediction, grad=grad)\n",
    "        return prediction\n",
    "\n",
    "    def fit(self,\n",
    "            x: np.array,\n",
    "            y: np.array,\n",
    "            n_epochs: int,\n",
    "            x_val: np.array = None,\n",
    "            y_val: np.array = None,\n",
    "            batch_size: int = None,\n",
    "            echo: bool = True\n",
    "            ):\n",
    "\n",
    "        batch_size = batch_size or len(x)\n",
    "        loss_print_epoch = np.max((1, n_epochs / 100))\n",
    "        amount_of_batches = np.ceil(len(x) / batch_size).astype(int)\n",
    "        metric_name = 'val_loss'\n",
    "        \n",
    "        self.best_model = copy.deepcopy(self)\n",
    "        val_history = []\n",
    "\n",
    "        try:\n",
    "            for _ in range(n_epochs):\n",
    "                start_time = time.time()\n",
    "                # it is good to do permutations in each epoch\n",
    "                idxs = np.random.permutation(len(x))\n",
    "                train_error = 0\n",
    "                for batch_idx in range(amount_of_batches):\n",
    "                    batch_slice = idxs[batch_idx * batch_size:batch_idx * batch_size + batch_size]\n",
    "                    x_batch = x[batch_slice]\n",
    "                    y_batch = y[batch_slice]\n",
    "\n",
    "                    preds = self.predict(x_batch, grad=True)\n",
    "                    train_error += self.loss(y_batch, preds)\n",
    "                    output_error = self.loss_derivative(y_batch, preds)\n",
    "                    for layer in reversed(self.layers):\n",
    "                        output_error = layer.backward(output_error)\n",
    "                if echo:\n",
    "                    if x_val is not None and y_val is not None:\n",
    "                        err_val = self.loss(y_val, self.predict(x_val))\n",
    "                        if err_val < self.best_loss:\n",
    "                            self.best_model = copy.deepcopy(self)\n",
    "                            self.best_loss = err_val\n",
    "                            \n",
    "                        if _ % loss_print_epoch == 0:\n",
    "                            print('*' * 30)\n",
    "                            print(f'Epoch time: {time.time()-start_time}')\n",
    "                            print(f'Epoch {_}  train_loss:{train_error / amount_of_batches}, {metric_name}:{err_val}')\n",
    "                    else:\n",
    "                        if _ % loss_print_epoch == 0:\n",
    "                            print('*' * 30)\n",
    "                            print(f'Epoch time: {time.time()-start_time}')\n",
    "                            print(f'Epoch {_}  train_loss:{train_error / amount_of_batches}')\n",
    "        except KeyboardInterrupt:\n",
    "            print('Interrupted by user')\n",
    "            return self\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3fe1e",
   "metadata": {},
   "source": [
    "# Сверточные слои\n",
    "\n",
    "В общем виде, логика свертки изображена ниже:\n",
    "\n",
    "![](./images/convolution.png)\n",
    "\n",
    "Ядра свертки домножаются на изображение, входных фильтров у свертки столько же, сколько и у входного изображения каналов\n",
    "На выходе получается изображение с числом каналов равным числу фильтров свертки, в примере на картинке выше: число входных каналов = 3, число выходных (оно же число фильтров свертки) = 2\n",
    "\n",
    "Небольшой трюк для улучшения вычислительной эффективности самописной свертки (чтобы меньше делать циклов) следующий:\n",
    "Необходимо вытянуть ядра свертки в строки, число строк в такой матрице = числу фильтров.\n",
    "Изображение необходимо также преобразовать, чтобы результат применения свертки был матричным произведением (преобразование изображения зависит от паддингов, страйда и тп)\n",
    "\n",
    "Если изображений несколько, то они конкатенируются по горизонтали\n",
    "\n",
    "Выходные изображения будут вытянуты в строки размера:\n",
    "$[{число фильтров (выходных каналов)}, {размер выходного изображения} \\cdot {число изображений}]$\n",
    "\n",
    "Вручную нетрудно убедиться, что результат такой свертки полностью эквивалентен применению обычной свертки\n",
    "\n",
    "![](./images/convolution_reformatted.svg)\n",
    "\n",
    "Для слоев пулинга ситуация аналогичная: но необходимо при обратном проходе запоминать места максимумов, так как именно по ним и пойдет ошибка, остальные позиции зануляются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778033b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(BaseLayer):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels  # number of input channels\n",
    "        self.out_channels = out_channels  # number of output channels\n",
    "        self.kernel_size = kernel_size  # kernel size, int\n",
    "        self.stride = stride  # stride, int\n",
    "        self.padding = padding  # padding, int, only zeros padding\n",
    "\n",
    "        limit = 1 / np.sqrt(self.kernel_size ** 2)\n",
    "        self.kernel = np.random.uniform(\n",
    "            -limit,\n",
    "            limit,\n",
    "            size=(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        )\n",
    "\n",
    "        self.bias = np.zeros((self.out_channels, 1))\n",
    "        self.kernel_optimizer = None\n",
    "        self.bias_optimizer = None\n",
    "\n",
    "        self.input_tensor = None\n",
    "        self.batch_size = None\n",
    "        self.reformatted_input = None\n",
    "        self.reformatted_kernel = None\n",
    "\n",
    "        self.channel_steps = []\n",
    "\n",
    "    def set_kernel(self, kernel: np.array) -> None:\n",
    "        \"\"\"\n",
    "        For setting the kernel manually\n",
    "        \"\"\"\n",
    "        assert len(kernel.shape) == 4\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.kernel_optimizer = copy.copy(optimizer)\n",
    "        self.bias_optimizer = copy.copy(optimizer)\n",
    "\n",
    "        self.kernel_optimizer.set_weight(self.kernel)\n",
    "        self.bias_optimizer.set_weight(self.bias)\n",
    "\n",
    "    def _reformat_kernel(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformat the kernel to perform convolution as matrix multiplication\n",
    "        \"\"\"\n",
    "        assert len(self.kernel.shape) == 4\n",
    "        return self.kernel.reshape((self.out_channels, -1))\n",
    "\n",
    "    def _reformat_input(self, input_tensor: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformat the batch of input images to perform convolution as matrix multiplication\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for image in input_tensor:\n",
    "            result.append(self._reformat_image(image))\n",
    "\n",
    "        return np.hstack(result)\n",
    "\n",
    "    def _reformat_image(self, image: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformatting each image in the batch\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for channel in image:\n",
    "            result.append(self._reformat_channel(channel))\n",
    "\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def _reformat_channel(self, channel: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformat each channel in the image with addinction of zeros as padding\n",
    "        \"\"\"\n",
    "        input_map = self._add_padding(channel)\n",
    "\n",
    "        self.padded_width_in, self.padded_height_in = input_map.shape\n",
    "\n",
    "        width_in, height_in = channel.shape\n",
    "\n",
    "        self.width_out = (width_in - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        self.height_out = (height_in - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        output_map = []\n",
    "\n",
    "        write_steps = True\n",
    "        if self.channel_steps:\n",
    "            write_steps = False\n",
    "\n",
    "        for i in range(self.width_out):\n",
    "            for j in range(self.height_out):\n",
    "                horizontal_slice_from = i * self.stride\n",
    "                horizontal_slice_to = self.kernel_size + i * self.stride\n",
    "\n",
    "                vertical_slice_from = j * self.stride\n",
    "                vertical_slice_to = self.kernel_size + j * self.stride\n",
    "\n",
    "                if write_steps:\n",
    "                    self.channel_steps.append(\n",
    "                        (horizontal_slice_from, horizontal_slice_to, vertical_slice_from, vertical_slice_to)\n",
    "                    )\n",
    "\n",
    "                subsample = input_map[horizontal_slice_from:horizontal_slice_to, vertical_slice_from:vertical_slice_to]\n",
    "                output_map.append(subsample.reshape(-1, 1))\n",
    "\n",
    "        return np.hstack(output_map)\n",
    "\n",
    "    def _add_padding(self, input_tensor: np.array) -> np.array:\n",
    "        new_width = input_tensor.shape[0] + self.padding * 2\n",
    "        new_height = input_tensor.shape[1] + self.padding * 2\n",
    "        padded_map = np.zeros((new_width, new_height))\n",
    "\n",
    "        padded_map[self.padding:new_width - self.padding, self.padding:new_height - self.padding] = input_tensor\n",
    "        return padded_map\n",
    "\n",
    "    def _reformat_forward(self, forward_output: np.array) -> np.array:\n",
    "        result = []\n",
    "        for image in forward_output:\n",
    "            result.append(image.reshape(self.batch_size, self.height_out, self.width_out))\n",
    "        return np.hstack(result).reshape((self.batch_size, self.out_channels, self.height_out, self.width_out))\n",
    "\n",
    "    @staticmethod\n",
    "    def _reformat_output(output_error: np.array) -> np.array:\n",
    "        images_result = []\n",
    "        for image in output_error:\n",
    "            channel_result = []\n",
    "            for channel in image:\n",
    "                channel_result.append(channel.ravel())\n",
    "            images_result.append(np.vstack(channel_result))\n",
    "        return np.hstack(images_result)\n",
    "\n",
    "    def _format_image_back(self, image: np.array) -> np.array:\n",
    "        zeros = np.zeros((self.in_channels, self.padded_width_in, self.padded_height_in))\n",
    "        elements_in_kernel = self.kernel_size ** 2\n",
    "        for i in range(self.in_channels):\n",
    "            for j, line in enumerate(image.T):\n",
    "                reshaped_conv = line[i * elements_in_kernel:(i + 1) * elements_in_kernel].reshape(self.kernel_size,\n",
    "                                                                                                  self.kernel_size)\n",
    "\n",
    "                horizontal_slice_from, horizontal_slice_to, vertical_slice_from, vertical_slice_to = self.channel_steps[\n",
    "                    j]\n",
    "                zeros[i][horizontal_slice_from:horizontal_slice_to,\n",
    "                         vertical_slice_from:vertical_slice_to] = reshaped_conv\n",
    "        return zeros\n",
    "\n",
    "    def _cut_padding(self, image: np.array) -> np.array:\n",
    "        result = []\n",
    "        for channel in image:\n",
    "            channel_no_pad = channel[self.padding:self.padded_width_in - self.padding,\n",
    "                                     self.padding:self.padded_height_in - self.padding]\n",
    "            result.append(channel_no_pad[np.newaxis, :])\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def _reformat_input_error(self, input_error: np.array) -> np.array:\n",
    "        assert len(input_error.T) % self.batch_size == 0\n",
    "        size_for_one_image = int(len(input_error.T) / self.batch_size)\n",
    "        result = []\n",
    "        for i in range(self.batch_size):\n",
    "            image = input_error[:, i * size_for_one_image:(i + 1) * size_for_one_image]\n",
    "            image = self._format_image_back(image)\n",
    "\n",
    "            if self.padding != 0:\n",
    "                image = self._cut_padding(image)\n",
    "\n",
    "            result.append(image[np.newaxis, :])\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def forward(self, input_tensor: np.array, grad: bool = False) -> np.array:\n",
    "        assert len(input_tensor.shape) == 4\n",
    "        self.input_tensor = input_tensor\n",
    "        self.batch_size = self.input_tensor.shape[0]\n",
    "\n",
    "        self.reformatted_input = self._reformat_input(input_tensor)\n",
    "        self.reformatted_kernel = self._reformat_kernel()\n",
    "\n",
    "        result = self.reformatted_kernel.dot(self.reformatted_input) + self.bias\n",
    "        return self._reformat_forward(result)\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        assert self.kernel_optimizer is not None and self.bias_optimizer is not None, 'You should set an optimizer'\n",
    "        output_error = self._reformat_output(output_error)\n",
    "        w_grad = output_error.dot(self.reformatted_input.T).reshape(self.kernel.shape)\n",
    "        b_grad = output_error.dot(np.ones((output_error.shape[1], 1)))\n",
    "\n",
    "        self.kernel = self.kernel_optimizer.step(w_grad)\n",
    "        self.bias = self.bias_optimizer.step(b_grad)\n",
    "\n",
    "        input_error = self.reformatted_kernel.T.dot(output_error)\n",
    "        input_error = self._reformat_input_error(input_error)\n",
    "        return input_error\n",
    "\n",
    "\n",
    "class MaxPool2D(BaseLayer):\n",
    "    def __init__(self, kernel_size: int, stride: int, padding: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.input_tensor = None\n",
    "        self.channel_steps = []\n",
    "\n",
    "    def _reformat_input(self, input_tensor: np.array) -> np.array:\n",
    "        result = []\n",
    "        grad_result = []\n",
    "        for image in input_tensor:\n",
    "            forward_result, grad = self._reformat_image(image)\n",
    "            result.append(forward_result)\n",
    "            grad_result.append(grad)\n",
    "\n",
    "        self.grad = grad_result\n",
    "        return np.vstack(result)\n",
    "\n",
    "    def _reformat_image(self, image: np.array) -> np.array:\n",
    "        result = []\n",
    "        grad_result = []\n",
    "        for channel in image:\n",
    "            forward_result, grad = self._reformat_channel(channel)\n",
    "            result.append(forward_result)\n",
    "            grad_result.append(grad)\n",
    "\n",
    "        return np.vstack(result), grad_result\n",
    "\n",
    "    def _reformat_channel(self, channel: np.array) -> Tuple[np.array, np.array]:\n",
    "        input_map = self._add_padding(channel)\n",
    "\n",
    "        width_in, height_in = channel.shape\n",
    "\n",
    "        self.width_out = (width_in - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        self.height_out = (height_in - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        output_map = np.zeros((self.height_out, self.width_out))\n",
    "\n",
    "        grad = []\n",
    "\n",
    "        write_steps = True\n",
    "        if self.channel_steps:\n",
    "            write_steps = False\n",
    "\n",
    "        for i in range(self.width_out):\n",
    "            for j in range(self.height_out):\n",
    "                horizontal_slice_from = i * self.stride\n",
    "                horizontal_slice_to = self.kernel_size + i * self.stride\n",
    "\n",
    "                vertical_slice_from = j * self.stride\n",
    "                vertical_slice_to = self.kernel_size + j * self.stride\n",
    "\n",
    "                if write_steps:\n",
    "                    self.channel_steps.append(\n",
    "                        (horizontal_slice_from, horizontal_slice_to, vertical_slice_from, vertical_slice_to)\n",
    "                    )\n",
    "\n",
    "                subsample = input_map[horizontal_slice_from:horizontal_slice_to, vertical_slice_from:vertical_slice_to]\n",
    "                output_map[i][j] = np.max(subsample)\n",
    "\n",
    "                grad.append(np.argmax(subsample))\n",
    "\n",
    "        return output_map, grad\n",
    "\n",
    "    def _add_padding(self, input_tensor: np.array) -> np.array:\n",
    "        new_width = input_tensor.shape[0] + self.padding * 2\n",
    "        new_height = input_tensor.shape[1] + self.padding * 2\n",
    "        padded_map = np.zeros((new_width, new_height))\n",
    "\n",
    "        padded_map[self.padding:new_width - self.padding, self.padding:new_height - self.padding] = input_tensor\n",
    "        return padded_map\n",
    "\n",
    "    def _rebuild_output_error(self, output_error: np.array) -> np.array:\n",
    "        zeros = np.zeros(self.input_tensor.shape)\n",
    "        for image_num, image in enumerate(output_error):\n",
    "            for channel_num, channel in enumerate(image):\n",
    "                for i in range(self.width_out * self.height_out):\n",
    "                    horizontal_slice_from, horizontal_slice_to, vertical_slice_from, vertical_slice_to \\\n",
    "                        = self.channel_steps[i]\n",
    "\n",
    "                    slice_ = zeros[image_num][channel_num][horizontal_slice_from:horizontal_slice_to,\n",
    "                                                           vertical_slice_from:vertical_slice_to]\n",
    "                    cur_max = self.grad[image_num][channel_num][i]\n",
    "\n",
    "                    flatten_channel = channel.flatten()\n",
    "                    flatten_zeros = slice_.flatten()\n",
    "\n",
    "                    flatten_zeros[cur_max] = flatten_channel[i]\n",
    "\n",
    "                    slice_ = flatten_zeros.reshape(self.kernel_size, self.kernel_size)\n",
    "                    zeros[image_num][channel_num][horizontal_slice_from:horizontal_slice_to,\n",
    "                                                  vertical_slice_from:vertical_slice_to] = slice_\n",
    "        return zeros\n",
    "\n",
    "    def forward(self, input_tensor: np.array, grad: bool = False) -> np.array:\n",
    "        assert len(input_tensor.shape) == 4\n",
    "        self.input_tensor = input_tensor\n",
    "\n",
    "        result = self._reformat_input(input_tensor)\n",
    "        return result.reshape((input_tensor.shape[0], input_tensor.shape[1], self.height_out, self.width_out))\n",
    "\n",
    "    def backward(self, output_error: np.array) -> np.array:\n",
    "        return self._rebuild_output_error(output_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bccca88",
   "metadata": {},
   "source": [
    "# Оптимизаторы\n",
    "\n",
    "Здесь представлен шаблон для оптимизатора любых параметров, а также [ADAM](https://optimization.cbe.cornell.edu/index.php?title=Adam#:~:text=Adam%20optimizer%20is%20the%20extended,was%20first%20introduced%20in%202014.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40ce303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOptimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class ADAM(BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm.\n",
    "\n",
    "    learning_rate (float, optional) – learning rate (default: 1e-3)\n",
    "    beta1, beta2 (Tuple[float, float], optional) –\n",
    "    coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "    eps (float, optional) – term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8,\n",
    "                 learning_rate: float = 3e-4, weight_decay: float = 0) -> None:\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.EMA1 = None\n",
    "        self.EMA2 = None\n",
    "\n",
    "        self.weight = None\n",
    "\n",
    "    def set_weight(self, weight: np.array) -> None:\n",
    "        self.weight = weight.copy()\n",
    "        self.EMA1 = np.zeros(shape=self.weight.shape)\n",
    "        self.EMA2 = np.zeros(shape=self.weight.shape)\n",
    "\n",
    "    def step(self, grad: np.array) -> np.array:\n",
    "        assert self.weight is not None, 'You should set the weight'\n",
    "        grad = grad.copy() + self.weight_decay * self.weight\n",
    "        self.EMA1 = (1 - self.beta1) * grad + self.beta1 * self.EMA1\n",
    "        self.EMA2 = (1 - self.beta2) * grad ** 2 + self.beta2 * self.EMA2\n",
    "        self.weight -= self.learning_rate * self.EMA1 / (np.sqrt(self.EMA2) + self.eps)\n",
    "\n",
    "        return self.weight.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53f97f",
   "metadata": {},
   "source": [
    "# Функция потерь и активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5830cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true: np.array, a_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    arg = -a_pred[lenght_y, y_true.ravel()]\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1)\n",
    "    loss = np.sum(arg + np.log(sum_exp))\n",
    "    return loss / len(y_true)\n",
    "\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true: np.array, a_pred: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss derivative for multi-classification tasks\n",
    "    :param y_true: 2D vector with classes, i.e. [[0], [3], [4], [1], [2]]\n",
    "    :param a_pred: scores for each class before softmax function with shape [n_samples, n_classes]\n",
    "    :return: np.array with shape [n_samples, n_classes] with CrossEntropyLoss derivatives for each weight\n",
    "    \"\"\"\n",
    "    lenght_y = list(range(len(y_true)))\n",
    "    sum_exp = np.sum(np.exp(a_pred), axis=1).reshape(-1, 1)\n",
    "    loss = np.exp(a_pred.copy()) / sum_exp\n",
    "    loss[lenght_y, y_true.ravel()] -= 1\n",
    "\n",
    "    return loss / len(y_true)\n",
    "\n",
    "\n",
    "def relu(z: Union[np.array, float, int, list]) -> Union[np.array, float, int]:\n",
    "    \"\"\"\n",
    "    ReLU function\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def relu_derivative(z: Union[np.array, float, int, list]) -> np.array:\n",
    "    \"\"\"\n",
    "    ReLU function derivative\n",
    "    \"\"\"\n",
    "    return (z > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac2e21",
   "metadata": {},
   "source": [
    "## Соберем нейросеть\n",
    "\n",
    "Архитектура - немного измененный [LeNet](https://en.wikipedia.org/wiki/LeNet), только с [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) в качестве функции активации\n",
    "\n",
    "Так как все-таки модуль сверток написан вручную, то работает он достаточно медленно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a48a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Epoch time: 800.0202407836914\n",
      "Epoch 0  train_loss:0.18472583488470692, val_loss:0.08301266881852988\n",
      "******************************\n",
      "Epoch time: 835.3740463256836\n",
      "Epoch 1  train_loss:0.0670767141465284, val_loss:0.06624281571914292\n",
      "******************************\n",
      "Epoch time: 901.5778431892395\n",
      "Epoch 2  train_loss:0.04446301732173773, val_loss:0.058130213556426356\n",
      "******************************\n",
      "Epoch time: 781.5154285430908\n",
      "Epoch 3  train_loss:0.0363526804755321, val_loss:0.05859034516944277\n",
      "******************************\n",
      "Epoch time: 783.0993678569794\n",
      "Epoch 4  train_loss:0.02904319436614454, val_loss:0.06992599421213257\n",
      "******************************\n",
      "Epoch time: 1024.2124621868134\n",
      "Epoch 5  train_loss:0.02356978075359472, val_loss:0.054019895322420554\n",
      "******************************\n",
      "Epoch time: 765.4455499649048\n",
      "Epoch 6  train_loss:0.019167610680537562, val_loss:0.06124898861876308\n",
      "******************************\n",
      "Epoch time: 767.1837525367737\n",
      "Epoch 7  train_loss:0.018189014205384574, val_loss:0.10793082797561979\n",
      "******************************\n",
      "Epoch time: 773.5598776340485\n",
      "Epoch 8  train_loss:0.017020164920313906, val_loss:0.0666306192657323\n",
      "******************************\n",
      "Epoch time: 766.0780692100525\n",
      "Epoch 9  train_loss:0.014932297139820376, val_loss:0.06469825969591496\n"
     ]
    }
   ],
   "source": [
    "optimizer = ADAM(learning_rate=1e-3)\n",
    "\n",
    "classification_nn = NeuralNetwork(optimizer, 42)\n",
    "classification_nn.use(cross_entropy_loss, cross_entropy_loss_derivative)\n",
    "classification_nn.add_layer(Conv2D(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2))\n",
    "classification_nn.add_layer(MaxPool2D(2, 2))\n",
    "classification_nn.add_layer(Activation(relu, relu_derivative))\n",
    "classification_nn.add_layer(Conv2D(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0))\n",
    "classification_nn.add_layer(MaxPool2D(2, 2))\n",
    "classification_nn.add_layer(Activation(relu, relu_derivative))\n",
    "classification_nn.add_layer(Ravel())\n",
    "classification_nn.add_layer(Linear(400, 120))\n",
    "classification_nn.add_layer(Activation(relu, relu_derivative))\n",
    "classification_nn.add_layer(Linear(120, 84))\n",
    "classification_nn.add_layer(Activation(relu, relu_derivative))\n",
    "classification_nn.add_layer(Linear(84, 10))\n",
    "\n",
    "classification_nn.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    x_val=x_test,\n",
    "    y_val=y_test,\n",
    "    n_epochs=10,\n",
    "    batch_size=32,\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "preds = np.argmax(classification_nn.predict(x_test), axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c73d1a",
   "metadata": {},
   "source": [
    "## Ради интереса взглянем на обученные ядра первого слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5887694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in classification_nn.layers[0].kernel:\n",
    "    plt.imshow(channel.reshape(5, 5), cmap='Greys')\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f72494ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_predict = np.argmax(classification_nn.best_model.predict(x_test), axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d7695f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852813852813853"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, best_model_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d31a6",
   "metadata": {},
   "source": [
    "# Построим полносвязную сеть для классификации и оценим ее качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa17319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Epoch time: 3.6547372341156006\n",
      "Epoch 0  train_loss:0.24971544549488192, val_loss:0.16915609880651797\n",
      "******************************\n",
      "Epoch time: 3.6830849647521973\n",
      "Epoch 1  train_loss:0.10699774954248718, val_loss:0.11688477028653002\n",
      "******************************\n",
      "Epoch time: 3.8882784843444824\n",
      "Epoch 2  train_loss:0.07509807720037913, val_loss:0.10925252187258308\n",
      "******************************\n",
      "Epoch time: 3.89764666557312\n",
      "Epoch 3  train_loss:0.05520927678390667, val_loss:0.10917946532038814\n",
      "******************************\n",
      "Epoch time: 3.731175661087036\n",
      "Epoch 4  train_loss:0.041058203198643714, val_loss:0.11457352669917326\n",
      "******************************\n",
      "Epoch time: 3.9116370677948\n",
      "Epoch 5  train_loss:0.030300611525994833, val_loss:0.10596136606275483\n",
      "******************************\n",
      "Epoch time: 3.7643418312072754\n",
      "Epoch 6  train_loss:0.02659439393996012, val_loss:0.10717448275516865\n",
      "******************************\n",
      "Epoch time: 4.125145673751831\n",
      "Epoch 7  train_loss:0.019006207303550873, val_loss:0.11921591629435555\n",
      "******************************\n",
      "Epoch time: 4.275602102279663\n",
      "Epoch 8  train_loss:0.014726998118679393, val_loss:0.11998439265081491\n",
      "******************************\n",
      "Epoch time: 4.279193639755249\n",
      "Epoch 9  train_loss:0.012446256275753896, val_loss:0.13045820426410198\n",
      "******************************\n",
      "Epoch time: 4.268081426620483\n",
      "Epoch 10  train_loss:0.014385688028369285, val_loss:0.1432228123615309\n",
      "******************************\n",
      "Epoch time: 4.2906763553619385\n",
      "Epoch 11  train_loss:0.013001645359556327, val_loss:0.13363846379231406\n",
      "******************************\n",
      "Epoch time: 4.330368995666504\n",
      "Epoch 12  train_loss:0.008343817992114455, val_loss:0.15527254559645648\n",
      "******************************\n",
      "Epoch time: 4.280194997787476\n",
      "Epoch 13  train_loss:0.012521034073635157, val_loss:0.14960523204188475\n",
      "******************************\n",
      "Epoch time: 4.165350437164307\n",
      "Epoch 14  train_loss:0.00799391219731647, val_loss:0.16139524895203408\n",
      "******************************\n",
      "Epoch time: 4.261462450027466\n",
      "Epoch 15  train_loss:0.010316690674669358, val_loss:0.14003804636873088\n",
      "******************************\n",
      "Epoch time: 4.243786334991455\n",
      "Epoch 16  train_loss:0.006457132903973195, val_loss:0.13863821118017347\n",
      "******************************\n",
      "Epoch time: 4.226344108581543\n",
      "Epoch 17  train_loss:0.005526855695670399, val_loss:0.1436360543149897\n",
      "******************************\n",
      "Epoch time: 4.223751783370972\n",
      "Epoch 18  train_loss:0.00843494337028033, val_loss:0.21008935089504774\n",
      "******************************\n",
      "Epoch time: 4.249584674835205\n",
      "Epoch 19  train_loss:0.015416670080154807, val_loss:0.1582206223135502\n",
      "******************************\n",
      "Epoch time: 4.2099244594573975\n",
      "Epoch 20  train_loss:0.003627817473867294, val_loss:0.15442797563634536\n",
      "******************************\n",
      "Epoch time: 4.193714618682861\n",
      "Epoch 21  train_loss:0.006463863344456922, val_loss:0.19154967762205233\n",
      "******************************\n",
      "Epoch time: 4.250837087631226\n",
      "Epoch 22  train_loss:0.007113120076461062, val_loss:0.153910967928758\n",
      "******************************\n",
      "Epoch time: 4.2295122146606445\n",
      "Epoch 23  train_loss:0.0037977512405012706, val_loss:0.15224075975321247\n",
      "******************************\n",
      "Epoch time: 4.274029970169067\n",
      "Epoch 24  train_loss:0.011648142422272453, val_loss:0.1956445609860952\n",
      "******************************\n",
      "Epoch time: 4.252838850021362\n",
      "Epoch 25  train_loss:0.006501317102147163, val_loss:0.16980685712810056\n",
      "******************************\n",
      "Epoch time: 4.234495162963867\n",
      "Epoch 26  train_loss:0.002901661855623712, val_loss:0.1565013463453679\n",
      "******************************\n",
      "Epoch time: 4.208660364151001\n",
      "Epoch 27  train_loss:0.0009388799070844378, val_loss:0.17866433556548394\n",
      "******************************\n",
      "Epoch time: 4.2070934772491455\n",
      "Epoch 28  train_loss:0.010624152069624837, val_loss:0.18905945323239856\n",
      "******************************\n",
      "Epoch time: 4.181597709655762\n",
      "Epoch 29  train_loss:0.01066666437573977, val_loss:0.1725019275104311\n"
     ]
    }
   ],
   "source": [
    "x_train_fc = x_train.reshape(-1, 784)\n",
    "x_test_fc = x_test.reshape(-1, 784)\n",
    "optimizer = ADAM(learning_rate=1e-3)\n",
    "\n",
    "classification_nn_fc = NeuralNetwork(optimizer, 42)\n",
    "classification_nn_fc.use(cross_entropy_loss, cross_entropy_loss_derivative)\n",
    "classification_nn_fc.add_layer(Linear(784, 120))\n",
    "classification_nn_fc.add_layer(Activation(relu, relu_derivative))\n",
    "classification_nn_fc.add_layer(Linear(120, 84))\n",
    "classification_nn_fc.add_layer(Activation(relu, relu_derivative))\n",
    "classification_nn_fc.add_layer(Linear(84, 10))\n",
    "\n",
    "classification_nn_fc.fit(\n",
    "    x=x_train_fc,\n",
    "    y=y_train,\n",
    "    x_val=x_test_fc,\n",
    "    y_val=y_test,\n",
    "    n_epochs=30,\n",
    "    batch_size=32,\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "preds_fc = np.argmax(classification_nn_fc.predict(x_test_fc), axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e38b58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_predict_fc = np.argmax(classification_nn_fc.best_model.predict(x_test_fc), axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "281d6c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96998556998557"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, best_model_predict_fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83c02b",
   "metadata": {},
   "source": [
    "### Видно, что качество обычной нейронной сети уступает сверточной\n",
    "Можно еще покрутить параметры, добавить эпох для сверточной сети и улучшить качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d84a805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_oot = test_data.values.reshape(-1, 1, 28, 28) / 255\n",
    "\n",
    "preds_oot = classification_nn.best_model.predict(x_oot)\n",
    "sample_submission.loc[:, 'Label'] = np.argmax(preds_oot, axis=1)\n",
    "sample_submission.to_csv('./data/digit_submission.csv', index=False) # 0.98356 public leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea41efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div #notebook {\n",
       "    background-color: #FFF9EE;\n",
       "    margin: auto;\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "    padding: 15px;\n",
       "    background-color: #FFFAFA;\n",
       "    min-height: 0;\n",
       "    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width to about 80 chars */\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "\n",
       "div.cell.border-box-sizing.code_cell.running { \n",
       "    border: 3px solid #111;\n",
       "}\n",
       "\n",
       "div.cell.code_cell {\n",
       "    background-color: #FFFAFA ;\n",
       "    border-radius: 5px;\n",
       "    padding: 1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Times New Roman';\n",
       "    color: #B8860B\n",
       "}\n",
       "\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 40pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 700;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 25pt;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: italic;\n",
       "    color: #8B4513;\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 20pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 16pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 10pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render p {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 15pt;\n",
       "    color: black;\n",
       "    text-align: justify;\n",
       "    text-justify: inter-word;\n",
       "    line-height: 1.5;\n",
       "}\n",
       "\n",
       "mark {\n",
       "  background: #D5EAFF;\n",
       "  color: black;\n",
       "}\n",
       "\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:2000px;  /* your desired max-height here */\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./style.css') as f:\n",
    "    style = f.read()\n",
    "HTML(style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_39",
   "language": "python",
   "name": "data_science_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
