{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient Boosting</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.datasets import load_boston, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor_:\n",
    "    '''\n",
    "    DecisionTreeRegressor class is used to permorm CART algorithm for Decision Tree Regressor.\n",
    "    *****************************************************************************************\n",
    "    Attributes:\n",
    "    max_depth - int; adjusts the depth of the tree\n",
    "    min_samples_split - int; set the minimum size to split\n",
    "    *****************************************************************************************\n",
    "    Methods:\n",
    "    make_dataset - from X and y makes one matrix\n",
    "    MSE - count the MSE\n",
    "    test_split - split the dataset into two groups with the threshold \n",
    "    get_split - find the best split using the best criterion value\n",
    "    to_terminal - make the final node\n",
    "    split - build the tree, recursively\n",
    "    fit - starts the tree building\n",
    "    print_tree - print the tree\n",
    "    predict_row - predict the row of data using the tree\n",
    "    predict - predict the whole data\n",
    "    '''\n",
    "    def __init__(self, max_depth=np.infty, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        \n",
    "    def make_dataset(self, X, y):\n",
    "        return np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "    \n",
    "    def MSE(self, groups):\n",
    "        n_instances = np.sum([len(group) for group in groups])\n",
    "        criterion = 0\n",
    "        for group in groups:\n",
    "            size = len(group)\n",
    "            if size == 0:\n",
    "                continue\n",
    "            score = np.mean(group[:, -1])\n",
    "            criterion += np.sum((score - group[:, -1]) ** 2) # not weighted, just sum or residuals\n",
    "        return criterion\n",
    "\n",
    "    def test_split(self, index, value, dataset):\n",
    "        left, right = list(), list()\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return np.array(left), np.array(right)\n",
    "\n",
    "    def get_split(self, dataset):\n",
    "        b_index, b_value, b_score, b_groups, MSE_history = None, None, np.infty, None, []\n",
    "        for index in range(len(dataset[0]) - 1):\n",
    "            for row in dataset:\n",
    "                groups = self.test_split(index, row[index], dataset)\n",
    "                criterion = self.MSE(groups)\n",
    "                MSE_history.append(criterion)\n",
    "                if criterion <= b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], criterion, groups\n",
    "        if len(set(MSE_history)) == 1: # checking if there's no need to split dataset\n",
    "            b_groups = self.test_split(0, np.min(dataset[:, 0]), dataset)\n",
    "        try:\n",
    "            b_value = (b_value + np.max(b_groups[0][:, b_index])) / 2\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "    def to_terminal(self, group):\n",
    "        outcomes = group[:, -1]\n",
    "        return np.mean(outcomes)\n",
    "\n",
    "    def split(self, node, depth):\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            node['left'] = node['right'] = self.to_terminal(np.array(left.tolist() + right.tolist()))\n",
    "            return\n",
    "        if depth >= self.max_depth:\n",
    "            node['left'], node['right'] = self.to_terminal(left), self.to_terminal(right)\n",
    "            return\n",
    "        if len(left) < self.min_samples_split:\n",
    "            node['left'] = self.to_terminal(left)\n",
    "        else:\n",
    "            node['left'] = self.get_split(left)\n",
    "            self.split(node['left'], depth+1)\n",
    "        if len(right) < self.min_samples_split:\n",
    "            node['right'] = self.to_terminal(right)\n",
    "        else:\n",
    "            node['right'] = self.get_split(right)\n",
    "            self.split(node['right'], depth+1)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        train = self.make_dataset(X, y)\n",
    "        root = self.get_split(train)\n",
    "        self.split(root, 1)\n",
    "        self.node = root\n",
    "\n",
    "    def print_tree(self, node, depth=0):\n",
    "        if isinstance(node, dict):\n",
    "            print('{0}[X[{1}] < {2}]'.format(depth * '>', node['index'], np.round(node['value'], 3)))\n",
    "            self.print_tree(node['left'], depth+1)\n",
    "            self.print_tree(node['right'], depth+1)\n",
    "        else:\n",
    "            print('{0}[{1}]'.format('   ' + depth * '>', node))\n",
    "    \n",
    "    def predict_row(self, node, row):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self.predict_row(node['left'], row)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.predict_row(node['right'], row)\n",
    "            else:\n",
    "                return node['right']\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([])\n",
    "        for row in X:\n",
    "            predictions = np.append(predictions, self.predict_row(self.node, row))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    '''\n",
    "    GradientBoostingRegressor class is used to permorm gragient boosting on trees.\n",
    "    *****************************************************************************************\n",
    "    Attributes:\n",
    "    lr - learning rate\n",
    "    n_iter - number of iterations\n",
    "    max_depth - int; adjusts the depth of the tree\n",
    "    min_samples_split - int; set the minimum size to split\n",
    "    *****************************************************************************************\n",
    "    Methods:\n",
    "    fit - build the algorithm\n",
    "    predict - predict the whole data\n",
    "    '''\n",
    "    def __init__(self, lr=0.1, n_iter=100, **model_args):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.model = DecisionTreeRegressor(**model_args)\n",
    "        self.models = list()\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            self.models.append(copy.deepcopy(self.model))\n",
    "            \n",
    "    def fit(self, x, y):\n",
    "        approximation = np.zeros((x.shape[0])).reshape(-1)\n",
    "        \n",
    "        for model in self.models:\n",
    "            grad = -(y.reshape(-1) - approximation)\n",
    "            \n",
    "            model.fit(x, grad) # fit model on residuals\n",
    "            approximation -= self.lr * model.predict(x)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        approximation = np.zeros((x.shape[0])).reshape(-1)\n",
    "        for model in self.models:\n",
    "            approximation -= self.lr * model.predict(x)\n",
    "        return approximation\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'GradientBoostingRegressor(lr={self.lr}, n_iter={self.n_iter})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "x = data['data']\n",
    "y = data['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(lr=0.1, n_iter=100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting = GradientBoostingRegressor(lr=0.1, max_depth=3)\n",
    "boosting.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.8828142228476458\n"
     ]
    }
   ],
   "source": [
    "print(f'{r2_score.__name__}: {r2_score(y_test, boosting.predict(x_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = DecisionTreeRegressor_(max_depth=3)\n",
    "my_tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.6860847825591382\n"
     ]
    }
   ],
   "source": [
    "print(f'{r2_score.__name__}: {r2_score(y_test, my_tree.predict(x_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_tree = DecisionTreeRegressor(max_depth=3)\n",
    "skl_tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.6860847825591382\n"
     ]
    }
   ],
   "source": [
    "print(f'{r2_score.__name__}: {r2_score(y_test, skl_tree.predict(x_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_classification(n_samples=2000, n_features=5, n_classes=4, n_clusters_per_class=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_train = ohe.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = ohe.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    '''\n",
    "    Softmax function, z - matrix with shape [n_objects; n_classes]\n",
    "    '''\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier:\n",
    "    def __init__(self, lr=0.1, n_iter=100, **model_args):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.model = DecisionTreeRegressor(**model_args) # yes, we build regressor\n",
    "        self.models = dict()\n",
    "        self.n_classes = None\n",
    "            \n",
    "    def loss(self, y_true, a_pred):\n",
    "        '''\n",
    "        Compute cross-entropy loss, a_pred - matrix with shape [n_objects; n_classes]\n",
    "        '''\n",
    "        loss = 0\n",
    "        for i, j in enumerate(y_true):\n",
    "            loss += -np.log(a_pred[i][j])\n",
    "        return loss\n",
    "            \n",
    "    def fit(self, x, y, test_set=None, echo=True):\n",
    "        self.n_classes = y.shape[1]\n",
    "        y_not_ohe = np.argmax(y, axis=1)\n",
    "        \n",
    "        # for each class\n",
    "        approximation = np.zeros((x.shape[0], self.n_classes))\n",
    "        for i in range(self.n_iter):\n",
    "            # for every iteration we create n models for n classes\n",
    "            self.models[i] = dict()\n",
    "            \n",
    "            if echo:\n",
    "                print(f'LOSS {i+1}: {self.loss(y_not_ohe, softmax(approximation))}')\n",
    "            \n",
    "            # because we build models independently, we need to normalize their preds with softmax\n",
    "            grad = -(y - softmax(approximation))\n",
    "            \n",
    "            preds = np.zeros(shape=approximation.shape)\n",
    "            for class_ in range(self.n_classes):\n",
    "                \n",
    "                # in multiclass classification we need to build n models for n classes\n",
    "                \n",
    "                model = copy.deepcopy(self.model)\n",
    "                grad_for_class = grad[:, class_]\n",
    "\n",
    "                model.fit(x, grad_for_class) # fit model on residuals\n",
    "                preds[:, class_] = model.predict(x)\n",
    "                \n",
    "                # save model for iteration and class\n",
    "                self.models[i][class_] = model\n",
    "\n",
    "            approximation -= self.lr * preds\n",
    "            if test_set is not None:\n",
    "                x_t, y_t = test_set\n",
    "                y_t = np.argmax(y_t, axis=1)\n",
    "                preds = self.predict(x_t)\n",
    "                if echo:\n",
    "                    print(f'TEST LOSS {i+1}: {self.loss(y_t, softmax(preds))}')\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        approximation = np.zeros((x.shape[0], self.n_classes))\n",
    "        for i in self.models.keys():\n",
    "            preds = np.zeros(approximation.shape)\n",
    "            for class_ in range(self.n_classes):\n",
    "                preds[:, class_] = self.models[i][class_].predict(x)\n",
    "            approximation -= self.lr * preds\n",
    "        return softmax(approximation)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'GradientBoostingClassifier(lr={self.lr}, n_iter={self.n_iter})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_clf = GradientBoostingClassifier(lr=0.1, n_iter=463, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS 1: 2218.0709777917327\n",
      "LOSS 2: 2124.8102895477955\n",
      "LOSS 3: 2036.2085162079009\n",
      "LOSS 4: 1952.2448309812635\n",
      "LOSS 5: 1872.8987104748887\n",
      "LOSS 6: 1798.1813118926648\n",
      "LOSS 7: 1727.6540558724314\n",
      "LOSS 8: 1660.9182707283755\n",
      "LOSS 9: 1598.4428696209434\n",
      "LOSS 10: 1539.685174954901\n",
      "LOSS 11: 1484.4864760676435\n",
      "LOSS 12: 1432.6232764327658\n",
      "LOSS 13: 1383.9251535469305\n",
      "LOSS 14: 1338.2183210467865\n",
      "LOSS 15: 1295.347858330585\n",
      "LOSS 16: 1255.1553130076895\n",
      "LOSS 17: 1217.4237880190576\n",
      "LOSS 18: 1181.878876523908\n",
      "LOSS 19: 1148.2829859115802\n",
      "LOSS 20: 1116.3690202923265\n",
      "LOSS 21: 1086.1797784393157\n",
      "LOSS 22: 1058.087177581934\n",
      "LOSS 23: 1031.3688740236992\n",
      "LOSS 24: 1006.1449474234932\n",
      "LOSS 25: 982.4357423031788\n",
      "LOSS 26: 959.9761234528261\n",
      "LOSS 27: 938.9562211621941\n",
      "LOSS 28: 919.0371342002221\n",
      "LOSS 29: 900.1024522880687\n",
      "LOSS 30: 882.1913477264865\n",
      "LOSS 31: 865.2992111067829\n",
      "LOSS 32: 849.1008698955894\n",
      "LOSS 33: 833.8201238375004\n",
      "LOSS 34: 819.2888040498311\n",
      "LOSS 35: 805.5193243744591\n",
      "LOSS 36: 792.3546006383817\n",
      "LOSS 37: 779.6766508999926\n",
      "LOSS 38: 767.7075492323333\n",
      "LOSS 39: 756.3049104350754\n",
      "LOSS 40: 745.3952287722386\n",
      "LOSS 41: 735.0083181427976\n",
      "LOSS 42: 725.052967087457\n",
      "LOSS 43: 715.5604027842188\n",
      "LOSS 44: 706.3579778017444\n",
      "LOSS 45: 697.6470662315667\n",
      "LOSS 46: 689.203790087744\n",
      "LOSS 47: 681.0674618631735\n",
      "LOSS 48: 673.2717390928537\n",
      "LOSS 49: 665.7010430651994\n",
      "LOSS 50: 658.4767215308723\n",
      "LOSS 51: 651.5654894415939\n",
      "LOSS 52: 644.8433919854755\n",
      "LOSS 53: 638.3003985081183\n",
      "LOSS 54: 632.0135075393487\n",
      "LOSS 55: 625.9186701579309\n",
      "LOSS 56: 620.0171121889547\n",
      "LOSS 57: 614.3167597096415\n",
      "LOSS 58: 608.8733295842211\n",
      "LOSS 59: 603.4749791870215\n",
      "LOSS 60: 598.258852691914\n",
      "LOSS 61: 593.2298044511557\n",
      "LOSS 62: 588.2691738924153\n",
      "LOSS 63: 583.5007881768306\n",
      "LOSS 64: 578.8365989385936\n",
      "LOSS 65: 574.4076784032128\n",
      "LOSS 66: 570.1430340650926\n",
      "LOSS 67: 565.9183660905079\n",
      "LOSS 68: 561.8126950793172\n",
      "LOSS 69: 557.8464160885176\n",
      "LOSS 70: 553.8464977089853\n",
      "LOSS 71: 549.9881253281862\n",
      "LOSS 72: 546.472466744553\n",
      "LOSS 73: 542.871152331338\n",
      "LOSS 74: 539.2209233217538\n",
      "LOSS 75: 535.8450383600903\n",
      "LOSS 76: 532.4996765432869\n",
      "LOSS 77: 528.9985602445547\n",
      "LOSS 78: 525.4790974838339\n",
      "LOSS 79: 522.232056633844\n",
      "LOSS 80: 519.0259212419604\n",
      "LOSS 81: 515.9291244047419\n",
      "LOSS 82: 513.0056057757836\n",
      "LOSS 83: 509.9570153287973\n",
      "LOSS 84: 507.01333760610885\n",
      "LOSS 85: 504.2172570002241\n",
      "LOSS 86: 501.440537739867\n",
      "LOSS 87: 498.72072219673726\n",
      "LOSS 88: 496.20524305599406\n",
      "LOSS 89: 493.5368771171315\n",
      "LOSS 90: 490.84355061169674\n",
      "LOSS 91: 488.1982916462366\n",
      "LOSS 92: 485.91813408003634\n",
      "LOSS 93: 483.2600673359567\n",
      "LOSS 94: 480.8705402751799\n",
      "LOSS 95: 478.4207541627999\n",
      "LOSS 96: 475.7873680479398\n",
      "LOSS 97: 473.13806351113567\n",
      "LOSS 98: 470.6736697270844\n",
      "LOSS 99: 468.09885517014146\n",
      "LOSS 100: 465.89625468503306\n",
      "LOSS 101: 463.54785707470074\n",
      "LOSS 102: 461.3046383132574\n",
      "LOSS 103: 459.0062860458352\n",
      "LOSS 104: 456.7820704133848\n",
      "LOSS 105: 454.6693716695017\n",
      "LOSS 106: 452.4865855468468\n",
      "LOSS 107: 450.21226603131464\n",
      "LOSS 108: 447.9408619377996\n",
      "LOSS 109: 445.8690455552827\n",
      "LOSS 110: 443.746686139171\n",
      "LOSS 111: 441.5315076734903\n",
      "LOSS 112: 439.67206470251966\n",
      "LOSS 113: 437.6679042768875\n",
      "LOSS 114: 435.7134020834445\n",
      "LOSS 115: 433.9910306274467\n",
      "LOSS 116: 432.05575027802973\n",
      "LOSS 117: 430.14535176052914\n",
      "LOSS 118: 428.2134715459787\n",
      "LOSS 119: 426.460798552337\n",
      "LOSS 120: 424.8499192370519\n",
      "LOSS 121: 423.0804829794688\n",
      "LOSS 122: 421.2522857992539\n",
      "LOSS 123: 419.60373383102296\n",
      "LOSS 124: 417.8406108994095\n",
      "LOSS 125: 416.30285458398674\n",
      "LOSS 126: 414.5358263417747\n",
      "LOSS 127: 412.9106208406183\n",
      "LOSS 128: 411.1736335782264\n",
      "LOSS 129: 409.5999944541399\n",
      "LOSS 130: 407.94335423444915\n",
      "LOSS 131: 406.18555923505033\n",
      "LOSS 132: 404.6456263874454\n",
      "LOSS 133: 402.97619425789964\n",
      "LOSS 134: 401.43046653750747\n",
      "LOSS 135: 399.7454698780534\n",
      "LOSS 136: 398.10517119476333\n",
      "LOSS 137: 396.60636194884336\n",
      "LOSS 138: 395.0886877310842\n",
      "LOSS 139: 393.71658767661563\n",
      "LOSS 140: 392.2763424767523\n",
      "LOSS 141: 390.6148226527375\n",
      "LOSS 142: 389.1572267944648\n",
      "LOSS 143: 387.5509880765909\n",
      "LOSS 144: 386.1702192842054\n",
      "LOSS 145: 384.6591939477667\n",
      "LOSS 146: 383.3075635238217\n",
      "LOSS 147: 382.0543944353462\n",
      "LOSS 148: 380.65410334648055\n",
      "LOSS 149: 379.1251075899419\n",
      "LOSS 150: 377.7104687107342\n",
      "LOSS 151: 376.3391429508833\n",
      "LOSS 152: 375.08776612225694\n",
      "LOSS 153: 373.66634761506907\n",
      "LOSS 154: 372.37024960212136\n",
      "LOSS 155: 370.95535996784326\n",
      "LOSS 156: 369.6576103675082\n",
      "LOSS 157: 368.59715301490957\n",
      "LOSS 158: 367.10428418899636\n",
      "LOSS 159: 365.8373991699887\n",
      "LOSS 160: 364.39049979814416\n",
      "LOSS 161: 362.9924194312588\n",
      "LOSS 162: 361.8581307052928\n",
      "LOSS 163: 360.574187263044\n",
      "LOSS 164: 359.4085592964855\n",
      "LOSS 165: 357.97017359242454\n",
      "LOSS 166: 356.6892758735595\n",
      "LOSS 167: 355.46636326863177\n",
      "LOSS 168: 354.3594872498159\n",
      "LOSS 169: 353.3332803246116\n",
      "LOSS 170: 351.88104387086645\n",
      "LOSS 171: 350.4760871826153\n",
      "LOSS 172: 349.3549212744523\n",
      "LOSS 173: 348.0554430916435\n",
      "LOSS 174: 346.8393554242151\n",
      "LOSS 175: 345.6578078786517\n",
      "LOSS 176: 344.3981375679801\n",
      "LOSS 177: 343.1611781503162\n",
      "LOSS 178: 341.9481948413661\n",
      "LOSS 179: 340.91640420728004\n",
      "LOSS 180: 339.8298224599827\n",
      "LOSS 181: 338.53625992110693\n",
      "LOSS 182: 337.3004430206057\n",
      "LOSS 183: 336.3105606772139\n",
      "LOSS 184: 335.20433626001494\n",
      "LOSS 185: 334.2789388468857\n",
      "LOSS 186: 333.2042961669248\n",
      "LOSS 187: 332.13047107573783\n",
      "LOSS 188: 331.0511618635001\n",
      "LOSS 189: 329.91254830582454\n",
      "LOSS 190: 328.757255863976\n",
      "LOSS 191: 327.63084400998764\n",
      "LOSS 192: 326.5347925519488\n",
      "LOSS 193: 325.5139538816386\n",
      "LOSS 194: 324.4936847690997\n",
      "LOSS 195: 323.45358715151895\n",
      "LOSS 196: 322.35214752279364\n",
      "LOSS 197: 321.3410081267982\n",
      "LOSS 198: 320.4038422986873\n",
      "LOSS 199: 319.52765215928576\n",
      "LOSS 200: 318.59227016032486\n",
      "LOSS 201: 317.6731832845246\n",
      "LOSS 202: 316.7811441831853\n",
      "LOSS 203: 315.7368264844675\n",
      "LOSS 204: 314.55590194926873\n",
      "LOSS 205: 313.6741277347168\n",
      "LOSS 206: 312.82876764297964\n",
      "LOSS 207: 311.8318901798865\n",
      "LOSS 208: 310.85102743159376\n",
      "LOSS 209: 309.9609791369157\n",
      "LOSS 210: 309.141869001914\n",
      "LOSS 211: 308.2742168471364\n",
      "LOSS 212: 307.1487952516079\n",
      "LOSS 213: 306.22885472631106\n",
      "LOSS 214: 305.3270965668558\n",
      "LOSS 215: 304.41582843229605\n",
      "LOSS 216: 303.44430644501995\n",
      "LOSS 217: 302.5884294294721\n",
      "LOSS 218: 301.7661226677108\n",
      "LOSS 219: 300.84930772472546\n",
      "LOSS 220: 299.8684554167673\n",
      "LOSS 221: 298.97603095103\n",
      "LOSS 222: 298.0097801800801\n",
      "LOSS 223: 297.1858913703155\n",
      "LOSS 224: 296.4376916292706\n",
      "LOSS 225: 295.74925072582124\n",
      "LOSS 226: 295.0160302788891\n",
      "LOSS 227: 294.0748199937906\n",
      "LOSS 228: 293.2071054876061\n",
      "LOSS 229: 292.29776967495053\n",
      "LOSS 230: 291.49146212223076\n",
      "LOSS 231: 290.77534752398503\n",
      "LOSS 232: 290.0452813610405\n",
      "LOSS 233: 289.3070431073632\n",
      "LOSS 234: 288.5395954157839\n",
      "LOSS 235: 287.7240025154154\n",
      "LOSS 236: 286.98898581145625\n",
      "LOSS 237: 286.2382017491072\n",
      "LOSS 238: 285.46264512730335\n",
      "LOSS 239: 284.76967843691887\n",
      "LOSS 240: 283.8594125478142\n",
      "LOSS 241: 283.20321517620613\n",
      "LOSS 242: 282.4186162245957\n",
      "LOSS 243: 281.4378393098886\n",
      "LOSS 244: 280.7434709736133\n",
      "LOSS 245: 280.093702538362\n",
      "LOSS 246: 279.3535457509218\n",
      "LOSS 247: 278.6547428758556\n",
      "LOSS 248: 277.9075754528697\n",
      "LOSS 249: 277.2097585402855\n",
      "LOSS 250: 276.58685108877427\n",
      "LOSS 251: 275.74404086818413\n",
      "LOSS 252: 275.08096264703073\n",
      "LOSS 253: 274.5318762499048\n",
      "LOSS 254: 273.80223876516277\n",
      "LOSS 255: 273.1602660847856\n",
      "LOSS 256: 272.4389079259529\n",
      "LOSS 257: 271.9270195711168\n",
      "LOSS 258: 271.25450194356233\n",
      "LOSS 259: 270.40495834155894\n",
      "LOSS 260: 269.70782161109076\n",
      "LOSS 261: 269.0830583333597\n",
      "LOSS 262: 268.4517164769175\n",
      "LOSS 263: 267.79976287011795\n",
      "LOSS 264: 267.09301986093203\n",
      "LOSS 265: 266.26694797750804\n",
      "LOSS 266: 265.7295672737853\n",
      "LOSS 267: 265.0231043432984\n",
      "LOSS 268: 264.23244706356746\n",
      "LOSS 269: 263.5574380058114\n",
      "LOSS 270: 263.02703008520757\n",
      "LOSS 271: 262.50922630719504\n",
      "LOSS 272: 261.6990163333016\n",
      "LOSS 273: 260.9976816240926\n",
      "LOSS 274: 260.50388275401247\n",
      "LOSS 275: 259.78641712227335\n",
      "LOSS 276: 259.13919406373515\n",
      "LOSS 277: 258.47699379228715\n",
      "LOSS 278: 257.9161856472166\n",
      "LOSS 279: 257.2701557348111\n",
      "LOSS 280: 256.59088161939417\n",
      "LOSS 281: 256.0568036087526\n",
      "LOSS 282: 255.5295792265647\n",
      "LOSS 283: 254.9135164759199\n",
      "LOSS 284: 254.1508997309133\n",
      "LOSS 285: 253.62219223948173\n",
      "LOSS 286: 253.01198665717214\n",
      "LOSS 287: 252.42272436978558\n",
      "LOSS 288: 251.82012300072716\n",
      "LOSS 289: 251.2153904146341\n",
      "LOSS 290: 250.7706105369587\n",
      "LOSS 291: 250.2262927836441\n",
      "LOSS 292: 249.63470223801264\n",
      "LOSS 293: 249.15937565620973\n",
      "LOSS 294: 248.491639829731\n",
      "LOSS 295: 248.0111218853172\n",
      "LOSS 296: 247.48840538942588\n",
      "LOSS 297: 246.81360743030308\n",
      "LOSS 298: 246.37387728440123\n",
      "LOSS 299: 245.82022041505448\n",
      "LOSS 300: 245.2342535871786\n",
      "LOSS 301: 244.6395588498188\n",
      "LOSS 302: 244.0201941746642\n",
      "LOSS 303: 243.39827052543853\n",
      "LOSS 304: 242.76049035851676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS 305: 242.1631538753679\n",
      "LOSS 306: 241.64035121138681\n",
      "LOSS 307: 241.09000193258592\n",
      "LOSS 308: 240.30071163805326\n",
      "LOSS 309: 239.773972184268\n",
      "LOSS 310: 239.25521821340286\n",
      "LOSS 311: 238.6278029563948\n",
      "LOSS 312: 237.98767213166954\n",
      "LOSS 313: 237.45410938360013\n",
      "LOSS 314: 236.8865816104125\n",
      "LOSS 315: 236.4352929767478\n",
      "LOSS 316: 235.8564282627429\n",
      "LOSS 317: 235.2845580865909\n",
      "LOSS 318: 234.70891928186617\n",
      "LOSS 319: 234.16428442588963\n",
      "LOSS 320: 233.64803264534464\n",
      "LOSS 321: 233.02812292726512\n",
      "LOSS 322: 232.44792856152333\n",
      "LOSS 323: 231.86458339077436\n",
      "LOSS 324: 231.38013581430167\n",
      "LOSS 325: 230.88295846393478\n",
      "LOSS 326: 230.31422068521198\n",
      "LOSS 327: 229.7520011672354\n",
      "LOSS 328: 229.14034393973174\n",
      "LOSS 329: 228.5738176142164\n",
      "LOSS 330: 228.05964412610987\n",
      "LOSS 331: 227.6617295792345\n",
      "LOSS 332: 227.3079836427407\n",
      "LOSS 333: 226.85639904169582\n",
      "LOSS 334: 226.2757213337857\n",
      "LOSS 335: 225.87159756531892\n",
      "LOSS 336: 225.51386451363507\n",
      "LOSS 337: 224.99790946349782\n",
      "LOSS 338: 224.53236809834905\n",
      "LOSS 339: 223.8655836486239\n",
      "LOSS 340: 223.34466227719616\n",
      "LOSS 341: 222.8070865697363\n",
      "LOSS 342: 222.20762452187617\n",
      "LOSS 343: 221.7169218652757\n",
      "LOSS 344: 221.21557671910634\n",
      "LOSS 345: 220.69963743359958\n",
      "LOSS 346: 220.2093963778739\n",
      "LOSS 347: 219.54927105002966\n",
      "LOSS 348: 219.08100767139112\n",
      "LOSS 349: 218.5136270980029\n",
      "LOSS 350: 218.0234792723103\n",
      "LOSS 351: 217.61119955827874\n",
      "LOSS 352: 217.1970229583723\n",
      "LOSS 353: 216.80775613270612\n",
      "LOSS 354: 216.17662165918676\n",
      "LOSS 355: 215.65948968185745\n",
      "LOSS 356: 215.03489922418905\n",
      "LOSS 357: 214.59428234757965\n",
      "LOSS 358: 214.09568517118\n",
      "LOSS 359: 213.6998784326713\n",
      "LOSS 360: 213.2346875358533\n",
      "LOSS 361: 212.48716950550505\n",
      "LOSS 362: 211.98623560076368\n",
      "LOSS 363: 211.5543290033036\n",
      "LOSS 364: 211.10956644603033\n",
      "LOSS 365: 210.59586667964828\n",
      "LOSS 366: 210.0257466172969\n",
      "LOSS 367: 209.48228003679588\n",
      "LOSS 368: 208.92106294864243\n",
      "LOSS 369: 208.56927846330578\n",
      "LOSS 370: 208.13338056567244\n",
      "LOSS 371: 207.5593973978899\n",
      "LOSS 372: 207.19617982602244\n",
      "LOSS 373: 206.7334447632451\n",
      "LOSS 374: 206.32735719423707\n",
      "LOSS 375: 205.93389984312796\n",
      "LOSS 376: 205.48778825636242\n",
      "LOSS 377: 205.1227474517975\n",
      "LOSS 378: 204.61356964206783\n",
      "LOSS 379: 204.10508710088732\n",
      "LOSS 380: 203.61160757882038\n",
      "LOSS 381: 203.11012947086613\n",
      "LOSS 382: 202.67331188291487\n",
      "LOSS 383: 202.36547328058867\n",
      "LOSS 384: 201.88123375980155\n",
      "LOSS 385: 201.4760997765266\n",
      "LOSS 386: 201.0239173530371\n",
      "LOSS 387: 200.63219776185215\n",
      "LOSS 388: 200.23432816525053\n",
      "LOSS 389: 199.72107885972602\n",
      "LOSS 390: 199.27926737110732\n",
      "LOSS 391: 198.95526255522634\n",
      "LOSS 392: 198.37163061068372\n",
      "LOSS 393: 197.7726294100766\n",
      "LOSS 394: 197.26825441295824\n",
      "LOSS 395: 196.92554335729892\n",
      "LOSS 396: 196.4940759960287\n",
      "LOSS 397: 196.00072932148854\n",
      "LOSS 398: 195.61714173455275\n",
      "LOSS 399: 195.2727518118047\n",
      "LOSS 400: 194.90186401099885\n",
      "LOSS 401: 194.51039219866564\n",
      "LOSS 402: 194.12210353611158\n",
      "LOSS 403: 193.68585781837086\n",
      "LOSS 404: 193.2935175628245\n",
      "LOSS 405: 192.77688084378644\n",
      "LOSS 406: 192.20948220156532\n",
      "LOSS 407: 191.76490380603545\n",
      "LOSS 408: 191.31226195866037\n",
      "LOSS 409: 190.98908489979854\n",
      "LOSS 410: 190.47946998368297\n",
      "LOSS 411: 190.10395990484147\n",
      "LOSS 412: 189.66624922090645\n",
      "LOSS 413: 189.24463773598293\n",
      "LOSS 414: 188.80909944026286\n",
      "LOSS 415: 188.12151421975346\n",
      "LOSS 416: 187.6626141028413\n",
      "LOSS 417: 187.30742829653627\n",
      "LOSS 418: 186.97840677830087\n",
      "LOSS 419: 186.64098913900878\n",
      "LOSS 420: 186.05462828395667\n",
      "LOSS 421: 185.6295077298895\n",
      "LOSS 422: 185.14505537760013\n",
      "LOSS 423: 184.69866059816061\n",
      "LOSS 424: 184.16099039404207\n",
      "LOSS 425: 183.8248906029158\n",
      "LOSS 426: 183.35444215417112\n",
      "LOSS 427: 183.02812248399266\n",
      "LOSS 428: 182.70938380434808\n",
      "LOSS 429: 182.07319672374103\n",
      "LOSS 430: 181.61676892208985\n",
      "LOSS 431: 181.1428187959007\n",
      "LOSS 432: 180.72351388415598\n",
      "LOSS 433: 180.28882847428355\n",
      "LOSS 434: 179.9416464521832\n",
      "LOSS 435: 179.63076241193622\n",
      "LOSS 436: 179.36840329322428\n",
      "LOSS 437: 178.86896692079563\n",
      "LOSS 438: 178.4495093388719\n",
      "LOSS 439: 178.01030008364134\n",
      "LOSS 440: 177.66486515536744\n",
      "LOSS 441: 177.13712385269375\n",
      "LOSS 442: 176.7639915325623\n",
      "LOSS 443: 176.21044187958665\n",
      "LOSS 444: 175.93589242231056\n",
      "LOSS 445: 175.4347485040116\n",
      "LOSS 446: 175.05859778261149\n",
      "LOSS 447: 174.6402147175643\n",
      "LOSS 448: 174.28212192866891\n",
      "LOSS 449: 173.97448485066732\n",
      "LOSS 450: 173.67223196377984\n",
      "LOSS 451: 173.27334709098884\n",
      "LOSS 452: 172.92559418353304\n",
      "LOSS 453: 172.63475844158708\n",
      "LOSS 454: 172.15504426002488\n",
      "LOSS 455: 171.74492372895813\n",
      "LOSS 456: 171.37029200688946\n",
      "LOSS 457: 170.94620069288038\n",
      "LOSS 458: 170.4911865607759\n",
      "LOSS 459: 170.1253561763544\n",
      "LOSS 460: 169.76740355939455\n",
      "LOSS 461: 169.48793527168561\n",
      "LOSS 462: 169.1163981460315\n",
      "LOSS 463: 168.8198203125936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(lr=0.1, n_iter=463)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00594921, 0.98338552, 0.00582911, 0.00483616],\n",
       "       [0.00706225, 0.00943849, 0.00715206, 0.9763472 ],\n",
       "       [0.95854109, 0.0091478 , 0.02357806, 0.00873304],\n",
       "       ...,\n",
       "       [0.00620146, 0.00964713, 0.00710539, 0.97704603],\n",
       "       [0.00818807, 0.01936631, 0.00953136, 0.96291426],\n",
       "       [0.00652486, 0.98134839, 0.00648617, 0.00564058]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = boosting_clf.predict(x_test)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN acc: 0.9825\n",
      "TEST acc: 0.8875\n"
     ]
    }
   ],
   "source": [
    "train_preds = boosting_clf.predict(x_train)\n",
    "print(f'TRAIN acc: {accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_preds, axis=1))}')\n",
    "print(f'TEST acc: {accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_preds, axis=1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "div #notebook {\n",
       "    background-color: #FFF9EE;\n",
       "    margin: auto;\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "    padding: 15px;\n",
       "    background-color: #FFFAFA;\n",
       "    min-height: 0;\n",
       "    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width to about 80 chars */\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "\n",
       "div.cell.border-box-sizing.code_cell.running { \n",
       "    border: 3px solid #111;\n",
       "}\n",
       "\n",
       "div.cell.code_cell {\n",
       "    background-color: #FFFAFA ;\n",
       "    border-radius: 5px;\n",
       "    padding: 1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Times New Roman';\n",
       "    color: #B8860B\n",
       "}\n",
       "\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 40pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 700;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 25pt;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: italic;\n",
       "    color: #8B4513;\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 20pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 16pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-weight: 300;\n",
       "    font-size: 10pt;\n",
       "    color: #8B4513;\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".text_cell_render p {\n",
       "    font-family: 'Times New Roman';\n",
       "    font-size: 15pt;\n",
       "    color: black;\n",
       "    text-align: justify;\n",
       "    text-justify: inter-word;\n",
       "    line-height: 1.5;\n",
       "}\n",
       "\n",
       "mark {\n",
       "  background: #D5EAFF;\n",
       "  color: black;\n",
       "}\n",
       "\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:2000px;  /* your desired max-height here */\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    background-color: #FFFAFA;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./style.css', 'r') as f:\n",
    "    style = f.read()\n",
    "HTML(style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_39",
   "language": "python",
   "name": "data_science_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
